---
title: 2021面试知识点
date: 2021-05-16 10:05:53
tags:
---

# URL

* 书
  *  https://zh.b-ok.cc/
  *  z-lib
     * https://2lib.org/
     * https://3lib.net/



# 知识点地图

- [ ] Java
  - [x] 反射
  - [x] 动态代理
  - [ ] 集合源码
    - [ ] ArrayList
    - [ ] LinkedList
    - [ ] ConcurrentHashMap源码
    - [ ] HashMap源码
  - [ ] IO
    - [x] 初步
    - [x] 阻塞Socket编程
    - [ ] NIO Socket编程
  - [ ] 序列化
    - [x] 初步
    - [ ] 手动实现
  - [ ] 注解
  - [x] 泛型
  - [ ] 异常
  - [ ] 时间
  - [x] 函数式编程
  - [x] 流式编程
  - [ ] 正则表达式
  - [x] Java8新特性
    - [ ] 重要部分编写
  - [ ] Java9新特性
- [ ] LeetCode50题
- [ ] spring/spring cloud
  - [ ] 基础
  - [ ] 如何解决循环依赖
- [ ] spring cloud alibaba
- [ ] netty
  - [x] 初步
  - [ ] 进阶
- [ ] tomcat
- [x] jvm/java虚拟机规范
- [ ] redis
  - [x] 初步
  - [ ] redis多线程
  - [ ] 面试题
- [ ] rocketmq
  - [x] 初步
  - [ ] 手动操作
  - [ ] 面试题
- [ ] rabbitmq
  - [ ] 初步
  - [ ] 手动操作
  - [ ] 面试题
- [ ] 并发编程
  - [ ] 基础
  - [ ] 并发包
  - [ ] ThreadLocal
  - [ ] 线程池调优
  - [ ] volatile
  - [ ] JMM 内存模型
- [ ] mysql
  - [ ] 基础
  - [ ] 事务
  - [x] 索引
- [ ] 设计模式
- [x] tcp/ip
  - [x] 三次握手与四次关闭
  - [x] 流量控制和拥塞控制
  - [x] tcp粘包与拆包
- [ ] http
  - [ ] http和https的区别
  - [ ] http中 get和post区别
  - [ ] http/1.0 http/1.1 http/2之间的区别
  - [ ] 幂等性
- [ ] linux
  - [x] find、grep、ps、cp、move、tar、head、tail、netstat、lsof、tree、wget、curl、ping、ssh、echo、free、top
  - [x] 自己手敲一遍
  - [ ] 高级运用
- [ ] Maven
- [ ] 敏捷开发

# 面试题

## 乐观锁VS悲观锁

- 悲观锁认为自己在使用数据的时候一定有别的线程来修改数据，因此在获取数据的时候会先加锁，确保数据不会被别的线程修改。Java中，synchronized关键字和Lock的实现类都是悲观锁。
- 而乐观锁认为自己在使用数据时不会有别的线程修改数据，所以不会添加锁，只是在更新数据的时候去判断之前有没有别的线程更新了这个数据。如果这个数据没有被更新，当前线程将自己修改的数据成功写入。如果数据已经被其他线程更新，则根据不同的实现方式执行不同的操作（例如报错或者自动重试），乐观锁在Java中是通过使用无锁编程来实现，最常采用的是CAS算法，Java原子类中的递增操作就通过CAS自旋实现的

　　根据从上面的概念描述我们可以发现：

　　**悲观锁适合写操作多的场景，先加锁可以保证写操作时数据正确**。

　　**乐观锁适合读操作多的场景，不加锁的特点能够使其读操作的性能大幅提升**。

## 公平锁VS非公平锁

公平锁是指多个线程按照申请锁的顺序来获取锁，线程直接进入队列中排队，队列中的第一个线程才能获得锁（排队获取锁）。

- 公平锁的优点是等待锁的线程不会饿死。
- 缺点是整体吞吐效率相对非公平锁要低，等待队列中除第一个线程以外的所有线程都会阻塞，CPU唤醒阻塞线程的开销比非公平锁大。

 非公平锁是多个线程加锁时直接尝试获取锁，获取不到才会到等待队列的队尾等待。但如果此时锁刚好可用，那么这个线程可以无需阻塞直接获取到锁，所以非公平锁有可能出现后申请锁的线程先获取锁的场景（尝试获取锁，不行就重新进队尾等待）。

- 非公平锁的优点是可以减少唤起线程的开销，整体的吞吐效率高，因为线程有几率不阻塞直接获得锁，CPU不必唤醒所有线程。
- 缺点是处于等待队列中的线程可能会饿死，或者等很久才会获得锁。

## 可重入锁与非可重入锁

　　可重入锁又名递归锁，**是指在同一个线程在外层方法获取锁的时候，再进入该线程的内层方法会自动获取锁（前提锁对象得是同一个对象或者class），不会因为之前已经获取过还没释放而阻塞**。Java中ReentrantLock和synchronized都是可重入锁，可重入锁的一个优点是可一定程度避免死锁 。

## 锁升级过程



# Linux

### ls

```sh
ls -a 列出目录所有文件，包含以.开始的隐藏文件
ls -A 列出除.及..的其它文件
ls -r 反序排列
ls -t 以文件修改时间排序
ls -S 以文件大小排序
ls -h 以易读大小显示
ls -l 除了文件名之外，还将文件的权限、所有者、文件大小等信息详细列出来
```

### cd

```sh
cd /
cd ~
# 进入上一个路径
cd - 
# 把上个命令的参数作为cd参数使用
cd !$
```

### mkdir

```sh
# 创建多级目录
mkdir -p /tmp/test/t1/t
```

### rm

```sh
# 删除前逐一询问
rm -i *.log
# 删除test目录及其子目录
rm -rf test

```

### cp

```sh
-i 提示
-r 复制目录及目录内所有项目
-a 复制的文件与原文件时间一样
```

### cat

```sh
cat filename
cat > filename
cat file1 file2 > file
```

- -b 对非空输出行号
- -n 输出所有行号

### more

​	功能类似于 cat, more 会以一页一页的显示方便使用者逐页阅读，而最基本的指令就是按空白键（space）就往下一页显示，按 b 键就会往回（back）一页显示。

```sh
+n      从笫 n 行开始显示
-n       定义屏幕大小为n行
+/pattern 在每个档案显示前搜寻该字串（pattern），然后从该字串前两行之后开始显示 
-c       从顶部清屏，然后显示
-d       提示“Press space to continue，’q’ to quit（按空格键继续，按q键退出）”，禁用响铃功能
-l        忽略Ctrl+l（换页）字符
-p       通过清除窗口而不是滚屏来对文件进行换页，与-c选项相似
-s       把连续的多个空行显示为一行
-u       把文件内容中的下画线去掉
```

### less

less 与 more 类似，但使用 less 可以随意浏览文件，而 more 仅能向前移动，却不能向后移动，而且 less 在查看之前不会加载整个文件。

```sh
-i  忽略搜索时的大小写
-N  显示每行的行号
-o  <文件名> 将less 输出的内容在指定文件中保存起来
-s  显示连续空行为一行
/字符串：向下搜索“字符串”的功能
?字符串：向上搜索“字符串”的功能
n：重复前一个搜索（与 / 或 ? 有关）
N：反向重复前一个搜索（与 / 或 ? 有关）
-x <数字> 将“tab”键显示为规定的数字空格
b  向后翻一页
d  向后翻半页
h  显示帮助界面
Q  退出less 命令
u  向前滚动半页
y  向前滚动一行
空格键 滚动一行
回车键 滚动一页
[pagedown]： 向下翻动一页
[pageup]：   向上翻动一页
```

### head

head 用来显示档案的开头至标准输出中，默认 head 命令打印其相应文件的开头 10 行。

```sh
-n<行数> 显示的行数（行数为复数表示从最后向前数）
```

（1）显示 1.log 文件中前 20 行

```sh
head 1.log -n 20
```

（2）显示 1.log 文件前 20 字节

```sh
head -c 20 log2014.log
```

（3）显示 t.log最后 10 行

```sh
head -n -10 t.log
```

### which

```sh
which     查看可执行文件的位置。
whereis 查看文件的位置。
locate  配合数据库查看文件位置。
find        实际搜寻硬盘查询文件名称。
```

查看当前 PATH 配置：

```sh
echo $PATH
```

### whereis

```sh
-b   定位可执行文件。
-m   定位帮助文件。
-s   定位源代码文件。
-u   搜索默认路径下除可执行文件、源代码文件、帮助文件以外的其它文件。
```



### find

```sh
find pathname -options [-print -exec -ok ...]

pathname: find命令所查找的目录路径。例如用.来表示当前目录，用/来表示系统根目录。
-print： find命令将匹配的文件输出到标准输出。
-exec： find命令对匹配的文件执行该参数所给出的shell命令。相应命令的形式为'command' {  } \;，注意{   }和\；之间的空格。
-ok： 和-exec的作用相同，只不过以一种更为安全的模式来执行该参数所给出的shell命令，在执行每一个命令之前，都会给出提示，让用户来确定是否执行。
```

**命令选项：**

```sh
-name 按照文件名查找文件
-perm 按文件权限查找文件
-user 按文件属主查找文件
-group  按照文件所属的组来查找文件。
-type  查找某一类型的文件，诸如：
   b - 块设备文件
   d - 目录
   c - 字符设备文件
   l - 符号链接文件
   p - 管道文件
   f - 普通文件

-size n :[c] 查找文件长度为n块文件，带有c时表文件字节大小
-amin n   查找系统中最后N分钟访问的文件
-atime n  查找系统中最后n*24小时访问的文件
-cmin n   查找系统中最后N分钟被改变文件状态的文件
-ctime n  查找系统中最后n*24小时被改变文件状态的文件
-mmin n   查找系统中最后N分钟被改变文件数据的文件
-mtime n  查找系统中最后n*24小时被改变文件数据的文件
(用减号-来限定更改时间在距今n日以内的文件，而用加号+来限定更改时间在距今n日以前的文件。 )
-maxdepth n 最大查找目录深度
-prune 选项来指出需要忽略的目录。在使用-prune选项时要当心，因为如果你同时使用了-depth选项，那么-prune选项就会被find命令忽略
-newer 如果希望查找更改时间比某个文件新但比另一个文件旧的所有文件，可以使用-newer选项
```

（1）查找 48 小时内修改过的文件

```sh
find -atime -2
```

（2）在当前目录查找 以 .log 结尾的文件。 **.** 代表当前目录

```sh
find ./ -name '*.log'
```

（3）查找 /opt 目录下 权限为 777 的文件

```sh
find /opt -perm 777
```

（4）查找大于 1K 的文件

```sh
find -size +1000c
```

查找等于 1000 字符的文件

```sh
find -size 1000c 
```

-exec 参数后面跟的是 command 命令，它的终止是以 ; 为结束标志的，所以这句命令后面的分号是不可缺少的，考虑到各个系统中分号会有不同的意义，所以前面加反斜杠。{} 花括号代表前面find查找出来的文件名。

**实例：**

（5）在当前目录中查找更改时间在10日以前的文件并删除它们(无提醒）

```
find . -type f -mtime +10 -exec rm -f {} \;
```

（6）当前目录中查找所有文件名以.log结尾、更改时间在5日以上的文件，并删除它们，只不过在删除之前先给出提示。 按y键删除文件，按n键不删除

```
find . -name '*.log' mtime +5 -ok -exec rm {} \;
```

（7）当前目录下查找文件名以 passwd 开头，内容包含 "pkg" 字符的文件

```
find . -f -name 'passwd*' -exec grep "pkg" {} \;
```

（8）用 exec 选项执行 cp 命令

```
find . -name '*.log' -exec cp {} test3 \;
```

-xargs find 命令把匹配到的文件传递给 xargs 命令，而 xargs 命令每次只获取一部分文件而不是全部，不像 -exec 选项那样。这样它可以先处理最先获取的一部分文件，然后是下一批，并如此继续下去。

实例：

（9）查找当前目录下每个普通文件，然后使用 xargs 来判断文件类型

```
find . -type f -print | xargs file
```

（10）查找当前目录下所有以 js 结尾的并且其中包含 'editor' 字符的普通文件

```
find . -type f -name "*.js" -exec grep -lF 'ueditor' {} \;
find -type f -name '*.js' | xargs grep -lF 'editor'
```

（11）利用 xargs 执行 mv 命令

```
find . -name "*.log" | xargs -i mv {} test4
```

（12）用 grep 命令在当前目录下的所有普通文件中搜索 hostnames 这个词，并标出所在行：

```
find . -name \*(转义） -type f -print | xargs grep -n 'hostnames'
```

（13）查找当前目录中以一个小写字母开头，最后是 4 到 9 加上 .log 结束的文件：

```
find . -name '[a-z]*[4-9].log' -print
```

（14）在 test 目录查找不在 test4 子目录查找

```
find test -path 'test/test4' -prune -o -print
```

（15）实例1：查找更改时间比文件 log2012.log新但比文件 log2017.log 旧的文件

```
find -newer log2012.log ! -newer log2017.log
```

**使用 depth 选项：**

depth 选项可以使 find 命令向磁带上备份文件系统时，希望首先备份所有的文件，其次再备份子目录中的文件。

实例：find 命令从文件系统的根目录开始，查找一个名为 CON.FILE 的文件。 它将首先匹配所有的文件然后再进入子目录中查找

```
find / -name "CON.FILE" -depth -print
```

### chmod

```sh
-c 当发生改变时，报告处理信息
-R 处理指定目录以及其子目录下所有文件
# 权限范围：
u ：目录或者文件的当前的用户
g ：目录或者文件的当前的群组
o ：除了目录或者文件的当前用户或群组之外的用户或者群组
a ：所有的用户及群组
# 权限代号
r ：读权限，用数字4表示
w ：写权限，用数字2表示
x ：执行权限，用数字1表示
- ：删除权限，用数字0表示
s ：特殊权限
```

（1）增加文件 t.log 所有用户可执行权限

```
chmod a+x t.log
```

（2）撤销原来所有的权限，然后使拥有者具有可读权限,并输出处理信息

```
chmod u=r t.log -c
```

（3）给 file 的属主分配读、写、执行(7)的权限，给file的所在组分配读、执行(5)的权限，给其他用户分配执行(1)的权限

```
chmod 751 t.log -c（或者：chmod u=rwx,g=rx,o=x t.log -c)
```

（4）将 test 目录及其子目录所有文件添加可读权限

```
chmod u+r,g+r,o+r -R text/ -c
```

### grep

强大的文本搜索命令，grep(Global Regular Expression Print) 全局正则表达式搜索。

grep 的工作方式是这样的，它在一个或多个文件中搜索字符串模板。如果模板包括空格，则必须被引用，模板后的所有字符串被看作文件名。搜索的结果被送到标准输出，不影响原文件内容。

命令格式：

```sh
grep [option] pattern file|dir
```

常用参数：

```sh
-A n --after-context显示匹配字符后n行
-B n --before-context显示匹配字符前n行
-C n --context 显示匹配字符前后n行
-c --count 计算符合样式的列数
-i 忽略大小写
-l 只列出文件内容符合指定的样式的文件名称
-f 从文件中读取关键词
-n 显示匹配内容的所在文件中行数
-R 递归查找文件夹
```

```sh
^  #锚定行的开始 如：'^grep'匹配所有以grep开头的行。 
$  #锚定行的结束 如：'grep$'匹配所有以grep结尾的行。 
.  #匹配一个非换行符的字符 如：'gr.p'匹配gr后接一个任意字符，然后是p。  
*  #匹配零个或多个先前字符 如：'*grep'匹配所有一个或多个空格后紧跟grep的行。
.*   #一起用代表任意字符。  
[]   #匹配一个指定范围内的字符，如'[Gg]rep'匹配Grep和grep。 
[^]  #匹配一个不在指定范围内的字符，如：'[^A-FH-Z]rep'匹配不包含A-R和T-Z的一个字母开头，紧跟rep的行。  
\(..\)  #标记匹配字符，如'\(love\)'，love被标记为1。   
\<      #锚定单词的开始，如:'\<grep'匹配包含以grep开头的单词的行。
\>      #锚定单词的结束，如'grep\>'匹配包含以grep结尾的单词的行。
x\{m\}  #重复字符x，m次，如：'0\{5\}'匹配包含5个o的行。 
x\{m,\}  #重复字符x,至少m次，如：'o\{5,\}'匹配至少有5个o的行。  
x\{m,n\}  #重复字符x，至少m次，不多于n次，如：'o\{5,10\}'匹配5--10个o的行。  
\w    #匹配文字和数字字符，也就是[A-Za-z0-9]，如：'G\w*p'匹配以G后跟零个或多个文字或数字字符，然后是p。  
\W    #\w的反置形式，匹配一个或多个非单词字符，如点号句号等。  
\b    #单词锁定符，如: '\bgrep\b'只匹配grep。
```

**实例：**

（1）查找指定进程

```sh
ps -ef | grep svn
```

（2）查找指定进程个数

```sh
ps -ef | grep svn -c
```

（3）从文件中读取关键词

```sh
cat test1.txt | grep -f key.log
```

（4）从文件夹中递归查找以grep开头的行，并只列出文件

```sh
grep -lR '^grep' /tmp
```

（5）查找非x开关的行内容

```sh
grep '^[^x]' test.txt
```

（6）显示包含 ed 或者 at 字符的内容行

```sh
grep -E 'ed|at' test.txt
```

### wc

wc(word count)功能为统计指定的文件中字节数、字数、行数，并将统计结果输出

**命令参数：**

```
-c 统计字节数
-l 统计行数
-m 统计字符数
-w 统计词数，一个字被定义为由空白、跳格或换行字符分隔的字符串
```

**实例：**

（1）查找文件的 行数 单词数 字节数 文件名

```sh
wc text.txt
```

结果：

```sh
7     8     70     test.txt
```

（2）统计输出结果的行数

```sh
cat test.txt | wc -l
```

### ps

ps(process status)，用来查看当前运行的进程状态，一次性查看，如果需要动态连续结果使用 top

linux上进程有5种状态:

- \1. 运行(正在运行或在运行队列中等待)
- \2. 中断(休眠中, 受阻, 在等待某个条件的形成或接受到信号)
- \3. 不可中断(收到信号不唤醒和不可运行, 进程必须等待直到有中断发生)
- \4. 僵死(进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放)
- \5. 停止(进程收到SIGSTOP, SIGSTP, SIGTIN, SIGTOU信号后停止运行运行)

ps 工具标识进程的5种状态码:

```sh
D 不可中断 uninterruptible sleep (usually IO)
R 运行 runnable (on run queue)
S 中断 sleeping
T 停止 traced or stopped
Z 僵死 a defunct (”zombie”) process
```

**命令参数：**

```sh
-A 显示所有进程
a 显示所有进程
-a 显示同一终端下所有进程
c 显示进程真实名称
e 显示环境变量
f 显示进程间的关系
r 显示当前终端运行的进程
-aux 显示所有包含其它使用的进程
```

**实例：**

（1）显示当前所有进程环境变量及进程间关系

```sh
ps -ef
```

（2）显示当前所有进程

```sh
ps -A
```

（3）与grep联用查找某进程

```sh
ps -aux | grep apache
```

（4）找出与 cron 与 syslog 这两个服务有关的 PID 号码

```sh
ps aux | grep '(cron|syslog)'
```

### df 

显示磁盘空间使用情况。获取硬盘被占用了多少空间，目前还剩下多少空间等信息，如果没有文件名被指定，则所有当前被挂载的文件系统的可用空间将被显示。默认情况下，磁盘空间将以 1KB 为单位进行显示，除非环境变量 POSIXLY_CORRECT 被指定，那样将以512字节为单位进行显示：

```sh
-a 全部文件系统列表
-h 以方便阅读的方式显示信息
-i 显示inode信息
-k 区块为1024字节
-l 只显示本地磁盘
-T 列出文件系统类型
```

**实例：**

（1）显示磁盘使用情况

```sh
df -l
```

（2）以易读方式列出所有文件系统及其类型

```sh
df -haT
```

### tar

用来压缩和解压文件。tar 本身不具有压缩功能，只具有打包功能，有关压缩及解压是调用其它的功能来完成。

弄清两个概念：打包和压缩。打包是指将一大堆文件或目录变成一个总的文件；压缩则是将一个大的文件通过一些压缩算法变成一个小文件

```sh
-c 建立新的压缩文件
-f 指定压缩文件
-r 添加文件到已经压缩文件包中
-u 添加改了和现有的文件到压缩包中
-x 从压缩包中抽取文件
-t 显示压缩文件中的内容
-z 支持gzip压缩
-j 支持bzip2压缩
-Z 支持compress解压文件
-v 显示操作过程
```

有关 gzip 及 bzip2 压缩:

```sh
gzip 实例：压缩 gzip fileName .tar.gz 和.tgz  解压：gunzip filename.gz 或 gzip -d filename.gz
          对应：tar zcvf filename.tar.gz     tar zxvf filename.tar.gz

bz2实例：压缩 bzip2 -z filename .tar.bz2 解压：bunzip filename.bz2或bzip -d filename.bz2
       对应：tar jcvf filename.tar.gz         解压：tar jxvf filename.tar.bz2
```

（1）将文件全部打包成 tar 包

```
tar -cvf log.tar 1.log,2.log 或tar -cvf log.*
```

（2）将 /etc 下的所有文件及目录打包到指定目录，并使用 gz 压缩

```
tar -zcvf /tmp/etc.tar.gz /etc
```

（3）查看刚打包的文件内容（一定加z，因为是使用 gzip 压缩的）

```
tar -ztvf /tmp/etc.tar.gz
```

（4）要压缩打包 /home, /etc ，但不要 /home/dmtsai

```
tar --exclude /home/dmtsai -zcvf myfile.tar.gz /home/* /etc
```

### kill

发送指定的信号到相应进程。不指定型号将发送SIGTERM（15）终止指定进程。如果任无法终止该程序可用"-KILL" 参数，其发送的信号为SIGKILL(9) ，将强制结束进程，使用ps命令或者jobs 命令可以查看进程号。root用户将影响用户的进程，非root用户只能影响自己的进程。

**常用参数：**

```
-l  信号，若果不加信号的编号参数，则使用“-l”参数会列出全部的信号名称
-a  当处理当前进程时，不限制命令名和进程号的对应关系
-p  指定kill 命令只打印相关进程的进程号，而不发送任何信号
-s  指定发送信号
-u  指定用户
```

**实例：**

（1）先使用ps查找进程pro1，然后用kill杀掉

```sh
kill -9 $(ps -ef | grep pro1)
```

### netstat

Linux netstat 命令用于显示网络状态。

利用 netstat 指令可让你得知整个 Linux 系统的网络情况。

```sh
netstat [-acCeFghilMnNoprstuvVwx][-A<网络类型>][--ip]
```

**参数说明**：

- -a或--all 显示所有连线中的Socket。
- -A<网络类型>或--<网络类型> 列出该网络类型连线中的相关地址。
- -c或--continuous 持续列出网络状态。
- -C或--cache 显示路由器配置的快取信息。
- -e或--extend 显示网络其他相关信息。
- -F或--fib 显示路由缓存。
- -g或--groups 显示多重广播功能群组组员名单。
- -h或--help 在线帮助。
- -i或--interfaces 显示网络界面信息表单。
- -l或--listening 显示监控中的服务器的Socket。
- -M或--masquerade 显示伪装的网络连线。
- -n或--numeric 直接使用IP地址，而不通过域名服务器。
- -N或--netlink或--symbolic 显示网络硬件外围设备的符号连接名称。
- -o或--timers 显示计时器。
- -p或--programs 显示正在使用Socket的程序识别码和程序名称。
- -r或--route 显示Routing Table。
- -s或--statistics 显示网络工作信息统计表。
- -t或--tcp 显示TCP传输协议的连线状况。
- -u或--udp 显示UDP传输协议的连线状况。
- -v或--verbose 显示指令执行过程。
- -V或--version 显示版本信息。
- -w或--raw 显示RAW传输协议的连线状况。
- -x或--unix 此参数的效果和指定"-A unix"参数相同。
- --ip或--inet 此参数的效果和指定"-A inet"参数相同。

#### 实例

显示详细的网络状况

```sh
# netstat -a
```

显示当前户籍UDP连接状况

```sh
# netstat -nu
```

显示UDP端口号的使用情况

```sh
# netstat -apu
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address        Foreign Address       State    PID/Program name  
udp    0   0 *:32768           *:*                   -          
udp    0   0 *:nfs            *:*                   -          
udp    0   0 *:641            *:*                   3006/rpc.statd   
udp    0   0 192.168.0.3:netbios-ns   *:*                   3537/nmbd      
udp    0   0 *:netbios-ns        *:*                   3537/nmbd      
udp    0   0 192.168.0.3:netbios-dgm   *:*                   3537/nmbd      
udp    0   0 *:netbios-dgm        *:*                   3537/nmbd      
udp    0   0 *:tftp           *:*                   3346/xinetd     
udp    0   0 *:999            *:*                   3366/rpc.rquotad  
udp    0   0 *:sunrpc          *:*                   2986/portmap    
udp    0   0 *:ipp            *:*                   6938/cupsd     
udp    0   0 *:1022           *:*                   3392/rpc.mountd   
udp    0   0 *:638            *:*                   3006/rpc.statd
```

显示网卡列表

```sh
# netstat -i
Kernel Interface table
Iface    MTU Met  RX-OK RX-ERR RX-DRP RX-OVR  TX-OK TX-ERR TX-DRP TX-OVR Flg
eth0    1500  0  181864   0   0   0  141278   0   0   0 BMRU
lo    16436  0   3362   0   0   0   3362   0   0   0 LRU
```

显示组播组的关系

```sh
# netstat -g
IPv6/IPv4 Group Memberships
Interface    RefCnt Group
--------------- ------ ---------------------
lo       1   ALL-SYSTEMS.MCAST.NET
eth0      1   ALL-SYSTEMS.MCAST.NET
lo       1   ff02::1
eth0      1   ff02::1:ff0a:b0c
eth0      1   ff02::1
```

显示网络统计信息

```sh
# netstat -s
Ip:
  184695 total packets received
  0 forwarded
  0 incoming packets discarded
  184687 incoming packets delivered
  143917 requests sent out
  32 outgoing packets dropped
  30 dropped because of missing route
Icmp:
  676 ICMP messages received
  5 input ICMP message failed.
  ICMP input histogram:
    destination unreachable: 44
    echo requests: 287
    echo replies: 345
  304 ICMP messages sent
  0 ICMP messages failed
  ICMP output histogram:
    destination unreachable: 17
    echo replies: 287
Tcp:
  473 active connections openings
  28 passive connection openings
  4 failed connection attempts
  11 connection resets received
  1 connections established
  178253 segments received
  137936 segments send out
  29 segments retransmited
  0 bad segments received.
  336 resets sent
Udp:
  5714 packets received
  8 packets to unknown port received.
  0 packet receive errors
  5419 packets sent
TcpExt:
  1 resets received for embryonic SYN_RECV sockets
  ArpFilter: 0
  12 TCP sockets finished time wait in fast timer
  572 delayed acks sent
  3 delayed acks further delayed because of locked socket
  13766 packets directly queued to recvmsg prequeue.
  1101482 packets directly received from backlog
  19599861 packets directly received from prequeue
  46860 packets header predicted
  14541 packets header predicted and directly queued to user
  TCPPureAcks: 12259
  TCPHPAcks: 9119
  TCPRenoRecovery: 0
  TCPSackRecovery: 0
  TCPSACKReneging: 0
  TCPFACKReorder: 0
  TCPSACKReorder: 0
  TCPRenoReorder: 0
  TCPTSReorder: 0
  TCPFullUndo: 0
  TCPPartialUndo: 0
  TCPDSACKUndo: 0
  TCPLossUndo: 0
  TCPLoss: 0
  TCPLostRetransmit: 0
  TCPRenoFailures: 0
  TCPSackFailures: 0
  TCPLossFailures: 0
  TCPFastRetrans: 0
  TCPForwardRetrans: 0
  TCPSlowStartRetrans: 0
  TCPTimeouts: 29
  TCPRenoRecoveryFail: 0
  TCPSackRecoveryFail: 0
  TCPSchedulerFailed: 0
  TCPRcvCollapsed: 0
  TCPDSACKOldSent: 0
  TCPDSACKOfoSent: 0
  TCPDSACKRecv: 0
  TCPDSACKOfoRecv: 0
  TCPAbortOnSyn: 0
  TCPAbortOnData: 1
  TCPAbortOnClose: 0
  TCPAbortOnMemory: 0
  TCPAbortOnTimeout: 3
  TCPAbortOnLinger: 0
  TCPAbortFailed: 3
  TCPMemoryPressures: 0
```

显示监听的套接口

```sh
# netstat -l
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address        Foreign Address       State   
tcp    0   0 *:32769           *:*             LISTEN   
tcp    0   0 *:nfs            *:*             LISTEN   
tcp    0   0 *:644            *:*             LISTEN   
tcp    0   0 *:1002           *:*             LISTEN   
tcp    0   0 *:netbios-ssn        *:*             LISTEN   
tcp    0   0 *:sunrpc          *:*             LISTEN   
tcp    0   0 vm-dev:ipp         *:*             LISTEN   
tcp    0   0 *:telnet          *:*             LISTEN   
tcp    0   0 *:601            *:*             LISTEN   
tcp    0   0 *:microsoft-ds       *:*             LISTEN   
tcp    0   0 *:http           *:*             LISTEN   
tcp    0   0 *:ssh            *:*             LISTEN   
tcp    0   0 *:https           *:*             LISTEN   
udp    0   0 *:32768           *:*                   
udp    0   0 *:nfs            *:*                   
udp    0   0 *:641            *:*                   
udp    0   0 192.168.0.3:netbios-ns   *:*                   
udp    0   0 *:netbios-ns        *:*                   
udp    0   0 192.168.0.3:netbios-dgm   *:*                   
udp    0   0 *:netbios-dgm        *:*                   
udp    0   0 *:tftp           *:*                   
udp    0   0 *:999            *:*                   
udp    0   0 *:sunrpc          *:*                   
udp    0   0 *:ipp            *:*                   
udp    0   0 *:1022           *:*                   
udp    0   0 *:638            *:*                   
Active UNIX domain sockets (only servers)
Proto RefCnt Flags    Type    State     I-Node Path
unix 2   [ ACC ]   STREAM   LISTENING   10621 @/tmp/fam-root-
unix 2   [ ACC ]   STREAM   LISTENING   7096  /var/run/acpid.socket
unix 2   [ ACC ]   STREAM   LISTENING   9792  /tmp/.gdm_socket
unix 2   [ ACC ]   STREAM   LISTENING   9927  /tmp/.X11-unix/X0
unix 2   [ ACC ]   STREAM   LISTENING   10489 /tmp/ssh-lbUnUf4552/agent.4552
unix 2   [ ACC ]   STREAM   LISTENING   10558 /tmp/ksocket-root/kdeinit__0
unix 2   [ ACC ]   STREAM   LISTENING   10560 /tmp/ksocket-root/kdeinit-:0
unix 2   [ ACC ]   STREAM   LISTENING   10570 /tmp/.ICE-unix/dcop4664-1270815442
unix 2   [ ACC ]   STREAM   LISTENING   10843 /tmp/.ICE-unix/4735
unix 2   [ ACC ]   STREAM   LISTENING   10591 /tmp/ksocket-root/klauncherah3arc.slave-socket
unix 2   [ ACC ]   STREAM   LISTENING   7763  /var/run/iiim/.iiimp-unix/9010
unix 2   [ ACC ]   STREAM   LISTENING   11047 /tmp/orbit-root/linc-1291-0-1e92c8082411
unix 2   [ ACC ]   STREAM   LISTENING   11053 /tmp/orbit-root/linc-128e-0-dc070659cbb3
unix 2   [ ACC ]   STREAM   LISTENING   8020  /var/run/dbus/system_bus_socket
unix 2   [ ACC ]   STREAM   LISTENING   58927 /tmp/mcop-root/vm-dev-2c28-4beba75f
unix 2   [ ACC ]   STREAM   LISTENING   7860  /tmp/.font-unix/fs7100
unix 2   [ ACC ]   STREAM   LISTENING   7658  /dev/gpmctl
unix 2   [ ACC ]   STREAM   LISTENING   10498 @/tmp/dbus-s2MLJGO5Ci
```

**netstat -tunlp** 用于显示 tcp，udp 的端口和进程等相关情况。

netstat 查看端口占用语法格式：

```
netstat -tunlp | grep 端口号
```

- -t (tcp) 仅显示tcp相关选项
- -u (udp)仅显示udp相关选项
- -n 拒绝显示别名，能显示数字的全部转化为数字
- -l 仅列出在Listen(监听)的服务状态
- -p 显示建立相关链接的程序名

例如查看 8000 端口的情况，使用以下命令：

```
# netstat -tunlp | grep 8000
tcp        0      0 0.0.0.0:8000            0.0.0.0:*               LISTEN      26993/nodejs   
```

更多命令：

```
netstat -ntlp   //查看当前所有tcp端口
netstat -ntulp | grep 80   //查看所有80端口使用情况
netstat -ntulp | grep 3306   //查看所有3306端口使用情况
```

### firewall

centos

```sh
1， 查看防火墙状态：

firewall-cmd --state

systemctl status firewalld.service

2， 开启防火墙：

systemctl start firewalld.service

3，设置开机自启：

systemctl enable firewalld.service

4， 重启防火墙：

systemctl restart firewalld.service

5， 查看防火墙设置开机自启是否成功：

systemctl is-enabled firewalld.service;echo $?

6，关闭防火墙：

systemctl stop firewalld.service

7，开端口命令：

打开单个端口：firewall-cmd --zone=public --add-port=80/tcp --permanent

打开多个端口：firewall-cmd --zone=public --add-port=20000-29999/tcp --permanent

--permanent  为永久生效，不加为单次有效（重启失效）

8，查看开启端口：

netstat -ntlp 或：firewall-cmd --list-ports

9，关闭端口命令：

firewall-cmd --zone=public --remove-port=80/tcp --permanent

10、查看端口是否打开

firewall-cmd --zone=public --query-port=80/tcp
```



### lsof

lsof语法格式是：

```sh
lsof ［options］ filename

lsof abc.txt 显示开启文件abc.txt的进程
lsof -c abc 显示abc进程现在打开的文件
lsof -c -p 1234 列出进程号为1234的进程所打开的文件
lsof -g gid 显示归属gid的进程情况
lsof +d /usr/local/ 显示目录下被进程开启的文件
lsof +D /usr/local/ 同上，但是会搜索目录下的目录，时间较长
lsof -d 4 显示使用fd为4的进程
lsof -i 用以显示符合条件的进程情况
lsof -i[46] [protocol][@hostname|hostaddr][:service|port]
  46 --> IPv4 or IPv6
  protocol --> TCP or UDP
  hostname --> Internet host name
  hostaddr --> IPv4地址
  service --> /etc/service中的 service name (可以不止一个)
  port --> 端口号 (可以不止一个)
```

lsof 查看端口占用语法格式：

```sh
lsof -i:端口号
```

### tree

Linux tree命令用于以树状图列出目录的内容。

执行tree指令，它会列出指定目录下的所有文件，包括子目录里的文件。

```sh
tree [-aACdDfFgilnNpqstux][-I <范本样式>][-P <范本样式>][目录...]
```

**参数说明**：

- -a 显示所有文件和目录。
- -A 使用ASNI绘图字符显示树状图而非以ASCII字符组合。
- -C 在文件和目录清单加上色彩，便于区分各种类型。
- -d 显示目录名称而非内容。
- -D 列出文件或目录的更改时间。
- -f 在每个文件或目录之前，显示完整的相对路径名称。
- -F 在执行文件，目录，Socket，符号连接，管道名称名称，各自加上"*","/","=","@","|"号。
- -g 列出文件或目录的所属群组名称，没有对应的名称时，则显示群组识别码。
- -i 不以阶梯状列出文件或目录名称。
- -L level 限制目录显示层级。
- -l 如遇到性质为符号连接的目录，直接列出该连接所指向的原始目录。
- -n 不在文件和目录清单加上色彩。
- -N 直接列出文件和目录名称，包括控制字符。
- -p 列出权限标示。
- -P<范本样式> 只显示符合范本样式的文件或目录名称。
- -q 用"?"号取代控制字符，列出文件和目录名称。
- -s 列出文件或目录大小。
- -t 用文件和目录的更改时间排序。
- -u 列出文件或目录的拥有者名称，没有对应的名称时，则显示用户识别码。
- -x 将范围局限在现行的文件系统中，若指定目录下的某些子目录，其存放于另一个文件系统上，则将该子目录予以排除在寻找范围外。

以树状图列出当前目录结构。可直接使用如下命令：

```
tree
```

该命令有如下输出结果：

```sh
# tree                   #以树状图列出当前目录结构  
.                        #当前目录结构  
|-- README  
|-- examples.desktop  
|-- file  
|-- file.new  
|-- index.htm  
|-- test  
| |-- README  
| |-- file  
| |-- testfile  
| |-- testfile1  
| |-- xaa  
| |-- xab  
| |-- xac  
| |-- xad  
| |-- xae  
| |-- xaf  
| |-- xag  
| |-- xah  
| `-- xai  
|-- test.tar.gz  
|-- test.zip  
|-- testfile  
|-- testfile.new  
|-- testfile.patch  
|-- testfile1  
|-- testfile2  
|-- testfile3  
|-- xaa  
|-- xab  
|-- xac  
|-- xad  
|-- xae  
|-- xaf  
|-- xag  
|-- xah  
|-- xai  
|-- \345\205\254\345\205\261\347\232\204  
|-- \345\233\276\347\211\207  
| |-- 075b5c2bb1628c1a5343c10a.jpg  
| |-- 0c978fe989ac787e799757095719d3c4.jpg  
| |-- 20050726194826866443.jpg  
| |-- 20061113171548785122.jpg  
| |-- 2007102221576687.jpg  
| |-- 39.jpg  
| |-- 434887ec4340916a78f0559a.jpg  
| |-- 498da016ac02fb2bc93d6d08.jpg  
| |-- 7b284f5a0f854da2f3bf90b204149a34.jpg  
| |-- 9196c030d342a68d5edf0e98.jpg  
| |-- a56c5a90de15c8a9a977a4cc.jpg  
| |-- c74f62167c9d2b244a90a79e.jpg  
| `-- img13.jpg  
|-- \346\226\207\346\241\243  
|-- \346\241\214\351\235\242  
|-- \346\250\241\346\235\277  
|-- \350\247\206\351\242\221  
`-- \351\237\263\344\271\220  
8 directories, 48 files           #统计信息，该目录共8个子目录，48个文件 
```

### wget

### curl

​	curl是一个非常实用的、用来与服务器之间传输数据的工具；支持的协议包括 (DICT, FILE, FTP, FTPS, GOPHER, HTTP, HTTPS, IMAP, IMAPS, LDAP, LDAPS, POP3, POP3S, RTMP, RTSP, SCP, SFTP, SMTP, SMTPS, TELNET and TFTP)，curl设计为无用户交互下完成工作；

　　curl提供了一大堆非常有用的功能，包括代理访问、用户认证、ftp上传下载、HTTP POST、SSL连接、cookie支持、断点续传...。

```sh
curl [options] [URL...]
```





|                            参数组                            | 参数                                                         | 描述                                                         |
| :----------------------------------------------------------: | :----------------------------------------------------------- | :----------------------------------------------------------- |
|                             url                              | url                                                          | 需要抓取的一到多个URLs； 多个下面通配符的方式：1、http://{www,ftp,mail}.aiezu.com； 　　2、http://aiezu.com/images/[001-999].jpg； 　　3、http://aiezu.com/images/[1-999].html； 　　4、ftp://aiezu.com/file[a-z].txt |
|                           请 求 头                           | -H "name: value" --header "name: value"                      | (HTTP)添加一个http header(http请求头)；                      |
|                 -H "name:" --header "name:"                  | (HTTP)移除一个http header(http请求头)；                      |                                                              |
| -A "string" --user-agent "string" [【参考】](http://aiezu.com/article/linux_curl_referer_useragent.html) | (HTTP)设置Http请求头“User-Agent”，服务器通过“User-Agent”可以判断客户端使用的浏览器名称和操作系统类型，伪造此参数能导致服务器做出错误判断。 也可以使用“-H”, “--header option”设置此选项； |                                                              |
| -e <URL> --referer <URL> [【参考】](http://aiezu.com/article/linux_curl_referer_useragent.html) | (HTTP)设置访问时的来源页面，告诉http服务从哪个页面进入到此页面； -e "aiezu.com"相当于“-H "Referer: www.qq.com"”； |                                                              |
|                           响 应 头                           | -I --head                                                    | (HTTP)只输出HTTP-header，不获取内容(HTTP/FTP/FILE)。 用于HTTP服务时，获取页面的http头；  （如：curl -I http://aiezu.com） 用于FTP/FILE时，将会获取文件大小、最后修改时间；  （如：curl -I file://test.txt） |
|                         -i --include                         | (HTTP)输出HTTP头和返回内容；                                 |                                                              |
|                -D <file> --dump-header <file>                | (HTTP)转储http响应头到指定文件；                             |                                                              |
|                            cookie                            | -b name=data --cookie name=data [【参考】](http://aiezu.com/article/linux_curl_http_cookie.html) | (HTTP)发送cookie数据到HTTP服务器，数据格式为："NAME1=VALUE1; NAME2=VALUE2"；  如果行中没有“=”，将把参数值当作cookie文件名；  这个cookie数据可以是由服务器的http响应头“Set-Cookie:”行发送过来的； |
| -c filename --cookie-jar file name [【参考】](http://aiezu.com/article/linux_curl_http_cookie.html) | (HTTP)完成操作后将服务器返回的cookies保存到指定的文件； 指定参数值为“-”将定向到标准输出“如控制台”； |                                                              |
|                  -j --junk-session-cookies                   | (HTTP)告诉curl放弃所有的"session cookies"； 相当于重启浏览器； |                                                              |
|                             代理                             | -x host:port -x [protocol://[user:pwd@]host[:port] --proxy [protocol://[user:pwd@]host[:port] [【参考】](http://aiezu.com/article/linux_curl_proxy_http_socks.html) | 使用HTTP代理访问；如果未指定端口，默认使用8080端口; protocol默认为http_proxy，其他可能的值包括： http_proxy、HTTPS_PROXY、socks4、socks4a、socks5； 如： --proxy 8.8.8.8:8080； -x "http_proxy://aiezu:123@aiezu.com:80" |
|                       -p --proxytunnel                       | 将“-x”参数的代理，作为通道的方式去代理非HTTP协议，如ftp；    |                                                              |
| --socks4 <host[:port]> --socks4a <host[:port]> --socks5 <host[:port]> [【参考】](http://aiezu.com/article/linux_curl_proxy_http_socks.html) | 使用SOCKS4代理； 使用SOCKS4A代理； 使用SOCKS5代理； 此参数会覆盖“-x”参数； |                                                              |
| --proxy-anyauth --proxy-basic --proxy-diges --proxy-negotiate --proxy-ntlm | http代理认证方式，参考： --anyauth --basic --diges --negotiate --ntlm |                                                              |
|       -U <user:password> --proxy-user <user:password>        | 设置代理的用户名和密码；                                     |                                                              |
|                          数据 传输                           | -G --get [【参考】](http://aiezu.com/article/linux_curl_getpost_datafile_json.html) | 如果使用了此参数，“-d/”、“--data”、“--data-binary”参数设置的数据，讲附加在url上，以GET的方式请求； |
| -d @file -d "string" --data "string" --data-ascii "string" --data-binary "string" --data-urlencode "string" [【参考】](http://aiezu.com/article/linux_curl_getpost_datafile_json.html) | (HTTP)使用HTTP POST方式发送“key/value对”数据，相当于浏览器表单属性（method="POST"，enctype="application/x-www-form-urlencoded"） 　　-d，--data：HTTP方式POST数据； 　　--data-ascii：HTTP方式POST ascii数据； 　　--data-binary：HTTP方式POST二进制数据； 　　--data-urlencode：HTTP方式POST数据（进行urlencode）； 如果数据以“@”开头，后紧跟一个文件，将post文件内的内容； |                                                              |
| -F name=@file -F name=<file -F name=content --form name=content [【参考】](http://aiezu.com/article/linux_curl_getpost_datafile_json.html) | (HTTP)使用HTTP POST方式发送类似“表单字段”的多类型数据，相当于同时设置浏览器表单属性（method="POST"，enctype="multipart/form-data"），可以使用此参数上传二进制文件。  如果字段内容以“@”开头，剩下的部分应该是文件名，curl将会上传此文件，如： curl -F "pic=@pic.jpg" http://aiezu.com； curl -F "page=@a.html;type=text/html" http://aiezu.com curl -F "page=@/tmp/a;filename=a.txt" http://aiezu.com  如果字段内容以“<”开头，剩下的部分应该是文件名，curl将从文件中获取作为此字段的值，如：curl -F "text=<text.txt" http://aiezu.com； |                                                              |
|                  --form-string <key=value>                   | (HTTP)类似于“--form”，但是“@”、“<”无特殊含义；               |                                                              |
|                  -T file --upload-file file                  | 通过“put”的方式将文件传输到远程网址；  选项参数只使用字符"-"，将通过stdin读入文件内容； 如： cat test.txt\|curl "http://aiezu.com/a.php" -T -  curl "http://aiezu.com/a.php" -T - <test.txt  此参数也可以使用通配符： curl -T "{file1,file2}" http://aiezu.com curl -T "img[1-1000].png" http://aiezu.com |                                                              |
|                          断点 续传                           | -C <offset> --continue-at <offset>                           | 断点续转，从文件头的指定位置开始继续下载/上传； offset续传开始的位置，如果offset值为“-”，curl会自动从文件中识别起始位置开始传输； |
|                  -r <range> --range <range>                  | (HTTP/FTP/SFTP/FILE) 只传输内容的指定部分： 0-499：最前面500字节； -500：最后面500字节； 9500-：最前面9500字节； 0-0,-1：最前面和最后面的1字节； 100-199,500-599：两个100字节； |                                                              |
|                             认证                             | --basic                                                      | (HTTP)告诉curl使用HTTP Basic authentication（HTTP协议时），这是默认认证方式； |
|                            --ntlm                            | (HTTP)使用NTLM身份验证方式，用于HTTP协议； 一般用于IIS使用NTLM的网站； |                                                              |
|                           --digest                           | (HTTP)使用HTTP Digest authentication加密，用于HTTP协议； 配合“-u/--user”选项，防止密码使用明文方式发送； |                                                              |
|                         --negotiate                          | (HTTP)使用GSS-Negotiate authentication方式，用于HTTP协议； 它主要目的是为它的主要目的是为kerberos5认证提供支持支持； |                                                              |
|                          --anyauth                           | (HTTP)告诉curl自动选择合适的身份认证方法，并选用最安全的方式； |                                                              |
|            -u user:password --user user:password             | 使用用户名、密码认证，此参数会覆盖“-n”、“--netrc”和“--netrc-optional”选项；  如果你只提供用户名，curl将要求你输入密码；  如果你使用“SSPI”开启的curl库做“NTLM”认证，可以使用不含用户名密码的“-u:”选项，强制curl使用当前登录的用户名密码进行认证；  此参数相当于设置http头“Authorization：”； |                                                              |
|                             证书                             | -E <证书[:密码]> --cert <证书[:密码]>                        | (SSL)指定“PEM”格式的证书文件和证书密码；                     |
|                      --cert-type <type>                      | (SSL)告诉curl所提供证书的类型：PEM、DER、ENG等； 默认为“PEM”； |                                                              |
|                      --cacert <CA证书>                       | (SSL)告诉curl所以指定的CA证书文件，必须是“PEM”格式；         |                                                              |
|                    --capath <CA证书路径>                     | (SSL)告诉curl所以指定目录下的CA证书用来验证； 这些证书必须是“PEM”格式； |                                                              |
|                       --crlfile <file>                       | (HTTPS/FTPS)提供一个PEM格式的文件，用于指定被吊销的证书列表； |                                                              |
|                        -k --insecure                         | (SSL)设置此选项将允许使用无证书的不安全SSL进行连接和传输。   |                                                              |
|                           SSL 其他                           | --ciphers <list of ciphers>                                  | (SSL)指定SSL要使用的加密方式；如：“aes_256_sha_256”；        |
|                       --engine <name>                        | 设置一个OpenSSL加密引擎用于加密操作； 使用“curl --engine list”查看支持的加密引擎列表； |                                                              |
|                        --random-file                         | (SSL)指定包含随机数据的文件路径名；数据是用来为SSL连接产生随机种子为； |                                                              |
|                      --egd-file <file>                       | (SSL)为随机种子生成器EGD(Entropy Gathering Daemon socket)指定的路径名； |                                                              |
| -1/--tlsv1 --tlsv1.0 --tlsv1.1 --tlsv1.2 -2/--sslv2 -3/--sslv3 | (SSL)使用TLS版本2与远程服务器通讯； (SSL)使用TLS 1.0版本与远程服务器通讯； (SSL)使用TLS 1.1版本与远程服务器通讯； (SSL)使用TLS 1.2版本与远程服务器通讯； (SSL)使用SSL版本2与远程服务器通讯； (SSL)使用SSL版本3与远程服务器通讯； |                                                              |
|                          私钥 公钥                           | --key <key>                                                  | (SSL/SSH)指定一个私钥文件名；为指定时自动尝试使用下面文件：“~/.ssh/id_rsa”、“~/.ssh/id_dsa”、“./id_rsa'”、 “./id_dsa”； |
|                      --key-type <type>                       | (SSL)指定私钥文件类型，支持：DER、PEM、ENG，默认是PEM；      |                                                              |
|                       --pass <phrase>                        | (SSL/SSH)指定私钥文件的密码；                                |                                                              |
|                        --pubkey <key>                        | (SSH)使用指定文件提供的您公钥；                              |                                                              |
|                             FTP                              | -P --ftp-port <接口>                                         | (FTP)FTP主动模式时，设置一个地址等待服务器的连接，如： 网卡：eth1 IP：8.8.8.8 主机名：aiezu.com 可以加端口号：eth1:20000-21000; |
|                            --crlf                            | (FTP)上传时将换行符(LF)转换为回车换行(CRLF)；                |                                                              |
|                     --ftp-account [data]                     | (FTP)ftp帐号信息；                                           |                                                              |
|                    --ftp-method [method]                     | (FTP)可选值：multicwd/nocwd/singlecwd；                      |                                                              |
|                          --ftp-pasv                          | (FTP)使用使用PASV(被动)/EPSV模式；                           |                                                              |
|                      --ftp-skip-pasv-ip                      | (FTP)使用PASV的时,跳过指定IP；                               |                                                              |
|                      --ftp-create-dirs                       | (FTP)上传时自动创建远程目录；                                |                                                              |
|                        -l --list-only                        | (FTP)列出ftp文件列表；                                       |                                                              |
|                        -B --use-ascii                        | (FTP/LDAP)使用Ascii传输模式，用于FTP、LDAP；在ftp中相当与使用了“type=A;”模式。 |                                                              |
|                        --disable-epsv                        | (FTP)告诉curl在PASV(被动模式)时不要使用EPSV；                |                                                              |
|                        --disable-eprt                        | (FTP)告诉curl在主动模式时禁用EPRT和LPRT；                    |                                                              |
|                             限速                             | --limit-rate <speed>                                         | 限制curl使用的最大带宽；如果未指定单位，默认单位为“bytes/秒”，你也可以指定单位为“K”、“M”、“G”等单位，如：“--limit-rate 1m”为限制最大使用带宽为“1m字节/秒”； |
|                    -y --speed-time <time>                    | If a download is slower than speed-limit bytes per second during a speed-time period, the download gets aborted. If speed-time is used, the default speed-limit will be 1 unless set with -Y. This option controls transfers and thus will not affect slow connects etc. If this is a concern for you, try the --connect-timeout option. |                                                              |
|                   -Y --speed-limit <speed>                   | If a download is slower than this given speed (in bytes per second) for speed-time seconds it gets aborted. speed-time is set with -y and is 30 if not set. |                                                              |
|                          其他 选项                           | -0/--http1.0                                                 | (HTTP) 强制curl使用HTTP 1.0而不是使用默认的HTTP 1.1；        |
|                      --interface <name>                      | 使用指定的网卡接口访问； curl --interface eth0 http://aiezu.com curl --interface 10.0.0.101 http://aiezu.com |                                                              |
|               -X <command> --request <command>               | （HTTP）指定与服务器通信使用的请求方法，如：GET、PUT、POST、DELETE等，默认GET； |                                                              |
|                  --keepalive-time <seconds>                  | 设置keepalive时间                                            |                                                              |
|                        --no-keepalive                        | 关闭keepalive功能；                                          |                                                              |
|                         --no-buffer                          | 禁用对输出流缓冲；                                           |                                                              |
|                           --buffer                           | 启用输出流缓冲；                                             |                                                              |
|                        -L --location                         | (HTTP/HTTPS)追随http响应头“Location：”定向到跳转后的页面； (在http响应码为3XX时使用，如301跳转、302跳转) |                                                              |
|                      --location-trusted                      | (HTTP/HTTPS)同“--location”，但跳转后会发送跳转前的用户名和密码； |                                                              |
|                         --compressed                         | (HTTP)请求对返回内容使用压缩算法进行压缩；curl支持对gzip压缩进行解压； |                                                              |
|                 --connect-timeout <seconds>                  | 指定最大连接超时，单位“秒”；                                 |                                                              |
|                -m seconds --max-time seconds                 | 限制整个curl操作的最长时间，单位为秒；                       |                                                              |
|                         -s --silent                          | 安静模式。不要显示进度表或错误消息；                         |                                                              |
|                      -# --progress-bar                       | 显示进度条；                                                 |                                                              |
|                          错误 选项                           | -f --fail                                                    | (HTTP)连接失败时（400以上错误）不返回默认错误页面，而是返回一个curl错误码“22”； |
| --retry <num> --retry-delay <seconds> --retry-max-time <seconds> | 失败重试次数； 重试间隔时间； 最大重试时间；                 |                                                              |
|                       -S --show-error                        | 安静模式下显示错误信息；                                     |                                                              |
|                       --stderr <file>                        | 错误信息保存文件；                                           |                                                              |
|                             输出                             | -o file --output file                                        | 将返回内容输出到文件。 如果是用过通配符获取多个url，可以使用“#”后跟“数字序号”，curl会自动将它替换对应的关键词，如： 　　curl "http://aiezu.com/{a,b}.txt" -o "#1.txt"; 　　将保存为：“a.txt”,“b.txt”;  　　curl "http://aiezu.com/{a,b}_[1-3].txt" -o "#1#2.txt"; 　　将保存为：a1.txt、a2.txt、a3.txt、b1.txt、b2.txt、b3.txt  　　如果要根据规则创建保存目录，参考：“--create-dirs”  指定“-”将定向到标准输出“如控制台”； |
|                       -O --remote-name                       | 将返回内容输出到当前目录下，和url中文件名相同的文件中（不含目录）； |                                                              |
|                        --create-dirs                         | 与“-o”参数配合使用，创建必要的本地目录层次结构               |                                                              |
|                    -w --write-out format                     | 操作完成后在返回信息尾部追加指定的内容；要追加的内容可以是一个字符串“string”、从文件中获取“@filename”、从标准输入中获取“@-”  格式参数中可以用%{variable_name} 方式使用响应信息的相关变量，如：%{content_type}、%{http_code}、%{local_ip}...，更多变量参考“man curl”获取；  格式参数可以使用“\n”、“\r”、“\t”等转义字符； |                                                              |
|                             调试                             | --trace <file>                                               | 转储所有传入和传出的数据到文件，包括描述信息； 使用“-”作为文件名将输出发送到标准输出。 |
|                      --trace-ascii file                      | 转储所有传入和传出的数据到文件，包括描述信息，只转储ASCII部分，更容易阅读； 使用“-”作为文件名将输出发送到标准输出。 这个选项会覆盖之前使用的-v、 --verbose、 --trace-ascii选项； |                                                              |
|                         --trace-time                         | 转储文件中添加时间信息；                                     |                                                              |
|                  -K --config <config file>                   | 从配置文件中读取参数，参考：http://curl.haxx.se/docs/        |                                                              |
|                         -v --verbose                         | 显示更详细的信息，调试时使用；                               |                                                              |
|                             帮助                             | -M --manual                                                  | 显示完整的帮助手册；                                         |
|                          -h --help                           | linux curl用法帮助；                                         |                                                              |

**用法演示：**
1、下载页面：

```
curl -o index.html http:``//aiezu``.com
```


2、下载文件并显示简单进度条：

```
curl -``# -o centos6.8.iso http://mirrors.aliyun.com/centos/6.8/isos/x86_64/CentOS-6.8-x86_64-minimal.iso
```


3、断点续传：

```
#继续完成上次终止的未完成的下载``curl -``# -o centos6.8.iso -C - http://mirrors.aliyun.com/centos/6.8/isos/x86_64/CentOS-6.8-x86_64-minimal.iso
```


4、伪造来源页面：

```
#告诉爱E族，我是从百度来的``curl -e http:``//baidu``.com http:``//aiezu``.com
```


 5、伪造代理设备：

```
#告诉爱E族，我是GOOGLE爬虫蜘蛛（其实我是curl命令）``curl -A ``" Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"` `http:``//aiezu``.com` `#告诉爱E族，我用的是微信内置浏览器``curl -A ``"Mozilla/5.0 AppleWebKit/600 Mobile MicroMessenger/6.0"` `http:``//aiezu``.com
```


6、http头：

```
# 看看本站的http头是怎么样的``curl -I http:``//aiezu``.com
```

输出：

```
HTTP/1.1 200 OK``Date: Fri, 25 Nov 2016 16:45:49 GMT``Server: Apache``Set-Cookie: rox__Session=abdrt8vesprhnpc3f63p1df7j4; path=/``Expires: Thu, 19 Nov 1981 08:52:00 GMT``Cache-Control: no-store, no-cache, must-revalidate, post-check=0, pre-check=0``Pragma: no-cache``Vary: Accept-Encoding``Content-Type: text/html; charset=utf-8
```


6、设置http请求头：

```
curl -H ``"Cache-Control:no-cache"` `http:``//aiezu``.com
```


7、发送表单数据：

```
curl -F ``"pic=@logo.png"` `-F ``"site=aiezu"` `http:``//aiezu``.com/
```


8、发送cookie：

```
curl -b ``"domain=aiezu.com"` `http:``//aiezu``.com
```

### ping

Linux ping 命令用于检测主机。

执行 ping 指令会使用 ICMP 传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常。

```sh
ping [-dfnqrRv][-c<完成次数>][-i<间隔秒数>][-I<网络界面>][-l<前置载入>][-p<范本样式>][-s<数据包大小>][-t<存活数值>][主机名称或IP地址]
```

**参数说明**：

- -d 使用Socket的SO_DEBUG功能。
- -c <完成次数> 设置完成要求回应的次数。
- -f 极限检测。
- -i<间隔秒数> 指定收发信息的间隔时间。
- -I<网络界面> 使用指定的网络接口送出数据包。
- -l<前置载入> 设置在送出要求信息之前，先行发出的数据包。
- -n 只输出数值。
- -p<范本样式> 设置填满数据包的范本样式。
- -q 不显示指令执行过程，开头和结尾的相关信息除外。
- -r 忽略普通的Routing Table，直接将数据包送到远端主机上。
- -R 记录路由过程。
- -s<数据包大小> 设置数据包的大小。
- -t<存活数值> 设置存活数值TTL的大小。
- -v 详细显示指令的执行过程。
- `-w <deadline>` 在 deadline 秒后退出。
- `-W <timeout>` 在等待 timeout 秒后开始执行。

#### 实例

检测是否与主机连通

```sh
# ping www.runoob.com //ping主机
PING aries.m.alikunlun.com (114.80.174.110) 56(84) bytes of data.
64 bytes from 114.80.174.110: icmp_seq=1 ttl=64 time=0.025 ms
64 bytes from 114.80.174.110: icmp_seq=2 ttl=64 time=0.036 ms
64 bytes from 114.80.174.110: icmp_seq=3 ttl=64 time=0.034 ms
64 bytes from 114.80.174.110: icmp_seq=4 ttl=64 time=0.034 ms
64 bytes from 114.80.174.110: icmp_seq=5 ttl=64 time=0.028 ms
64 bytes from 114.80.174.110: icmp_seq=6 ttl=64 time=0.028 ms
64 bytes from 114.80.174.110: icmp_seq=7 ttl=64 time=0.034 ms
64 bytes from 114.80.174.110: icmp_seq=8 ttl=64 time=0.034 ms
64 bytes from 114.80.174.110: icmp_seq=9 ttl=64 time=0.036 ms
64 bytes from 114.80.174.110: icmp_seq=10 ttl=64 time=0.041 ms

--- aries.m.alikunlun.com ping statistics ---
10 packets transmitted, 30 received, 0% packet loss, time 29246ms
rtt min/avg/max/mdev = 0.021/0.035/0.078/0.011 ms

//需要手动终止Ctrl+C
```

指定接收包的次数

```sh
# ping -c 2 www.runoob.com
PING aries.m.alikunlun.com (114.80.174.120) 56(84) bytes of data.
64 bytes from 114.80.174.120: icmp_seq=1 ttl=54 time=6.18 ms
64 bytes from 114.80.174.120: icmp_seq=2 ttl=54 time=15.4 ms

--- aries.m.alikunlun.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1016ms
rtt min/avg/max/mdev = 6.185/10.824/15.464/4.640 ms

//收到两次包后，自动退出
```

多参数使用

```sh
# ping -i 3 -s 1024 -t 255 g.cn //ping主机
PING g.cn (203.208.37.104) 1024(1052) bytes of data.
1032 bytes from bg-in-f104.1e100.net (203.208.37.104): icmp_seq=0 ttl=243 time=62.5 ms
1032 bytes from bg-in-f104.1e100.net (203.208.37.104): icmp_seq=1 ttl=243 time=63.9 ms
1032 bytes from bg-in-f104.1e100.net (203.208.37.104): icmp_seq=2 ttl=243 time=61.9 ms

--- g.cn ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 6001ms
rtt min/avg/max/mdev = 61.959/62.843/63.984/0.894 ms, pipe 2
[root@linux ~]# 

//-i 3 发送周期为 3秒 -s 设置发送包的大小 -t 设置TTL值为 255
```



### free

显示系统内存使用情况，包括物理内存、交互区内存(swap)和内核缓冲区内存。

**命令参数：**

```sh
-b 以Byte显示内存使用情况
-k 以kb为单位显示内存使用情况
-m 以mb为单位显示内存使用情况
-g 以gb为单位显示内存使用情况
-s<间隔秒数> 持续显示内存
-t 显示内存使用总合
```

**实例：**

（1）显示内存使用情况

```sh
free
free -k
free -m
```

（2）以总和的形式显示内存的使用信息

```sh
free -t
```

（3）周期性查询内存使用情况

```sh
free -s 10
```

### top

显示当前系统正在执行的进程的相关信息，包括进程 ID、内存占用率、CPU 占用率等

**常用参数：**

```sh
-c 显示完整的进程命令
-s 保密模式
-p <进程号> 指定进程显示
-n <次数>循环显示次数
```

实例：

**（1）**

```sh
top - 14:06:23 up 70 days, 16:44,  2 users,  load average: 1.25, 1.32, 1.35
Tasks: 206 total,   1 running, 205 sleeping,   0 stopped,   0 zombie
Cpu(s):  5.9%us,  3.4%sy,  0.0%ni, 90.4%id,  0.0%wa,  0.0%hi,  0.2%si,  0.0%st
Mem:  32949016k total, 14411180k used, 18537836k free,   169884k buffers
Swap: 32764556k total,        0k used, 32764556k free,  3612636k cached
PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND  
28894 root      22   0 1501m 405m  10m S 52.2  1.3   2534:16 java  
```

前五行是当前系统情况整体的统计信息区。

**第一行，任务队列信息，同 uptime 命令的执行结果，具体参数说明情况如下：**

14:06:23 — 当前系统时间

up 70 days, 16:44 — 系统已经运行了70天16小时44分钟（在这期间系统没有重启过的吆！）

2 users — 当前有2个用户登录系统

load average: 1.15, 1.42, 1.44 — load average后面的三个数分别是1分钟、5分钟、15分钟的负载情况。

load average数据是每隔5秒钟检查一次活跃的进程数，然后按特定算法计算出的数值。如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了。

**第二行，Tasks — 任务（进程），具体信息说明如下：**

系统现在共有206个进程，其中处于运行中的有1个，205个在休眠（sleep），stoped状态的有0个，zombie状态（僵尸）的有0个。

**第三行，cpu状态信息，具体属性说明如下：**

```sh
5.9%us — 用户空间占用CPU的百分比。
3.4% sy — 内核空间占用CPU的百分比。
0.0% ni — 改变过优先级的进程占用CPU的百分比
90.4% id — 空闲CPU百分比
0.0% wa — IO等待占用CPU的百分比
0.0% hi — 硬中断（Hardware IRQ）占用CPU的百分比
0.2% si — 软中断（Software Interrupts）占用CPU的百分比
```

**备注：**在这里CPU的使用比率和windows概念不同，需要理解linux系统用户空间和内核空间的相关知识！

第四行，内存状态，具体信息如下：

```sh
32949016k total — 物理内存总量（32GB）
14411180k used — 使用中的内存总量（14GB）
18537836k free — 空闲内存总量（18GB）
169884k buffers — 缓存的内存量 （169M）
```

**第五行，swap交换分区信息，具体信息说明如下：**

```sh
32764556k total — 交换区总量（32GB）
0k used — 使用的交换区总量（0K）
32764556k free — 空闲交换区总量（32GB）
3612636k cached — 缓冲的交换区总量（3.6GB）
```

**第六行，空行。**

**第七行以下：各进程（任务）的状态监控，项目列信息说明如下：**

```sh
PID — 进程id
USER — 进程所有者
PR — 进程优先级
NI — nice值。负值表示高优先级，正值表示低优先级
VIRT — 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES
RES — 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA
SHR — 共享内存大小，单位kb
S — 进程状态。D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程
%CPU — 上次更新到现在的CPU时间占用百分比
%MEM — 进程使用的物理内存百分比
TIME+ — 进程使用的CPU时间总计，单位1/100秒
COMMAND — 进程名称（命令名/命令行）
```

**top 交互命令**

```sh
h 显示top交互命令帮助信息
c 切换显示命令名称和完整命令行
m 以内存使用率排序
P 根据CPU使用百分比大小进行排序
T 根据时间/累计时间进行排序
W 将当前设置写入~/.toprc文件中
o或者O 改变显示项目的顺序
```

# Java源码

## HashMap

学习视频：https://www.bilibili.com/video/BV1FE411t7M7

### 1.HashMap介绍

​	基于哈希表的实现的`Map`接口，是以key-value存储形式存在，即主要用来存放键值对。HashMap的实现不是同步的，这意味着它不是线程安全的。它的key和value都可以为null。此外，HashMap中的映射不是有序的。

​	JDK1.8之前HasmMap是由数字+链表组成的。数组是HashMap的主体，链表则是主要为了解决哈希冲突（**两个对象调用的hashCode方法计算的Hash码值一致导致计算的数组的索引值相同**)而存在("拉链法"解决冲突)。JDK1.8以后在解决哈希冲突时有了比较大的变化。**当链表长度大于阈值(或者红黑树的边界值，默认为8)并且当前数组长度大于64时，此时此索引位置上的所有数据改为使用红黑树存储**。

​	补充：将链表转换成红黑树前会判断，即使阈值大于8，但是当前数组长度小于64，此时并不会将链表变为红黑树，而是选择进行数组扩容。

​	这样做的目的是因为数组比较小，尽量避开红黑树结构，这种情况下变为红黑树结构，反而会降低效率，因为红黑树需要进行左旋，右旋，变色这些操作来保持平衡。数组长度小于64时，搜索时间相对要快些，所以综上所述，为了提高性能和减少搜索时间，底层在阈值大于8并且数组长度大于64时，链表才转换为红黑树，具体可以参考 treeifyBin方法。

特点：

1. 存取无序的
2. 键和值位置都可以是null，但是键位置只能是一个null
3. 键位置是唯一的，底层的数据结构控制键的
4. jdk1.8前数据结构是：链表+数组，jdk1.8之后是：链表+数组+红黑树
5. 阈值（边界值）> 8并且数组长度大于64，才将链表转换为红黑树，变为红黑树的目的是为了高效查询。

### 2.HashMap集合底层的数据结构

#### 2.1数据结构概念

​	数据结构是计算机存储，组织数据的方式，数据结构是指相互之间存在一种或者多种特定关系的数据元素的集合。通常情况下，精心选择的数据结构可以带来更高的运行或者存储效率。数据结构往往同高效的检测算法和索引技术有关。

在JDK1.8之前HashMap由数组+链表 数据结构组成

在JDK1.8以及之后HashMap由数组+链表+红黑树数据结构组成

#### 2.2 HashMap底层的数据结构存储过程

1. HashMap<String, Integer> hm = new HashMap();

   当创建HashMap集合数据的时候，在jdk8以前，构造方法创造一个一个长度是16的Entry[] table 用来存储键值对数据的。 在jdk8后不是在HashMap的构造方法底层创建数组了，而是第一次调用put方法时创建数组 Node[] table

2. 假设向哈希表存储<柳岩,18>数据，根据柳岩调用String类中重写之后的HashCode()方法计算出值，然后结合数组长度采用某种算法计算出Node数组中存储数据的空间的索引值。如果计算出的索引空间没有值，则直接将数据存储到数组中。

   * 面试题：哈希表底层采用何种算法计算hash值？还有哪些算法可以计算出hash值？

     底层采用key的hashCode方法的值结合数组长度进行无符号右移(>>>)，按位抑或(^),按位与(&)计算出索引。

     还可以采用：平方取中法，取余数，伪随机数。

3. 假设刘德华的key也是3，假设刘德华计算出的hashCode结合数组长度计算出的索引和柳岩一样，那么此时数组空间不是null，此时底层会比较柳岩和刘德华的hash值是否一致，如果不一致，则在此空间上划出一个节点来存储键值对数据<刘德华,40>

   这种方式称为拉链法

4. 假设向哈希表存储数据<柳岩,20>，首先计算出的索引值肯定一样，如果hash值相等，此时发生hash碰撞，那么底层会先判断hashCode一致，一致就发生哈希碰撞，如果相等，就覆盖；如果都不相等，此空间上划出一个节点来存储键值对数据

![](https://pic.imgdb.cn/item/60a9f2e535c5199ba7baabe5.jpg)

	5. 在不断添加数据的过程中，会涉及到扩容问题，当超出临界值时(且要存放的位置非空)，扩容。默认扩容方式：扩容为原来的两倍，并将原有数据复制过来。

   	6. 通过上述描述，当一个链表中元素较多，即hash值相等但是内容不相等时，通过key值一次查询的效率较低。而JDK1.8中，哈希表存储采用是数组+链表+红黑树实现，当链表长度超过8且当前数组长度>64时，将链表转换为空黑叔，这样大大减少了查找时间。jdk8在哈希表中引入红黑树的原因只是为了查找效率更高。

**问题：传统hashMap的缺点,1.8为什么引入红黑树？这样的结构的话不是更麻烦了吗，为何发阈值大于8换成红黑树？**

​	JDK1.8以前HashMap的实现是数组+链表，即使哈希函数取得再好，也很难达到元素百分百均匀分布。当HashMap中有大量的元素都存放到同一个桶中时，这个桶下有一条常常的链表，这个时候HashMap就相当于一个单链表，假如单链表有n个元素，遍历的时间复杂度就是O(n)，完全失去了它的优势。针对这种情况，JDK1.8才引入了红黑树（查找时间复杂度为O(logn))来优化这个问题。当链表长度很小时，即使遍历，速度也非常快，但是当链表不断变长，肯定会对查询性能有一定影响，所以才需要转成数。

​	至于为什么阈值是9，我们看了源代码再来回答。

1. size代表HashMap中KV的实时数量，不等于数组长度
2. threshold(临界值) = capacity*loadFactor（加载因子）。size超过这个临界值就重新扩容，扩容后容量是之前两倍。

### 3.继承关系

```java
public class HashMap<K,V> extends AbstractMap<K,V>
    implements Map<K,V>, Cloneable, Serializable
```

说明：

* Cloneable表示可以克隆
* Serializable表示可以序列化
* AbstractMap父类提供了Map实现接口，以最大限度减少实现此接口所需要的工作。

补充：通过上述继承关系可以发现一个很奇怪的现象，就是HashMao已经继承了AbstractMap而AbstractMap已经实现了Map接口，那为什么HashMap还要再实现Mao接口呢？同样在ArrayList和LinkedList都是这种结构。

>据Java集合框架创世人Josh Blosh描述，这样的写法是一个失误。在Java框架中，类似的写法很多，最开始写Java集合的时候，他认为这样写，在某些地方可能是有价值的，直到他意识到错了。显然的，JDK的维护者后台不认为这个小小的失误只能去修改，所以就这样存在下来了。

### 4.HashMap集合类的成员变量

1.序列化版本号

```java
private static final long serialVersionUID = 362498820763181265L;
```

2.默认容量(必须是二的n次幂)

```java
static final int DEFAULT_INITIAL_CAPACITY = 1 << 4; // aka 16
```

问：为什么必须是2的n次幂？如果输入值不是2的幂比如10会怎么样？

```java
public HashMap(int initialCapacity) 
```

根据上述讲解我们已经知道，当向HashMap添加一个元素时，需要根据key的hash值，去确认其在数组中的具体位置，HashMap为了存储高效，要尽量减少碰撞，就是要尽量把数据分配均匀，每个链表长度大致相同，这个实现就是吧数据存储到哪个链表中的算法。

这个算法实际就是取模，hash%length，计算机直接求余的效率不如位运算效率高，所以源码中做了优化，使用hash&(length-1),实际上hash%length等于hash&(length-1)的前提是length是2的n次幂。

​	为什么这样能均匀缝补减少碰撞呢？2的n次方就是1后面n个0,，2的n次方-1吗，实际上就是n个1。

3.最大容量

```java
static final int MAXIMUM_CAPACITY = 1 << 30;
```

4.默认加载因子

```java
static final float DEFAULT_LOAD_FACTOR = 0.75f;
```

5.变树链表长度阈值

```java
static final int TREEIFY_THRESHOLD = 8;
```

**转换红黑树的边界值为什么是8？**

因为树节点内存大概是普通内存的2倍，所以只在链表上有足够节点（8）才转换为红黑树。当节点变得很小时(6)又转换成为链表。理想情况下，在随机哈希码下，链表中节点服从泊松分布。

选择8是因为泊松分布，

>```
>* Because TreeNodes are about twice the size of regular nodes, we
>* use them only when bins contain enough nodes to warrant use
>* (see TREEIFY_THRESHOLD). And when they become too small (due to
>* removal or resizing) they are converted back to plain bins.  In
>* usages with well-distributed user hashCodes, tree bins are
>* rarely used.  Ideally, under random hashCodes, the frequency of
>* nodes in bins follows a Poisson distribution
>* (http://en.wikipedia.org/wiki/Poisson_distribution) with a
>* parameter of about 0.5 on average for the default resizing
>* threshold of 0.75, although with a large variance because of
>* resizing granularity. Ignoring variance, the expected
>* occurrences of list size k are (exp(-0.5) * pow(0.5, k) /
>* factorial(k)). The first values are:
>*
>* 0:    0.60653066
>* 1:    0.30326533
>* 2:    0.07581633
>* 3:    0.01263606
>* 4:    0.00157952
>* 5:    0.00015795
>* 6:    0.00001316
>* 7:    0.00000094
>* 8:    0.00000006
>* more: less than 1 in ten million
>```



6.树退化为链表的阈值

```java
static final int UNTREEIFY_THRESHOLD = 6;
```

7.最小树化的数组容量

当Map中的数组容量超过这个值时， 表中的桶才会进行树化，否则桶元素太多时会进行扩容，而不是树化。为了避免扩容、树化的选择冲突，这个值不能小于4 * TREEIFY_THRESHOLD(8)

```java
static final int MIN_TREEIFY_CAPACITY = 64;
```

8.数据存放的table

```java
transient Node<K,V>[] table;
```

9.缓存的EntrySet

```java
transient Set<Map.Entry<K,V>> entrySet;
```

10.数据大小

```java
transient int size;
```

11.结构化修改次数

```java
transient int modCount;
```

12.扩容的阈值(capacity * load factor)

```java
int threshold;
```

13.加载因子

```java
final float loadFactor;
```

说明：

1. 加载因子是用来衡量HashMap满的程度，表示HashMap的疏密程度，影响hash操作到同一个数组位置的概率。计算HashMap的实时加载因子的方法为：size/capacity,而不是占用桶的数量去除以capacity。capacity是桶的数量，也就是table的长度length
2. loadFactor太大导致查找元素效率低，太小导致数组的利用率低，存放的数据会很分散。loadFactor的默认值为0.75f是官方给的一个比较好的边界值。
3. 当HashMap里面容纳的元素已经达到75%数组长度时，表示HashMap太挤了，需要扩容，而扩容这个过程涉及到rehash，复制数据等操作，非常消耗性能。所以开发中尽量减少扩容的次数，可以通过创建HashMap集合对象时初始容量时指定初始容量来尽量避免。

### 5.构造器

| 构造方法                                                     | 描述                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| **[HashMap](http://www.matools.com/file/manual/jdk_api_1.8_google/java/util/HashMap.html#HashMap--)**() | 构造一个空的 `HashMap` ，默认初始容量（16）和默认负载因子（0.75） |
| **[HashMap](http://www.matools.com/file/manual/jdk_api_1.8_google/java/util/HashMap.html#HashMap-int-)**(int initialCapacity) | 构造一个空的 `HashMap`具有指定的初始容量和默认负载因子（0.75） |
| **[HashMap](http://www.matools.com/file/manual/jdk_api_1.8_google/java/util/HashMap.html#HashMap-int-float-)**(int initialCapacity, float loadFactor) | 构造一个空的 `HashMap`具有指定的初始容量和负载因子           |
| **[HashMap](http://www.matools.com/file/manual/jdk_api_1.8_google/java/util/HashMap.html#HashMap-java.util.Map-)**([Map](http://www.matools.com/file/manual/jdk_api_1.8_google/java/util/Map.html)<? extends [K](http://www.matools.com/file/manual/jdk_api_1.8_google/java/util/HashMap.html),? extends [V](http://www.matools.com/file/manual/jdk_api_1.8_google/java/util/HashMap.html)> m) | 构造一个新的 `HashMap`与指定的相同的映射 Map                 |

#### HashMap()

容量在第一次put时初始化。

```java
public HashMap() {
    this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted
}
```

#### HashMap(int initialCapacity)

```java
public HashMap(int initialCapacity) {
    this(initialCapacity, DEFAULT_LOAD_FACTOR);
}
```

#### HashMap(int initialCapacity, float loadFactor)

这里主要分析tableSizeFor方法

这和方法主要给刚好大于等于cap的最小2的n次幂

1. 首先对cap执行-1操作

   为了防止cap已经是2的幂，如果cap已经是2的幂，又没有执行这个-1操作，则执行完几条无符号右移之后，返回的capacity将是这个cap的2倍。

2. 如果n为0，经过几次无符号右移依然是0，最后返回的值是1，因为后面有一个n+1操作。

Tips1：

几次位运算可以将n的进制表示的最高位的1后面的值全部全部置为1,然后在+1就可以求出刚好比他大的2的n次幂

* 得到这个capacity被赋值给threshold

```java
public HashMap(int initialCapacity, float loadFactor) {
    if (initialCapacity < 0)
        throw new IllegalArgumentException("Illegal initial capacity: " +
                initialCapacity);
    if (initialCapacity > MAXIMUM_CAPACITY)
        initialCapacity = MAXIMUM_CAPACITY;
    if (loadFactor <= 0 || Float.isNaN(loadFactor))
        throw new IllegalArgumentException("Illegal load factor: " +
                loadFactor);
    this.loadFactor = loadFactor;
 	//这里不等于capacity*loadFactor
    this.threshold = tableSizeFor(initialCapacity);
}

/**
 * Returns a power of two size for the given target capacity.
 */
static final int tableSizeFor(int cap) {
    int n = cap - 1;
    n |= n >>> 1;
    n |= n >>> 2;
    n |= n >>> 4;
    n |= n >>> 8;
    n |= n >>> 16;
    return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;
}
```

#### HashMap(Map<? extends K, ? extends V> m) 

```java
public HashMap(Map<? extends K, ? extends V> m) {
    this.loadFactor = DEFAULT_LOAD_FACTOR;
    putMapEntries(m, false);
}
final void putMapEntries(Map<? extends K, ? extends V> m, boolean evict) {
    int s = m.size();
    if (s > 0) {
        //如果table没有数据，那么是初始化
        if (table == null) { // pre-size
            //计算容量 +1是为了减少扩容
            float ft = ((float)s / loadFactor) + 1.0F;
            int t = ((ft < (float)MAXIMUM_CAPACITY) ?
                    (int)ft : MAXIMUM_CAPACITY);
            //不为0就计算边界值
            if (t > threshold)
                threshold = tableSizeFor(t);
        }
        //如果有数据，并且大于边界值，就扩容
        else if (s > threshold)
            resize();
        //然后把数据放进去
        for (Map.Entry<? extends K, ? extends V> e : m.entrySet()) {
            K key = e.getKey();
            V value = e.getValue();
            putVal(hash(key), key, value, false, evict);
        }
    }
}
```

### 6. 增加方法

putVal方法是比较复杂的

1. 先通过hash值计算放到哪个桶中

2. 如果桶上没有碰撞冲突，就直接插入

3. 如果碰撞冲突了，则需要冲突冲突

   a. 如果该桶使用红黑树处理冲突，则调用红黑树插入数据

   b. 否则采用传统的链式方法插入，如果链的长度达到边界值，则把链转变为红黑树

4. 如果同种存在重复键，则替换该键为新值value

5. 如果size大于阈值threshold，则进行扩容。

主要参数

* hashKey的hash值
* key 原始key
* value 要存放的值
* onlyIfAbsent 如果true代表不更改现有值
* evict 如果为false表示table为创建状态

```java
public V put(K key, V value) {
    return putVal(hash(key), key, value, false, true);
}


final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
               boolean evict) {
    HashMap.Node<K,V>[] tab; HashMap.Node<K,V> p; int n, i;
    //table没有数据，就扩容
    if ((tab = table) == null || (n = tab.length) == 0)
        n = (tab = resize()).length;
    //(n-1)&hash等于 n%hash
    if ((p = tab[i = (n - 1) & hash]) == null)
        //创建链表的第一个节点
        tab[i] = newNode(hash, key, value, null);
    else {
       
        HashMap.Node<K,V> e; K k;
        //如果hash并且第一个key相等，直接替换
        if (p.hash == hash &&
                ((k = p.key) == key || (key != null && key.equals(k))))
            e = p;
        //判断是否是树节点
        else if (p instanceof HashMap.TreeNode)
            e = ((HashMap.TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
        else {
            //是链表，就逐个比对
            for (int binCount = 0; ; ++binCount) {
                if ((e = p.next) == null) {
                    //如果找不到，就把数据添加到链表最后
                    p.next = newNode(hash, key, value, null);
                    //如果大于8就转换为树
                    if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                        treeifyBin(tab, hash);
                    break;
                }
                if (e.hash == hash &&
                        ((k = e.key) == key || (key != null && key.equals(k))))
                    break;
                p = e;
            }
        }
        if (e != null) { // existing mapping for key
            //存在就执行替换操作 
            V oldValue = e.value;
            if (!onlyIfAbsent || oldValue == null)
                e.value = value;
            afterNodeAccess(e);
            return oldValue;
        }
    }
    ++modCount;
    if (++size > threshold)
        resize();
    afterNodeInsertion(evict);
    return null;
}
```



#### hash方法分析

* 搞16bit不变，低16bit和高16bit做了一个抑或

为什么要这么操作？

如果当n即数组长度很小，假设为16的话，那么n-1为--->1111,这样的值和hashCode()做按位与操作，实际上只使用了哈希值的后4位，如果当hash值的高位变化很大，低位变化很小，就容易造成hash冲突，所以这里把高低位都利用起来，从而解决这个问题。

```java
static final int hash(Object key) {
   int h;
   return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

#### treeifyBin(Node<K,V>[] tab, int hash)

```java
final void treeifyBin(Node<K,V>[] tab, int hash) {
    int n, index; Node<K,V> e;
    //当少于容量64，只是扩容
    if (tab == null || (n = tab.length) < MIN_TREEIFY_CAPACITY)
        resize();
    else if ((e = tab[index = (n - 1) & hash]) != null) {
        TreeNode<K,V> hd = null, tl = null;
        do {
            //遍历链表，然后转换为树节点
            TreeNode<K,V> p = replacementTreeNode(e, null);
            if (tl == null)
                hd = p;
            else {
                p.prev = tl;
                tl.next = p;
            }
            tl = p;
        } while ((e = e.next) != null);
        //构建红黑树
        if ((tab[index] = hd) != null)
            hd.treeify(tab);
    }
}
```

#### 扩容方法 resize

想了解HashMap的扩容机制要有这两个问题

* 1.什么时候才需要扩容
* 2.HashMap的扩容是什么

1. 什么时候才需要扩容

当HashMap中的元素个数超过数组大小（数组长度）* loadFactor时，就会进行数组扩容，loadFactor的默认值是0.75，这是一个这种的取值。也就是说，默认情况下，数组大小为16，那么HashMap中的个数超过16 * 0.75 = 12，就把数组的大小扩展为2 * 16， 即扩大一倍。 然后重新计算每个元素在数组中的位置，而这是一个非常小号性能的操作，所以如果我们已经预知HashMap中的元素个数，那么与预知个数能够有效提高HashMap的性能。

补充：

当HashMap中的一个链表对象个数达到9个，次数数组长度没有达到64，那么HashMap先会扩容处理，如果已经达到了64，那么这个链表会变成红黑树，阶段类型由Node变成TreeNode类型。当然，如果映射关系被移除后，下次执行resize方法判断树的节点个数低于6，也会再把树转换为链表。

2. HashMap的扩容是什么？

进行扩容，会伴随着一次重新hash分配，并且会遍历hash表中所有元素，是非常耗时的，在编写程序中，要尽量避免resize。

HashMap在进行扩容时，使用的rehash方式非常巧妙，因为每次扩容都是翻倍，与元来计算(n-1)&hash的结果项目，只是多了一个bit位，所以节点要么就在原来的位置，要么就被分配到**原位置+旧容量**这个位置

```java
final Node<K,V>[] resize() {
    Node<K,V>[] oldTab = table;
    int oldCap = (oldTab == null) ? 0 : oldTab.length;
    int oldThr = threshold;
    int newCap, newThr = 0;
    if (oldCap > 0) {
        if (oldCap >= MAXIMUM_CAPACITY) {
            threshold = Integer.MAX_VALUE;
            return oldTab;
        }
        else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&
                 oldCap >= DEFAULT_INITIAL_CAPACITY)
            newThr = oldThr << 1; // double threshold
    }
    else if (oldThr > 0) // initial capacity was placed in threshold
        newCap = oldThr;
    else {               // zero initial threshold signifies using defaults
        newCap = DEFAULT_INITIAL_CAPACITY;
        newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
    }
    if (newThr == 0) {
        float ft = (float)newCap * loadFactor;
        newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?
                  (int)ft : Integer.MAX_VALUE);
    }
    threshold = newThr;
    @SuppressWarnings({"rawtypes","unchecked"})
        Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
    table = newTab;
    if (oldTab != null) {
        for (int j = 0; j < oldCap; ++j) {
            Node<K,V> e;
            if ((e = oldTab[j]) != null) {
                oldTab[j] = null;
                if (e.next == null)
                    newTab[e.hash & (newCap - 1)] = e;
                else if (e instanceof TreeNode)
                    ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);
                else { // preserve order
                    Node<K,V> loHead = null, loTail = null;
                    Node<K,V> hiHead = null, hiTail = null;
                    Node<K,V> next;
                    do {
                        next = e.next;
                        if ((e.hash & oldCap) == 0) {
                            if (loTail == null)
                                loHead = e;
                            else
                                loTail.next = e;
                            loTail = e;
                        }
                        else {
                            if (hiTail == null)
                                hiHead = e;
                            else
                                hiTail.next = e;
                            hiTail = e;
                        }
                    } while ((e = next) != null);
                    if (loTail != null) {
                        loTail.next = null;
                        newTab[j] = loHead;
                    }
                    if (hiTail != null) {
                        hiTail.next = null;
                        newTab[j + oldCap] = hiHead;
                    }
                }
            }
        }
    }
    return newTab;
}
```



# Java

## Java8新特性

| [--/--](http://openjdk.java.net/projects/jdk8/features#--/--) | [126](http://openjdk.java.net/jeps/126) | [Lambda Expressions & Virtual Extension Methods](http://openjdk.java.net/projects/jdk8/features#126) | Lambda表达式                                                 |
| ------------------------------------------------------------ | --------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
|                                                              | [138](http://openjdk.java.net/jeps/138) | [Autoconf-Based Build System](http://openjdk.java.net/projects/jdk8/features#138) |                                                              |
|                                                              | [160](http://openjdk.java.net/jeps/160) | [Lambda-Form Representation for Method Handles](http://openjdk.java.net/projects/jdk8/features#160) |                                                              |
|                                                              | [161](http://openjdk.java.net/jeps/161) | [Compact Profiles](http://openjdk.java.net/projects/jdk8/features#161) |                                                              |
|                                                              | [162](http://openjdk.java.net/jeps/162) | [Prepare for Modularization](http://openjdk.java.net/projects/jdk8/features#162) | 为模块化做了一些准备                                         |
|                                                              | [164](http://openjdk.java.net/jeps/164) | [Leverage CPU Instructions for AES Cryptography](http://openjdk.java.net/projects/jdk8/features#164) |                                                              |
|                                                              | [174](http://openjdk.java.net/jeps/174) | [Nashorn JavaScript Engine](http://openjdk.java.net/projects/jdk8/features#174) |                                                              |
|                                                              | [176](http://openjdk.java.net/jeps/176) | [Mechanical Checking of Caller-Sensitive Methods](http://openjdk.java.net/projects/jdk8/features#176) |                                                              |
|                                                              | [179](http://openjdk.java.net/jeps/179) | [Document JDK API Support and Stability](http://openjdk.java.net/projects/jdk8/features#179) |                                                              |
| [vm/--](http://openjdk.java.net/projects/jdk8/features#vm/--) | [142](http://openjdk.java.net/jeps/142) | [Reduce Cache Contention on Specified Fields](http://openjdk.java.net/projects/jdk8/features#142) |                                                              |
| [vm/gc](http://openjdk.java.net/projects/jdk8/features#vm/gc) | [122](http://openjdk.java.net/jeps/122) | [Remove the Permanent Generation](http://openjdk.java.net/projects/jdk8/features#122) |                                                              |
|                                                              | [173](http://openjdk.java.net/jeps/173) | [Retire Some Rarely-Used GC Combinations](http://openjdk.java.net/projects/jdk8/features#173) |                                                              |
| [vm/rt](http://openjdk.java.net/projects/jdk8/features#vm/rt) | [136](http://openjdk.java.net/jeps/136) | [Enhanced Verification Errors](http://openjdk.java.net/projects/jdk8/features#136) |                                                              |
|                                                              | [147](http://openjdk.java.net/jeps/147) | [Reduce Class Metadata Footprint](http://openjdk.java.net/projects/jdk8/features#147) |                                                              |
|                                                              | [148](http://openjdk.java.net/jeps/148) | [Small VM](http://openjdk.java.net/projects/jdk8/features#148) |                                                              |
|                                                              | [171](http://openjdk.java.net/jeps/171) | [Fence Intrinsics](http://openjdk.java.net/projects/jdk8/features#171) |                                                              |
| [core/--](http://openjdk.java.net/projects/jdk8/features#core/--) | [153](http://openjdk.java.net/jeps/153) | [Launch JavaFX Applications](http://openjdk.java.net/projects/jdk8/features#153) |                                                              |
| [core/lang](http://openjdk.java.net/projects/jdk8/features#core/lang) | [101](http://openjdk.java.net/jeps/101) | [Generalized Target-Type Inference](http://openjdk.java.net/projects/jdk8/features#101) |                                                              |
|                                                              | [104](http://openjdk.java.net/jeps/104) | [Annotations on Java Types](http://openjdk.java.net/projects/jdk8/features#104) |                                                              |
|                                                              | [105](http://openjdk.java.net/jeps/105) | [DocTree API](http://openjdk.java.net/projects/jdk8/features#105) |                                                              |
|                                                              | [106](http://openjdk.java.net/jeps/106) | [Add Javadoc to javax.tools](http://openjdk.java.net/projects/jdk8/features#106) |                                                              |
|                                                              | [117](http://openjdk.java.net/jeps/117) | [Remove the Annotation-Processing Tool (apt)](http://openjdk.java.net/projects/jdk8/features#117) |                                                              |
|                                                              | [118](http://openjdk.java.net/jeps/118) | [Access to Parameter Names at Runtime](http://openjdk.java.net/projects/jdk8/features#118) |                                                              |
|                                                              | [120](http://openjdk.java.net/jeps/120) | [Repeating Annotations](http://openjdk.java.net/projects/jdk8/features#120) | 重复的注解                                                   |
|                                                              | [139](http://openjdk.java.net/jeps/139) | [Enhance javac to Improve Build Speed](http://openjdk.java.net/projects/jdk8/features#139) |                                                              |
|                                                              | [172](http://openjdk.java.net/jeps/172) | [DocLint](http://openjdk.java.net/projects/jdk8/features#172) |                                                              |
| [core/libs](http://openjdk.java.net/projects/jdk8/features#core/libs) | [103](http://openjdk.java.net/jeps/103) | [Parallel Array Sorting](http://openjdk.java.net/projects/jdk8/features#103) | 并发数组排序                                                 |
|                                                              | [107](http://openjdk.java.net/jeps/107) | [Bulk Data Operations for Collections](http://openjdk.java.net/projects/jdk8/features#107) |                                                              |
|                                                              | [109](http://openjdk.java.net/jeps/109) | [Enhance Core Libraries with Lambda](http://openjdk.java.net/projects/jdk8/features#109) |                                                              |
|                                                              | [112](http://openjdk.java.net/jeps/112) | [Charset Implementation Improvements](http://openjdk.java.net/projects/jdk8/features#112) |                                                              |
|                                                              | [119](http://openjdk.java.net/jeps/119) | [javax.lang.model Implementation Backed by Core Reflection](http://openjdk.java.net/projects/jdk8/features#119) |                                                              |
|                                                              | [135](http://openjdk.java.net/jeps/135) | [Base64 Encoding & Decoding](http://openjdk.java.net/projects/jdk8/features#135) |                                                              |
|                                                              | [149](http://openjdk.java.net/jeps/149) | [Reduce Core-Library Memory Usage](http://openjdk.java.net/projects/jdk8/features#149) |                                                              |
|                                                              | [150](http://openjdk.java.net/jeps/150) | [Date & Time API](http://openjdk.java.net/projects/jdk8/features#150) | date和时间api                                                |
|                                                              | [155](http://openjdk.java.net/jeps/155) | [Concurrency Updates](http://openjdk.java.net/projects/jdk8/features#155) | 可升级的变量，对ConcurrentHashMap API的面向缓存的增强，ForkJoinPool的改进，以及额外的Lock和Future类。 |
|                                                              | [170](http://openjdk.java.net/jeps/170) | [JDBC 4.2](http://openjdk.java.net/projects/jdk8/features#170) |                                                              |
|                                                              | [177](http://openjdk.java.net/jeps/177) | [Optimize java.text.DecimalFormat.format](http://openjdk.java.net/projects/jdk8/features#177) |                                                              |
|                                                              | [178](http://openjdk.java.net/jeps/178) | [Statically-Linked JNI Libraries](http://openjdk.java.net/projects/jdk8/features#178) |                                                              |
|                                                              | [180](http://openjdk.java.net/jeps/180) | [Handle Frequent HashMap Collisions with Balanced Trees](http://openjdk.java.net/projects/jdk8/features#180) |                                                              |
| [core/i18n](http://openjdk.java.net/projects/jdk8/features#core/i18n) | [127](http://openjdk.java.net/jeps/127) | [Improve Locale Data Packaging and Adopt Unicode CLDR Data](http://openjdk.java.net/projects/jdk8/features#127) |                                                              |
|                                                              | [128](http://openjdk.java.net/jeps/128) | [BCP 47 Locale Matching](http://openjdk.java.net/projects/jdk8/features#128) |                                                              |
|                                                              | [133](http://openjdk.java.net/jeps/133) | [Unicode 6.2](http://openjdk.java.net/projects/jdk8/features#133) |                                                              |
| [core/net](http://openjdk.java.net/projects/jdk8/features#core/net) | [184](http://openjdk.java.net/jeps/184) | [HTTP URL Permissions](http://openjdk.java.net/projects/jdk8/features#184) |                                                              |
| [core/sec](http://openjdk.java.net/projects/jdk8/features#core/sec) | [113](http://openjdk.java.net/jeps/113) | [MS-SFU Kerberos 5 Extensions](http://openjdk.java.net/projects/jdk8/features#113) |                                                              |
|                                                              | [114](http://openjdk.java.net/jeps/114) | [TLS Server Name Indication (SNI) Extension](http://openjdk.java.net/projects/jdk8/features#114) |                                                              |
|                                                              | [115](http://openjdk.java.net/jeps/115) | [AEAD CipherSuites](http://openjdk.java.net/projects/jdk8/features#115) |                                                              |
|                                                              | [121](http://openjdk.java.net/jeps/121) | [Stronger Algorithms for Password-Based Encryption](http://openjdk.java.net/projects/jdk8/features#121) |                                                              |
|                                                              | [123](http://openjdk.java.net/jeps/123) | [Configurable Secure Random-Number Generation](http://openjdk.java.net/projects/jdk8/features#123) |                                                              |
|                                                              | [124](http://openjdk.java.net/jeps/124) | [Enhance the Certificate Revocation-Checking API](http://openjdk.java.net/projects/jdk8/features#124) |                                                              |
|                                                              | [129](http://openjdk.java.net/jeps/129) | [NSA Suite B Cryptographic Algorithms](http://openjdk.java.net/projects/jdk8/features#129) |                                                              |
|                                                              | [130](http://openjdk.java.net/jeps/130) | [SHA-224 Message Digests](http://openjdk.java.net/projects/jdk8/features#130) |                                                              |
|                                                              | [131](http://openjdk.java.net/jeps/131) | [PKCS#11 Crypto Provider for 64-bit Windows](http://openjdk.java.net/projects/jdk8/features#131) |                                                              |
|                                                              | [140](http://openjdk.java.net/jeps/140) | [Limited doPrivileged](http://openjdk.java.net/projects/jdk8/features#140) |                                                              |
|                                                              | [166](http://openjdk.java.net/jeps/166) | [Overhaul JKS-JCEKS-PKCS12 Keystores](http://openjdk.java.net/projects/jdk8/features#166) |                                                              |
| [web/jaxp](http://openjdk.java.net/projects/jdk8/features#web/jaxp) | [185](http://openjdk.java.net/jeps/185) | [Restrict Fetching of External XML Resources](http://openjdk.java.net/projects/jdk8/features#185) |                                                              |

### java8时间api

https://www.runoob.com/java/java8-datetime-api.html

## 动态代理

**代理模式**

* 给目标对象提供一个代理对象，并由代理对象控制对目标对象的引用。
  * 通过引入代理对象的方式间接访问目标对象，防止直接访问目标对象给系统带来的不必要复杂性。
  * 通过代理对象对原有业务增强。

![](https://pic.imgdb.cn/item/60a23ac76ae4f77d35def118.jpg)

### 静态代理

```java
package top.sorie.java.proxy;
interface ObjBase {
    void hello();
}
class Obj1 implements ObjBase {
    @Override
    public void hello() {
        System.out.println("Hello");
    }
}
public class SimpleProxy1 implements ObjBase {
    private ObjBase base;

    public SimpleProxy1(ObjBase base) {
        this.base = base;
    }
    @Override
    public void hello() {
        // something before
        base.hello();
        // something end
    }

    public static void main(String[] args) {
        SimpleProxy1 proxy1 = new SimpleProxy1(new Obj1());
        proxy1.hello();
    }
}
```

如果需要继承多个接口，代理类添加XXBase。

**缺点**

* 扩展能力差
* 可维护性差，违反开闭原则。

### 动态代理

* Proxy
* InvocationHandler

```java
interface ObjBase2 {
    void hello();
}
class Obj2 implements ObjBase2 {
    @Override
    public void hello() {
        System.out.println("Hello");
    }
}
public class DynamicProxy2 implements InvocationHandler {
    // 被代理的对象
    private ObjBase2 objBase;

    public DynamicProxy2(ObjBase2 objBase) {
        this.objBase = objBase;
    }

    @Override
    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
        // somethingBefore
        System.out.println("Hello Before");
        Object ret = null;
        try {
            ret = method.invoke(objBase, args);
        } catch (Exception e) {
            e.printStackTrace();
        }
        // somethingEnd
        System.out.println("Hello End");

        return ret;
    }

    public Object getProxyInstance() {
        return Proxy.newProxyInstance(objBase.getClass().getClassLoader(), objBase.getClass().getInterfaces(), this);
    }

    public static void main(String[] args) {
        DynamicProxy2 proxyFactory = new DynamicProxy2(new Obj2());
        ObjBase2 proxy2 = (ObjBase2)proxyFactory.getProxyInstance();
        proxy2.hello();
    }
}
```

### 原理

ProxyFactory

```java
@Override
        public Class<?> apply(ClassLoader loader, Class<?>[] interfaces) {

            Map<Class<?>, Boolean> interfaceSet = new IdentityHashMap<>(interfaces.length);
            for (Class<?> intf : interfaces) {
                /*
                 * Verify that the class loader resolves the name of this
                 * interface to the same Class object.
                 */
                Class<?> interfaceClass = null;
                try {
                    interfaceClass = Class.forName(intf.getName(), false, loader);
                } catch (ClassNotFoundException e) {
                }
                if (interfaceClass != intf) {
                    throw new IllegalArgumentException(
                        intf + " is not visible from class loader");
                }
                /*
                 * Verify that the Class object actually represents an
                 * interface.
                 */
                if (!interfaceClass.isInterface()) {
                    throw new IllegalArgumentException(
                        interfaceClass.getName() + " is not an interface");
                }
                /*
                 * Verify that this interface is not a duplicate.
                 */
                if (interfaceSet.put(interfaceClass, Boolean.TRUE) != null) {
                    throw new IllegalArgumentException(
                        "repeated interface: " + interfaceClass.getName());
                }
            }

            String proxyPkg = null;     // package to define proxy class in
            int accessFlags = Modifier.PUBLIC | Modifier.FINAL;

            /*
             * Record the package of a non-public proxy interface so that the
             * proxy class will be defined in the same package.  Verify that
             * all non-public proxy interfaces are in the same package.
             */
            for (Class<?> intf : interfaces) {
                int flags = intf.getModifiers();
                if (!Modifier.isPublic(flags)) {
                    accessFlags = Modifier.FINAL;
                    String name = intf.getName();
                    int n = name.lastIndexOf('.');
                    String pkg = ((n == -1) ? "" : name.substring(0, n + 1));
                    if (proxyPkg == null) {
                        proxyPkg = pkg;
                    } else if (!pkg.equals(proxyPkg)) {
                        throw new IllegalArgumentException(
                            "non-public interfaces from different packages");
                    }
                }
            }

            if (proxyPkg == null) {
                // if no non-public proxy interfaces, use com.sun.proxy package
                proxyPkg = ReflectUtil.PROXY_PACKAGE + ".";
            }

            /*
             * Choose a name for the proxy class to generate.
             */
            long num = nextUniqueNumber.getAndIncrement();
            String proxyName = proxyPkg + proxyClassNamePrefix + num;

            /*
             * Generate the specified proxy class.
             */
            byte[] proxyClassFile = ProxyGenerator.generateProxyClass(
                proxyName, interfaces, accessFlags);
            try {
                return defineClass0(loader, proxyName,
                                    proxyClassFile, 0, proxyClassFile.length);
            } catch (ClassFormatError e) {
                /*
                 * A ClassFormatError here means that (barring bugs in the
                 * proxy class generation code) there was some other
                 * invalid aspect of the arguments supplied to the proxy
                 * class creation (such as virtual machine limitations
                 * exceeded).
                 */
                throw new IllegalArgumentException(e.toString());
            }
        }
    }
```

### 动态代理面试题

**Spring AOP/事务实现原理**

* 动态代理



## 函数式编程

**Strategy** 接口提供了单一的 `approach()` 方法来承载函数式功能。通过创建不同的 **Strategy** 对象，我们可以创建不同的行为。

我们一般通过创建一个实现**Strategy**接口的类来实现这种行为，正如在**Soft**里所做的。

- **[1]** 在 **Strategize** 中，你可以看到 **Soft** 作为默认策略，在构造函数中赋值。

- **[2]** 一种较为简洁且更加自然的方法是创建一个**匿名内部类**。即便如此，仍有相当数量的冗余代码。你总需要仔细观察后才会发现：“哦，我明白了，原来这里使用了匿名内部类。”

- **[3]** Java 8 的 Lambda 表达式，其参数和函数体被箭头 `->` 分隔开。箭头右侧是从 Lambda 返回的表达式。它与单独定义类和采用匿名内部类是等价的，但代码少得多。

- **[4]** Java 8 的**方法引用**，它以 `::` 为特征。 `::` 的左边是类或对象的名称， `::` 的右边是方法的名称，但是没有参数列表。

- **[5]** 在使用默认的 **Soft**  策略之后，我们逐步遍历数组中的所有 **Strategy**，并通过调用 `changeStrategy()` 方法将每个 **Strategy** 传入变量 `s` 中。

- **[6]** 现在，每次调用 `communicate()` 都会产生不同的行为，具体取决于此刻正在使用的策略**代码对象**。我们传递的是行为，而并不仅仅是数据。[^3]

在 Java 8 之前，我们能够通过 **[1]** 和 **[2]** 的方式传递功能。然而，这种语法的读写非常笨拙，并且我们别无选择。方法引用和 Lambda 表达式的出现让我们可以在需要时**传递功能**，而不是仅在必要时才这么做。

### Lambda表达式

1. Lambda 表达式产生函数，而不是类。 虽然在 JVM（Java Virtual Machine，Java 虚拟机）上，一切都是类，但是幕后有各种操作执行让 Lambda 看起来像函数 —— 作为程序员，你可以高兴地假装它们“就是函数”。
2. Lambda 语法尽可能少，这正是为了使 Lambda 易于编写和使用。

```java
// functional/LambdaExpressions.java

interface Description {
  String brief();
}

interface Body {
  String detailed(String head);
}

interface Multi {
  String twoArg(String head, Double d);
}

public class LambdaExpressions {

  static Body bod = h -> h + " No Parens!"; // [1]

  static Body bod2 = (h) -> h + " More details"; // [2]

  static Description desc = () -> "Short info"; // [3]

  static Multi mult = (h, n) -> h + n; // [4]

  static Description moreLines = () -> { // [5]
    System.out.println("moreLines()");
    return "from moreLines()";
  };

  public static void main(String[] args) {
    System.out.println(bod.detailed("Oh!"));
    System.out.println(bod2.detailed("Hi!"));
    System.out.println(desc.brief());
    System.out.println(mult.twoArg("Pi! ", 3.14159));
    System.out.println(moreLines.brief());
  }
}
```

任何 Lambda 表达式的基本语法是：

1. 参数。

2. 接着 `->`，可视为“产出”。

3. `->` 之后的内容都是方法体。

  - **[1]** 当只用一个参数，可以不需要括号 `()`。 然而，这是一个特例。
  - **[2]** 正常情况使用括号 `()` 包裹参数。 为了保持一致性，也可以使用括号 `()` 包裹单个参数，虽然这种情况并不常见。
  - **[3]** 如果没有参数，则必须使用括号 `()` 表示空参数列表。
  - **[4]** 对于多个参数，将参数列表放在括号 `()` 中。

到目前为止，所有 Lambda 表达式方法体都是单行。 该表达式的结果自动成为 Lambda 表达式的返回值，在此处使用 **return** 关键字是非法的。 这是 Lambda 表达式简化相应语法的另一种方式。

**[5]** 如果在 Lambda 表达式中确实需要多行，则必须将这些行放在花括号中。 在这种情况下，就需要使用 **return**。

### 递归

```java
// functional/RecursiveFactorial.java

public class RecursiveFactorial {
  static IntCall fact;
  public static void main(String[] args) {
    fact = n -> n == 0 ? 1 : n * fact.call(n - 1);
    for(int i = 0; i <= 10; i++)
      System.out.println(fact.call(i));
  }
}
```

### 方法引用

Java 8 方法引用没有历史包袱。方法引用组成：类名或对象名，后面跟 `::` [^4]，然后跟方法名称。

```java
// functional/MethodReferences.java

import java.util.*;

interface Callable { // [1]
  void call(String s);
}

class Describe {
  void show(String msg) { // [2]
    System.out.println(msg);
  }
}

public class MethodReferences {
  static void hello(String name) { // [3]
    System.out.println("Hello, " + name);
  }
  static class Description {
    String about;
    Description(String desc) { about = desc; }
    void help(String msg) { // [4]
      System.out.println(about + " " + msg);
    }
  }
  static class Helper {
    static void assist(String msg) { // [5]
      System.out.println(msg);
    }
  }
  public static void main(String[] args) {
    Describe d = new Describe();
    Callable c = d::show; // [6]
    c.call("call()"); // [7]

    c = MethodReferences::hello; // [8]
    c.call("Bob");

    c = new Description("valuable")::help; // [9]
    c.call("information");

    c = Helper::assist; // [10]
    c.call("Help!");
  }
}
```

输出

```
call()
Hello, Bob
valuable information
Help!
```

**[1]** 我们从单一方法接口开始（同样，你很快就会了解到这一点的重要性）。

**[2]** `show()` 的签名（参数类型和返回类型）符合 **Callable** 的 `call()` 的签名。

**[3]** `hello()` 也符合 `call()` 的签名。 

**[4]**  `help()` 也符合，它是静态内部类中的非静态方法。

**[5]** `assist()` 是静态内部类中的静态方法。

**[6]** 我们将 **Describe** 对象的方法引用赋值给 **Callable** ，它没有 `show()` 方法，而是 `call()` 方法。 但是，Java 似乎接受用这个看似奇怪的赋值，因为方法引用符合 **Callable** 的 `call()` 方法的签名。

**[7]** 我们现在可以通过调用 `call()` 来调用 `show()`，因为 Java 将 `call()` 映射到 `show()`。

**[8]** 这是一个**静态**方法引用。

**[9]** 这是 **[6]** 的另一个版本：对已实例化对象的方法的引用，有时称为*绑定方法引用*。

**[10]** 最后，获取静态内部类中静态方法的引用与 **[8]** 中通过外部类引用相似。

上例只是简短的介绍，我们很快就能看到方法引用的所有不同形式。

### Runnable接口

​	**Runnable** 接口自 1.0 版以来一直在 Java 中，因此不需要导入。它也符合特殊的单方法接口格式：它的方法 `run()` 不带参数，也没有返回值。因此，我们可以使用 Lambda 表达式和方法引用作为 **Runnable**：

```java
// functional/RunnableMethodReference.java

// 方法引用与 Runnable 接口的结合使用

class Go {
  static void go() {
    System.out.println("Go::go()");
  }
}

public class RunnableMethodReference {
  public static void main(String[] args) {

    new Thread(new Runnable() {
      public void run() {
        System.out.println("Anonymous");
      }
    }).start();

    new Thread(
      () -> System.out.println("lambda")
    ).start();

    new Thread(Go::go).start();
  }
}
```

### 未绑定的方法引用

未绑定的方法引用是指没有关联对象的普通（非静态）方法。 使用未绑定的引用时，我们必须先提供对象：

```java
// functional/UnboundMethodReference.java

// 没有方法引用的对象

class X {
  String f() { return "X::f()"; }
}

interface MakeString {
  String make();
}

interface TransformX {
  String transform(X x);
}

public class UnboundMethodReference {
  public static void main(String[] args) {
    // MakeString ms = X::f; // [1]
    TransformX sp = X::f;
    X x = new X();
    System.out.println(sp.transform(x)); // [2]
    System.out.println(x.f()); // 同等效果
  }
}
```

输出

```java
X::f()
X::f()
```

到目前为止，我们已经见过了方法引用和对应接口的签名（参数类型和返回类型）一致的几个赋值例子。 在 **[1]** 中，我们尝试同样的做法，把 `X` 的 `f()` 方法引用赋值给 **MakeString**。结果即使 `make()` 与 `f()` 具有相同的签名，编译也会报“invalid method reference”（无效方法引用）错误。 问题在于，这里其实还需要另一个隐藏参数参与：我们的老朋友 `this`。 你不能在没有 `X` 对象的前提下调用 `f()`。 因此，`X :: f` 表示未绑定的方法引用，因为它尚未“绑定”到对象。

要解决这个问题，我们需要一个 `X` 对象，因此我们的接口实际上需要一个额外的参数，正如在 **TransformX** 中看到的那样。 如果将 `X :: f` 赋值给 **TransformX**，在 Java 中是允许的。我们必须做第二个心理调整——使用未绑定的引用时，函数式方法的签名（接口中的单个方法）不再与方法引用的签名完全匹配。 原因是：你需要一个对象来调用方法。

**[2]** 的结果有点像脑筋急转弯。我拿到未绑定的方法引用，并且调用它的`transform()`方法，将一个X类的对象传递给它，最后使得 `x.f()` 以某种方式被调用。Java知道它必须拿第一个参数，该参数实际就是`this` 对象，然后对此调用方法。

如果你的方法有更多个参数，就以第一个参数接受`this`的模式来处理。

```java
// functional/MultiUnbound.java

// 未绑定的方法与多参数的结合运用

class This {
  void two(int i, double d) {}
  void three(int i, double d, String s) {}
  void four(int i, double d, String s, char c) {}
}

interface TwoArgs {
  void call2(This athis, int i, double d);
}

interface ThreeArgs {
  void call3(This athis, int i, double d, String s);
}

interface FourArgs {
  void call4(
    This athis, int i, double d, String s, char c);
}

public class MultiUnbound {
  public static void main(String[] args) {
    TwoArgs twoargs = This::two;
    ThreeArgs threeargs = This::three;
    FourArgs fourargs = This::four;
    This athis = new This();
    twoargs.call2(athis, 11, 3.14);
    threeargs.call3(athis, 11, 3.14, "Three");
    fourargs.call4(athis, 11, 3.14, "Four", 'Z');
  }
}
```

​	需要指出的是，我将类命名为 **This**，并将函数式方法的第一个参数命名为 **athis**，但你在生产级代码中应该使用其他名字，以防止混淆。

### 构造函数引用

还可以捕获构造函数的引用，然后通过引用调用该构造函数。

```java
// functional/CtorReference.java

class Dog {
  String name;
  int age = -1; // For "unknown"
  Dog() { name = "stray"; }
  Dog(String nm) { name = nm; }
  Dog(String nm, int yrs) { name = nm; age = yrs; }
}

interface MakeNoArgs {
  Dog make();
}

interface Make1Arg {
  Dog make(String nm);
}

interface Make2Args {
  Dog make(String nm, int age);
}

public class CtorReference {
  public static void main(String[] args) {
    MakeNoArgs mna = Dog::new; // [1]
    Make1Arg m1a = Dog::new;   // [2]
    Make2Args m2a = Dog::new;  // [3]

    Dog dn = mna.make();
    Dog d1 = m1a.make("Comet");
    Dog d2 = m2a.make("Ralph", 4);
  }
}
```

**Dog** 有三个构造函数，函数式接口内的 `make()` 方法反映了构造函数参数列表（ `make()` 方法名称可以不同）。

**注意**我们如何对 **[1]**，**[2]** 和 **[3]** 中的每一个使用 `Dog :: new`。 这三个构造函数只有一个相同名称：`:: new`，但在每种情况下赋值给不同的接口，编译器可以从中知道具体使用哪个构造函数。

编译器知道调用函数式方法（本例中为 `make()`）就相当于调用构造函数。

<!-- Functional Interfaces -->

### 函数式接口

方法引用和 Lambda 表达式都必须被赋值，同时赋值需要类型信息才能使编译器保证类型的正确性。尤其是Lambda 表达式，它引入了新的要求。 代码示例：

```java
x -> x.toString()
```

我们清楚这里返回类型必须是 **String**，但 `x` 是什么类型呢？

Lambda 表达式包含 *类型推导* （编译器会自动推导出类型信息，避免了程序员显式地声明）。编译器必须能够以某种方式推导出 `x` 的类型。

下面是第二个代码示例：

```java
(x, y) -> x + y
```

现在 `x` 和 `y` 可以是任何支持 `+` 运算符连接的数据类型，可以是两个不同的数值类型或者是 一个 **String** 加任意一种可自动转换为 **String** 的数据类型（这包括了大多数类型）。 但是，当 Lambda 表达式被赋值时，编译器必须确定 `x` 和 `y` 的确切类型以生成正确的代码。

该问题也适用于方法引用。 假设你要传递 `System.out :: println` 到你正在编写的方法 ，你怎么知道传递给方法的参数的类型？

为了解决这个问题，Java 8 引入了 `java.util.function` 包。它包含一组接口，这些接口是 Lambda 表达式和方法引用的目标类型。 每个接口只包含一个抽象方法，称为 *函数式方法* 。

在编写接口时，可以使用 `@FunctionalInterface` 注解强制执行此“函数式方法”模式：

```java
// functional/FunctionalAnnotation.java

@FunctionalInterface
interface Functional {
  String goodbye(String arg);
}

interface FunctionalNoAnn {
  String goodbye(String arg);
}

/*
@FunctionalInterface
interface NotFunctional {
  String goodbye(String arg);
  String hello(String arg);
}
产生错误信息:
NotFunctional is not a functional interface
multiple non-overriding abstract methods
found in interface NotFunctional
*/

public class FunctionalAnnotation {
  public String goodbye(String arg) {
    return "Goodbye, " + arg;
  }
  public static void main(String[] args) {
    FunctionalAnnotation fa =
      new FunctionalAnnotation();
    Functional f = fa::goodbye;
    FunctionalNoAnn fna = fa::goodbye;
    // Functional fac = fa; // Incompatible
    Functional fl = a -> "Goodbye, " + a;
    FunctionalNoAnn fnal = a -> "Goodbye, " + a;
  }
}
```

`@FunctionalInterface` 注解是可选的; Java 会在 `main()` 中把 **Functional** 和 **FunctionalNoAnn** 都当作函数式接口来看待。 在 `NotFunctional` 的定义中可看出`@FunctionalInterface` 的作用：当接口中抽象方法多于一个时产生编译期错误。

仔细观察在定义 `f` 和 `fna` 时发生了什么。 `Functional` 和 `FunctionalNoAnn` 声明了是接口，然而被赋值的只是方法 `goodbye()`。首先，这只是一个方法而不是类；其次，它甚至都不是实现了该接口的类中的方法。这是添加到Java 8中的一点小魔法：如果将方法引用或 Lambda 表达式赋值给函数式接口（类型需要匹配），Java 会适配你的赋值到目标接口。 编译器会在后台把方法引用或 Lambda 表达式包装进实现目标接口的类的实例中。

虽然 `FunctionalAnnotation` 确实符合 `Functional` 模型，但是 Java不允许我们像`fac`定义的那样，将 `FunctionalAnnotation` 直接赋值给 `Functional`，因为 `FunctionalAnnotation` 并没有显式地去实现 `Functional` 接口。唯一的惊喜是，Java 8 允许我们将函数赋值给接口，这样的语法更加简单漂亮。

`java.util.function` 包旨在创建一组完整的目标接口，使得我们一般情况下不需再定义自己的接口。主要因为基本类型的存在，导致预定义的接口数量有少许增加。 如果你了解命名模式，顾名思义就能知道特定接口的作用。

 以下是基本命名准则：

1. 如果只处理对象而非基本类型，名称则为 `Function`，`Consumer`，`Predicate` 等。参数类型通过泛型添加。

2. 如果接收的参数是基本类型，则由名称的第一部分表示，如 `LongConsumer`，`DoubleFunction`，`IntPredicate` 等，但返回基本类型的 `Supplier` 接口例外。

3. 如果返回值为基本类型，则用 `To` 表示，如 `ToLongFunction <T>` 和 `IntToLongFunction`。

4. 如果返回值类型与参数类型相同，则是一个 `Operator` ：单个参数使用 `UnaryOperator`，两个参数使用 `BinaryOperator`。

5. 如果接收参数并返回一个布尔值，则是一个 **谓词** (`Predicate`)。

6. 如果接收的两个参数类型不同，则名称中有一个 `Bi`。

下表描述了 `java.util.function` 中的目标类型（包括例外情况）：

| **特征**                                                     |                       **函数式方法名**                       |                           **示例**                           |
| :----------------------------------------------------------- | :----------------------------------------------------------: | :----------------------------------------------------------: |
| 无参数； <br> 无返回值                                       |         **Runnable** <br> (java.lang)  <br>  `run()`         |                         **Runnable**                         |
| 无参数； <br> 返回类型任意                                   |         **Supplier** <br> `get()` <br> `getAs类型()`         | **Supplier`<T>`  <br> BooleanSupplier  <br> IntSupplier  <br> LongSupplier  <br> DoubleSupplier** |
| 无参数； <br> 返回类型任意                                   |   **Callable** <br> (java.util.concurrent)  <br> `call()`    |                      **Callable`<V>`**                       |
| 1 参数； <br> 无返回值                                       |                 **Consumer** <br> `accept()`                 | **`Consumer<T>` <br> IntConsumer <br> LongConsumer <br> DoubleConsumer** |
| 2 参数 **Consumer**                                          |                **BiConsumer** <br> `accept()`                |                    **`BiConsumer<T,U>`**                     |
| 2 参数 **Consumer**； <br> 第一个参数是 引用； <br> 第二个参数是 基本类型 |             **Obj类型Consumer** <br> `accept()`              | **`ObjIntConsumer<T>` <br> `ObjLongConsumer<T>` <br> `ObjDoubleConsumer<T>`** |
| 1 参数； <br> 返回类型不同                                   | **Function** <br> `apply()` <br> **To类型** 和 **类型To类型** <br> `applyAs类型()` | **Function`<T,R>` <br> IntFunction`<R>` <br> `LongFunction<R>` <br> DoubleFunction`<R>` <br> ToIntFunction`<T>` <br> `ToLongFunction<T>` <br> `ToDoubleFunction<T>` <br> IntToLongFunction <br> IntToDoubleFunction <br> LongToIntFunction <br> LongToDoubleFunction <br> DoubleToIntFunction <br> DoubleToLongFunction** |
| 1 参数； <br> 返回类型相同                                   |               **UnaryOperator** <br> `apply()`               | **`UnaryOperator<T>` <br> IntUnaryOperator <br> LongUnaryOperator <br> DoubleUnaryOperator** |
| 2 参数，类型相同； <br> 返回类型相同                         |              **BinaryOperator** <br> `apply()`               | **`BinaryOperator<T>` <br> IntBinaryOperator <br> LongBinaryOperator <br> DoubleBinaryOperator** |
| 2 参数，类型相同; <br> 返回整型                              |         Comparator <br> (java.util) <br> `compare()`         |                     **`Comparator<T>`**                      |
| 2 参数； <br> 返回布尔型                                     |                 **Predicate** <br> `test()`                  | **`Predicate<T>` <br> `BiPredicate<T,U>` <br> IntPredicate <br> LongPredicate <br> DoublePredicate** |
| 参数基本类型； <br> 返回基本类型                             |         **类型To类型Function** <br> `applyAs类型()`          | **IntToLongFunction <br> IntToDoubleFunction <br> LongToIntFunction <br> LongToDoubleFunction <br> DoubleToIntFunction <br> DoubleToLongFunction** |
| 2 参数； <br>类型不同                                        |                 **Bi操作** <br> (不同方法名)                 | **`BiFunction<T,U,R>` <br> `BiConsumer<T,U>` <br> `BiPredicate<T,U>` <br> `ToIntBiFunction<T,U>` <br> `ToLongBiFunction<T,U>` <br> `ToDoubleBiFunction<T>`** |

下面枚举了基于 Lambda 表达式的所有不同 **Function** 变体的示例：

```java
// functional/FunctionVariants.java

import java.util.function.*;

class Foo {}

class Bar {
  Foo f;
  Bar(Foo f) { this.f = f; }
}

class IBaz {
  int i;
  IBaz(int i) {
    this.i = i;
  }
}

class LBaz {
  long l;
  LBaz(long l) {
    this.l = l;
  }
}

class DBaz {
  double d;
  DBaz(double d) {
    this.d = d;
  }
}

public class FunctionVariants {
  static Function<Foo,Bar> f1 = f -> new Bar(f);
  static IntFunction<IBaz> f2 = i -> new IBaz(i);
  static LongFunction<LBaz> f3 = l -> new LBaz(l);
  static DoubleFunction<DBaz> f4 = d -> new DBaz(d);
  static ToIntFunction<IBaz> f5 = ib -> ib.i;
  static ToLongFunction<LBaz> f6 = lb -> lb.l;
  static ToDoubleFunction<DBaz> f7 = db -> db.d;
  static IntToLongFunction f8 = i -> i;
  static IntToDoubleFunction f9 = i -> i;
  static LongToIntFunction f10 = l -> (int)l;
  static LongToDoubleFunction f11 = l -> l;
  static DoubleToIntFunction f12 = d -> (int)d;
  static DoubleToLongFunction f13 = d -> (long)d;

  public static void main(String[] args) {
    Bar b = f1.apply(new Foo());
    IBaz ib = f2.apply(11);
    LBaz lb = f3.apply(11);
    DBaz db = f4.apply(11);
    int i = f5.applyAsInt(ib);
    long l = f6.applyAsLong(lb);
    double d = f7.applyAsDouble(db);
    l = f8.applyAsLong(12);
    d = f9.applyAsDouble(12);
    i = f10.applyAsInt(12);
    d = f11.applyAsDouble(12);
    i = f12.applyAsInt(13.0);
    l = f13.applyAsLong(13.0);
  }
}
```

这些 Lambda 表达式尝试生成适合函数签名的最简代码。 在某些情况下有必要进行强制类型转换，否则编译器会报截断错误。

方法引用有自己的小魔法：

```java
/ functional/MethodConversion.java

import java.util.function.*;

class In1 {}
class In2 {}

public class MethodConversion {
  static void accept(In1 i1, In2 i2) {
    System.out.println("accept()");
  }
  static void someOtherName(In1 i1, In2 i2) {
    System.out.println("someOtherName()");
  }
  public static void main(String[] args) {
    BiConsumer<In1,In2> bic;

    bic = MethodConversion::accept;
    bic.accept(new In1(), new In2());

    bic = MethodConversion::someOtherName;
    // bic.someOtherName(new In1(), new In2()); // Nope
    bic.accept(new In1(), new In2());
  }
}
```

输出结果：

```
accept()
someOtherName()
```

查看 `BiConsumer` 的文档，你会看到它的函数式方法为 `accept()` 。 的确，如果我们将方法命名为 `accept()`，它就可以作为方法引用。 但是我们也可用不同的名称，比如 `someOtherName()`。只要参数类型、返回类型与 `BiConsumer` 的 `accept()` 相同即可。

因此，在使用函数接口时，名称无关紧要——只要参数类型和返回类型相同。 Java 会将你的方法映射到接口方法。 要调用方法，可以调用接口的函数式方法名（在本例中为 `accept()`），而不是你的方法名。

现在我们来看看，将方法引用应用于基于类的函数式接口（即那些不包含基本类型的函数式接口）。下面的例子中，我创建了适合函数式方法签名的最简单的方法：

```java
// functional/ClassFunctionals.java

import java.util.*;
import java.util.function.*;

class AA {}
class BB {}
class CC {}

public class ClassFunctionals {
  static AA f1() { return new AA(); }
  static int f2(AA aa1, AA aa2) { return 1; }
  static void f3(AA aa) {}
  static void f4(AA aa, BB bb) {}
  static CC f5(AA aa) { return new CC(); }
  static CC f6(AA aa, BB bb) { return new CC(); }
  static boolean f7(AA aa) { return true; }
  static boolean f8(AA aa, BB bb) { return true; }
  static AA f9(AA aa) { return new AA(); }
  static AA f10(AA aa1, AA aa2) { return new AA(); }
  public static void main(String[] args) {
    Supplier<AA> s = ClassFunctionals::f1;
    s.get();
    Comparator<AA> c = ClassFunctionals::f2;
    c.compare(new AA(), new AA());
    Consumer<AA> cons = ClassFunctionals::f3;
    cons.accept(new AA());
    BiConsumer<AA,BB> bicons = ClassFunctionals::f4;
    bicons.accept(new AA(), new BB());
    Function<AA,CC> f = ClassFunctionals::f5;
    CC cc = f.apply(new AA());
    BiFunction<AA,BB,CC> bif = ClassFunctionals::f6;
    cc = bif.apply(new AA(), new BB());
    Predicate<AA> p = ClassFunctionals::f7;
    boolean result = p.test(new AA());
    BiPredicate<AA,BB> bip = ClassFunctionals::f8;
    result = bip.test(new AA(), new BB());
    UnaryOperator<AA> uo = ClassFunctionals::f9;
    AA aa = uo.apply(new AA());
    BinaryOperator<AA> bo = ClassFunctionals::f10;
    aa = bo.apply(new AA(), new AA());
  }
}
```

请**注意**，每个方法名称都是随意的（如 `f1()`，`f2()`等）。正如你刚才看到的，一旦将方法引用赋值给函数接口，我们就可以调用与该接口关联的函数方法。 在此示例中为 `get()`、`compare()`、`accept()`、`apply()` 和 `test()`。

<!-- Functional Interfaces with More Arguments -->

### 多参数函数式接口

`java.util.functional` 中的接口是有限的。比如有 `BiFunction`，但也仅此而已。 如果需要三参数函数的接口怎么办？ 其实这些接口非常简单，很容易查看 Java 库源代码并自行创建。代码示例：

```java
// functional/TriFunction.java

@FunctionalInterface
public interface TriFunction<T, U, V, R> {
    R apply(T t, U u, V v);
}
```

简单测试，验证它是否有效：

```java
// functional/TriFunctionTest.java

public class TriFunctionTest {
  static int f(int i, long l, double d) { return 99; }
  public static void main(String[] args) {
    TriFunction<Integer, Long, Double, Integer> tf =
      TriFunctionTest::f;
    tf = (i, l, d) -> 12;
  }
}
```

这里我们同时测试了方法引用和 Lambda 表达式。

让我们重温一下 `BiConsumer`，看看我们将如何创建各种缺失的预定义组合，涉及 **int**，**long** 和 **double** （基本类型）：

```java
// functional/BiConsumerPermutations.java

import java.util.function.*;

public class BiConsumerPermutations {
  static BiConsumer<Integer, Double> bicid = (i, d) ->
    System.out.format("%d, %f%n", i, d);
  static BiConsumer<Double, Integer> bicdi = (d, i) ->
    System.out.format("%d, %f%n", i, d);
  static BiConsumer<Integer, Long> bicil = (i, l) ->
    System.out.format("%d, %d%n", i, l);
  public static void main(String[] args) {
    bicid.accept(47, 11.34);
    bicdi.accept(22.45, 92);
    bicil.accept(1, 11L);
  }
}
```

输出结果：

```
47, 11.340000
92, 22.450000
1, 11
```

这里使用 `System.out.format()` 来显示。它类似于 `System.out.println()` 但提供了更多的显示选项。 这里，`%f` 表示我将 `n` 作为浮点值给出，`%d` 表示 `n` 是一个整数值。 这其中可以包含空格，输入 `%n` 会换行 — 当然使用传统的 `\n` 也能换行，但 `%n` 是自动跨平台的，这是使用 `format()` 的另一个原因。

上例只是简单使用了合适的包装类型，而装箱和拆箱负责它与基本类型之间的来回转换。 又比如，我们可以将包装类型和`Function`一起使用，而不去用各种针对基本类型的预定义接口。代码示例：

```java
// functional/FunctionWithWrapped.java

import java.util.function.*;

public class FunctionWithWrapped {
  public static void main(String[] args) {
    Function<Integer, Double> fid = i -> (double)i;
    IntToDoubleFunction fid2 = i -> i;
  }
}
```

如果没有强制转换，则会收到错误消息：“Integer cannot be converted to Double”（**Integer** 无法转换为 **Double**），而使用 **IntToDoubleFunction** 就没有此类问题。 **IntToDoubleFunction** 接口的源代码是这样的：

```java
@FunctionalInterface 
public interface IntToDoubleFunction { 
  double applyAsDouble(int value); 
}
```

因为我们可以简单地写 `Function <Integer，Double>` 并产生正常的结果，所以用基本类型（`IntToDoubleFunction`）的唯一理由是可以避免传递参数和返回结果过程中的自动拆装箱，进而提升性能。

似乎是考虑到使用频率，某些函数类型并没有预定义。

当然，如果因为缺少针对基本类型的函数式接口造成了性能问题，你可以轻松编写自己的接口（ 参考 Java 源代码）——尽管这里出现性能瓶颈的可能性不大。

<!-- Higher-Order Functions-->

### 高阶函数


这个名字可能听起来令人生畏，但是：[高阶函数](https://en.wikipedia.org/wiki/Higher-order_function)（Higher-order Function）只是一个消费或产生函数的函数。

我们先来看看如何产生一个函数：

```java
// functional/ProduceFunction.java

import java.util.function.*;

interface
FuncSS extends Function<String, String> {} // [1]

public class ProduceFunction {
  static FuncSS produce() {
    return s -> s.toLowerCase(); // [2]
  }
  public static void main(String[] args) {
    FuncSS f = produce();
    System.out.println(f.apply("YELLING"));
  }
}
```

输出结果：

```
yelling
```

这里，`produce()` 是高阶函数。

**[1]** 使用继承，可以轻松地为专用接口创建别名。

**[2]** 使用 Lambda 表达式，可以轻松地在方法中创建和返回一个函数。

要消费一个函数，消费函数需要在参数列表正确地描述函数类型。代码示例：

```java
// functional/ConsumeFunction.java

import java.util.function.*;

class One {}
class Two {}

public class ConsumeFunction {
  static Two consume(Function<One,Two> onetwo) {
    return onetwo.apply(new One());
  }
  public static void main(String[] args) {
    Two two = consume(one -> new Two());
  }
}
```

当基于消费函数生成新函数时，事情就变得相当有趣了。代码示例如下：

```java
// functional/TransformFunction.java

import java.util.function.*;

class I {
  @Override
  public String toString() { return "I"; }
}

class O {
  @Override
  public String toString() { return "O"; }
}

public class TransformFunction {
  static Function<I,O> transform(Function<I,O> in) {
    return in.andThen(o -> {
      System.out.println(o);
      return o;
    });
  }
  public static void main(String[] args) {
    Function<I,O> f2 = transform(i -> {
      System.out.println(i);
      return new O();
    });
    O o = f2.apply(new I());
  }
}
```

输出结果：

```
I
O
```

在这里，`transform()` 生成一个与传入的函数具有相同签名的函数，但是你可以生成任何你想要的类型。

这里使用到了 `Function` 接口中名为 `andThen()` 的默认方法，该方法专门用于操作函数。 顾名思义，在调用 `in` 函数之后调用 `andThen()`（还有个 `compose()` 方法，它在 `in` 函数之前应用新函数）。 要附加一个 `andThen()` 函数，我们只需将该函数作为参数传递。 `transform()` 产生的是一个新函数，它将 `in` 的动作与 `andThen()` 参数的动作结合起来。

<!-- Closures -->

### 闭包

在上一节的 `ProduceFunction.java` 中，我们从方法中返回 Lambda 函数。 虽然过程简单，但是有些问题必须再回过头来探讨一下。

**闭包**（Closure）一词总结了这些问题。 它非常重要，利用闭包可以轻松生成函数。

考虑一个更复杂的 Lambda，它使用函数作用域之外的变量。 返回该函数会发生什么？ 也就是说，当你调用函数时，它对那些 “外部 ”变量引用了什么?  如果语言不能自动解决，那问题将变得非常棘手。 能够解决这个问题的语言被称作 *支持闭包*，或者称作 *词法定界*（*lexically scoped* ，基于词法作用域的）( 也有用术语 *变量捕获* *variable capture* 称呼的)。Java 8 提供了有限但合理的闭包支持，我们将用一些简单的例子来研究它。

首先，下列方法返回一个函数，该函数访问对象字段和方法参数：

```java
// functional/Closure1.java

import java.util.function.*;

public class Closure1 {
  int i;
  IntSupplier makeFun(int x) {
    return () -> x + i++;
  }
}
```

但是，仔细考虑一下，`i` 的这种用法并非是个大难题，因为对象很可能在你调用 `makeFun()` 之后就存在了——实际上，垃圾收集器几乎肯定会保留以这种方式被绑定到现存函数的对象[^5]。当然，如果你对同一个对象多次调用 `makeFun()` ，你最终会得到多个函数，它们共享 `i` 的存储空间：

```java
// functional/SharedStorage.java

import java.util.function.*;

public class SharedStorage {
  public static void main(String[] args) {
    Closure1 c1 = new Closure1();
    IntSupplier f1 = c1.makeFun(0);
    IntSupplier f2 = c1.makeFun(0);
    IntSupplier f3 = c1.makeFun(0);
    System.out.println(f1.getAsInt());
    System.out.println(f2.getAsInt());
    System.out.println(f3.getAsInt());
  }
}
```

输出结果：

```
0
1
2
```

每次调用 `getAsInt()` 都会增加 `i`，表明存储是共享的。

如果 `i` 是 `makeFun()` 的局部变量怎么办？ 在正常情况下，当 `makeFun()` 完成时 `i` 就消失。 但它仍可以编译：

```java
// functional/Closure2.java

import java.util.function.*;

public class Closure2 {
  IntSupplier makeFun(int x) {
    int i = 0;
    return () -> x + i;
  }
}
```

由 `makeFun()` 返回的 `IntSupplier` “关住了” `i` 和 `x`，因此即使`makeFun()`已执行完毕，当你调用返回的函数时`i` 和 `x`仍然有效，而不是像正常情况下那样在 `makeFun()` 执行后 `i` 和`x`就消失了。 但请注意，我没有像 `Closure1.java` 那样递增 `i`，因为会产生编译时错误。代码示例：

```java
// functional/Closure3.java

// {WillNotCompile}
import java.util.function.*;

public class Closure3 {
  IntSupplier makeFun(int x) {
    int i = 0;
    // x++ 和 i++ 都会报错：
    return () -> x++ + i++;
  }
}
```

`x` 和 `i` 的操作都犯了同样的错误：被 Lambda 表达式引用的局部变量必须是 `final` 或者是等同 `final` 效果的。

如果使用 `final` 修饰 `x`和 `i`，就不能再递增它们的值了。代码示例：

```java
// functional/Closure4.java

import java.util.function.*;

public class Closure4 {
  IntSupplier makeFun(final int x) {
    final int i = 0;
    return () -> x + i;
  }
}
```

那么为什么在 `Closure2.java` 中， `x` 和 `i` 非 `final` 却可以运行呢？

这就叫做**等同 final 效果**（Effectively Final）。这个术语是在 Java 8 才开始出现的，表示虽然没有明确地声明变量是 `final` 的，但是因变量值没被改变过而实际有了 `final` 同等的效果。 如果局部变量的初始值永远不会改变，那么它实际上就是 `final` 的。

如果 `x` 和 `i` 的值在方法中的其他位置发生改变（但不在返回的函数内部），则编译器仍将视其为错误。每个递增操作则会分别产生错误消息。代码示例：

```java
// functional/Closure5.java

// {无法编译成功}
import java.util.function.*;

public class Closure5 {
  IntSupplier makeFun(int x) {
    int i = 0;
    i++;
    x++;
    return () -> x + i;
  }
}
```

**等同 final 效果**意味着可以在变量声明前加上 **final** 关键字而不用更改任何其余代码。 实际上它就是具备 `final` 效果的，只是没有明确说明。

在闭包中，在使用 `x` 和 `i` 之前，通过将它们赋值给 `final` 修饰的变量，我们解决了 `Closure5.java` 中遇到的问题。代码示例：

```java
// functional/Closure6.java

import java.util.function.*;

public class Closure6 {
  IntSupplier makeFun(int x) {
    int i = 0;
    i++;
    x++;
    final int iFinal = i;
    final int xFinal = x;
    return () -> xFinal + iFinal;
  }
}
```

上例中 `iFinal` 和 `xFinal` 的值在赋值后并没有改变过，因此在这里使用 `final` 是多余的。

如果改用包装类型会是什么情况呢？我们可以把`int`类型改为`Integer`类型研究一下：

```java
// functional/Closure7.java

// {无法编译成功}
import java.util.function.*;

public class Closure7 {
  IntSupplier makeFun(int x) {
    Integer i = 0;
    i = i + 1;
    return () -> x + i;
  }
}
```

编译器非常聪明地识别到变量 `i` 的值被更改过。 包装类型可能是被特殊处理了，我们再尝试下 **List**：

```java
// functional/Closure8.java

import java.util.*;
import java.util.function.*;

public class Closure8 {
  Supplier<List<Integer>> makeFun() {
    final List<Integer> ai = new ArrayList<>();
    ai.add(1);
    return () -> ai;
  }
  public static void main(String[] args) {
    Closure8 c7 = new Closure8();
    List<Integer>
      l1 = c7.makeFun().get(),
      l2 = c7.makeFun().get();
    System.out.println(l1);
    System.out.println(l2);
    l1.add(42);
    l2.add(96);
    System.out.println(l1);
    System.out.println(l2);
  }
}
```

输出结果：

```
[1]
[1]
[1, 42]
[1, 96]
```

可以看到，这次一切正常。我们改变了 **List** 的内容却没产生编译时错误。通过观察本例的输出结果，我们发现这看起来非常安全。这是因为每次调用 `makeFun()` 时，其实都会创建并返回一个全新而非共享的 `ArrayList`。也就是说，每个闭包都有自己独立的 `ArrayList`，它们之间互不干扰。

请**注意**我已经声明 `ai` 是 `final` 的了。尽管在这个例子中你可以去掉 `final` 并得到相同的结果（试试吧！）。 应用于对象引用的 `final` 关键字仅表示不会重新赋值引用。 它并不代表你不能修改对象本身。

我们来看看 `Closure7.java` 和 `Closure8.java` 之间的区别。我们看到：在 `Closure7.java` 中变量 `i` 有过重新赋值。 也许这就是触发**等同 final 效果**错误消息的原因。

```java
// functional/Closure9.java

// {无法编译成功}
import java.util.*;
import java.util.function.*;

public class Closure9 {
  Supplier<List<Integer>> makeFun() {
    List<Integer> ai = new ArrayList<>();
    ai = new ArrayList<>(); // Reassignment
    return () -> ai;
  }
}
```

上例，重新赋值引用会触发错误消息。如果只修改指向的对象则没问题，只要没有其他人获得对该对象的引用（这意味着你有多个实体可以修改对象，此时事情会变得非常混乱），基本上就是安全的[^6]。

让我们回顾一下 `Closure1.java`。那么现在问题来了：为什么变量 `i` 被修改编译器却没有报错呢。 它既不是 `final` 的，也不是**等同 final 效果**的。因为 `i` 是外部类的成员，所以这样做肯定是安全的（除非你正在创建共享可变内存的多个函数）。是的，你可以辩称在这种情况下不会发生变量捕获（Variable Capture）。但可以肯定的是，`Closure3.java` 的错误消息是专门针对局部变量的。因此，规则并非只是 “在 Lambda 之外定义的任何变量必须是 `final` 的或**等同 final 效果**” 那么简单。相反，你必须考虑捕获的变量是否是**等同 final 效果**的。 如果它是对象中的字段（实例变量），那么它有独立的生命周期，不需要任何特殊的捕获以便稍后在调用 Lambda 时存在。（注：结论是——Lambda 可以没有限制地引用 实例变量和静态变量。但 局部变量必须显式声明为final，或事实上是final 。）

<!-- Inner Classes as Closures -->

### 作为闭包的内部类

我们可以使用匿名内部类重写之前的例子:

```java
// functional/AnonymousClosure.java

import java.util.function.*;

public class AnonymousClosure {
  IntSupplier makeFun(int x) {
    int i = 0;
    // 同样规则的应用:
    // i++; // 非等同 final 效果
    // x++; // 同上
    return new IntSupplier() {
      public int getAsInt() { return x + i; }
    };
  }
}
```

实际上只要有内部类，就会有闭包（Java 8 只是简化了闭包操作）。在 Java 8 之前，变量 `x` 和 `i` 必须被明确声明为 `final`。在 Java 8 中，内部类的规则放宽，包括**等同 final 效果**。

<!-- Function Composition -->

### 函数组合


函数组合（Function Composition）意为“多个函数组合成新函数”。它通常是函数式编程的基本组成部分。在前面的 `TransformFunction.java` 类中，就有一个使用 `andThen()` 的函数组合示例。一些 `java.util.function` 接口中包含支持函数组合的方法 [^7]。

| 组合方法                                                     | 支持接口                                                     |
| :----------------------------------------------------------- | :----------------------------------------------------------- |
| `andThen(argument)` <br> 执行原操作,再执行参数操作           | **Function <br> BiFunction <br> Consumer <br> BiConsumer <br> IntConsumer <br> LongConsumer <br> DoubleConsumer <br> UnaryOperator <br> IntUnaryOperator <br> LongUnaryOperator <br> DoubleUnaryOperator <br> BinaryOperator** |
| `compose(argument)` <br> 执行参数操作,再执行原操作           | **Function <br> UnaryOperator <br> IntUnaryOperator <br> LongUnaryOperator <br> DoubleUnaryOperator** |
| `and(argument)`  <br> 原谓词(Predicate)和参数谓词的短路**逻辑与** | **Predicate <br> BiPredicate <br> IntPredicate <br> LongPredicate <br> DoublePredicate** |
| `or(argument)` <br> 原谓词和参数谓词的短路**逻辑或**         | **Predicate <br> BiPredicate <br> IntPredicate <br> LongPredicate <br> DoublePredicate** |
| `negate()` <br> 该谓词的**逻辑非**                           | **Predicate <br> BiPredicate <br> IntPredicate <br> LongPredicate <br> DoublePredicate** |


下例使用了 `Function` 里的 `compose()`和 `andThen()`。代码示例：

```java
// functional/FunctionComposition.java

import java.util.function.*;

public class FunctionComposition {
  static Function<String, String>
    f1 = s -> {
      System.out.println(s);
      return s.replace('A', '_');
    },
    f2 = s -> s.substring(3),
    f3 = s -> s.toLowerCase(),
    f4 = f1.compose(f2).andThen(f3);
  public static void main(String[] args) {
    System.out.println(
      f4.apply("GO AFTER ALL AMBULANCES"));
  }
}
```

输出结果：

```
AFTER ALL AMBULANCES
_fter _ll _mbul_nces
```

这里我们重点看正在创建的新函数 `f4`。它调用 `apply()` 的方式与常规几乎无异[^8]。

当 `f1` 获得字符串时，它已经被`f2` 剥离了前三个字符。这是因为 `compose（f2）` 表示 `f2` 的调用发生在 `f1` 之前。

下例是 谓词(`Predicate`) 的逻辑运算演示.代码示例：

```java
// functional/PredicateComposition.java

import java.util.function.*;
import java.util.stream.*;

public class PredicateComposition {
  static Predicate<String>
    p1 = s -> s.contains("bar"),
    p2 = s -> s.length() < 5,
    p3 = s -> s.contains("foo"),
    p4 = p1.negate().and(p2).or(p3);
  public static void main(String[] args) {
    Stream.of("bar", "foobar", "foobaz", "fongopuckey")
      .filter(p4)
      .forEach(System.out::println);
  }
}
```

输出结果：

```
foobar
foobaz
```

`p4` 获取到了所有谓词(`Predicate`)并组合成一个更复杂的谓词。解读：如果字符串中不包含 `bar` 且长度小于 5，或者它包含 `foo` ，则结果为 `true`。

正因它产生如此清晰的语法，我在主方法中采用了一些小技巧，并借用了下一章的内容。首先，我创建了一个字符串对象的流，然后将每个对象传递给 `filter()` 操作。 `filter()` 使用 `p4` 的谓词来确定对象的去留。最后我们使用 `forEach()` 将 `println` 方法引用应用在每个留存的对象上。

从输出结果我们可以看到 `p4` 的工作流程：任何带有 `"foo"` 的字符串都得以保留，即使它的长度大于 5。 `"fongopuckey"` 因长度超出且不包含 `foo` 而被丢弃。

<!-- Currying and  Partial Evaluation -->

### 柯里化和部分求值

[柯里化](https://en.wikipedia.org/wiki/Currying)（Currying）的名称来自于其发明者之一 *Haskell Curry*。他可能是计算机领域唯一姓氏和名字都命名过重要概念的人（另外就是 Haskell 编程语言）。 柯里化意为：将一个多参数的函数，转换为一系列单参数函数。

```java
// functional/CurryingAndPartials.java

import java.util.function.*;

public class CurryingAndPartials {
   // 未柯里化:
   static String uncurried(String a, String b) {
      return a + b;
   }
   public static void main(String[] args) {
      // 柯里化的函数:
      Function<String, Function<String, String>> sum =
         a -> b -> a + b; // [1]

      System.out.println(uncurried("Hi ", "Ho"));

      Function<String, String>
        hi = sum.apply("Hi "); // [2]
      System.out.println(hi.apply("Ho"));

      // 部分应用:
      Function<String, String> sumHi =
        sum.apply("Hup ");
      System.out.println(sumHi.apply("Ho"));
      System.out.println(sumHi.apply("Hey"));
   }
}
```

输出结果：

```
Hi Ho
Hi Ho
Hup Ho
Hup Hey
```

**[1]** 这一连串的箭头很巧妙。*注意*，在函数接口声明中，第二个参数是另一个函数。

**[2]** 柯里化的目的是能够通过提供单个参数来创建一个新函数，所以现在有了一个“带参函数”和剩下的 “自由函数”（free argument） 。实际上，你从一个双参数函数开始，最后得到一个单参数函数。

我们可以通过继续添加层级来柯里化一个三参数函数：

```java
// functional/Curry3Args.java

import java.util.function.*;

public class Curry3Args {
   public static void main(String[] args) {
      Function<String,
        Function<String,
          Function<String, String>>> sum =
            a -> b -> c -> a + b + c;
      Function<String,
        Function<String, String>> hi =
          sum.apply("Hi ");
      Function<String, String> ho =
        hi.apply("Ho ");
      System.out.println(ho.apply("Hup"));
   }
}
```

输出结果：

```
Hi Ho Hup
```

对于每一级的箭头级联（Arrow-cascading），你都会在类型声明周围包裹另一个 **Function** 。

处理基本类型和装箱时，请使用适当的函数式接口：

```java
// functional/CurriedIntAdd.java

import java.util.function.*;

public class CurriedIntAdd {
  public static void main(String[] args) {
    IntFunction<IntUnaryOperator>
      curriedIntAdd = a -> b -> a + b;
    IntUnaryOperator add4 = curriedIntAdd.apply(4);
    System.out.println(add4.applyAsInt(5));
	  }
}
```

输出结果：

```
9
```

可以在互联网上找到更多的柯里化示例。通常它们是用 Java 之外的语言实现的，但如果理解了柯里化的基本概念，你可以很轻松地用 Java 实现它们。

<!-- Pure Functional Programming -->

### 纯函数式编程


只要多加练习，用没有函数式支持的语言也可以写出纯函数式程序（即使是 C 这样的原始语言）。Java 8 让函数式编程更简单，不过我们要确保一切是 `final` 的，同时你的所有方法和函数没有副作用。因为 Java 在本质上并非是不可变语言，所以编译器对我们犯的错误将无能为力。

这种情况下，我们可以借助第三方工具[^9]，但使用 Scala 或 Clojure 这样的语言可能更简单。因为它们从一开始就是为保持不变性而设计的。你可以采用这些语言来编写你的 Java 项目的一部分。如果必须要用纯函数式编写，则可以用 Scala（需要一些练习） 或 Clojure （仅需更少的练习）。虽然 Java 支持[并发编程](./24-Concurrent-Programming.md)，但如果这是你项目的核心部分，你应该考虑在项目部分功能中使用 `Scala` 或 `Clojure` 之类的语言。

<!-- Summary -->

### 本章小结


Lambda 表达式和方法引用并没有将 Java 转换成函数式语言，而是提供了对函数式编程的支持。这对 Java 来说是一个巨大的改进。因为这允许你编写更简洁明了，易于理解的代码。在下一章中，你会看到它们在 *流式编程(streams)* 中的应用。相信你会像我一样，喜欢上流式编程。

这些特性满足了很大一部分的、羡慕Clojure 和 Scala 这类更函数化语言的Java程序员。阻止了他们投奔向那些语言（或者至少让他们在投奔之前做好准备）。

但是，Lambdas 和方法引用远非完美，我们永远要为 Java 设计者早期的草率决定付出代价。特别是没有泛型 Lambda，所以 Lambda 在 Java 中并非一等公民。虽然我不否认 Java 8 的巨大改进，但这意味着和许多 Java 特性一样，它终究还是会让人感觉沮丧和鸡肋。

当你遇到学习困难时，请记住通过 IDE（NetBeans、IntelliJ Idea 和 Eclipse）获得帮助，因为 IDE 可以智能提示你何时使用 Lambda 表达式或方法引用，甚至有时还能为你优化代码。

<!--下面是脚注-->

[^1]: 功能粘贴在一起的方法的确有点与众不同，但它仍不失为一个库。
[^2]: 例如,这个电子书是利用 [Pandoc](http://pandoc.org/) 制作出来的，它是用纯函数式语言 [Haskell](https://www.haskell.org/) 编写的一个程序 。
[^3]: 有时函数式语言将其描述为“代码即数据”。
[^4]: 这个语法来自 C++。
[^5]: 我还没有验证过这种说法。
[^6]: 当你理解了[并发编程](./24-Concurrent-Programming.md)章节的内容，你就能明白为什么更改共享变量 “不是线程安全的” 的了。
[^7]: 接口能够支持方法的原因是它们是 Java 8 默认方法，你将在下一章中了解到。
[^8]: 一些语言，如 Python，允许像调用其他函数一样调用组合函数。但这是 Java，所以我们要量力而行。
[^9]: 例如，[Immutables](https://immutables.github.io/) 和 [Mutability Detector](https://mutabilitydetector.github.io/MutabilityDetector/)。

<!-- 分页 -->

## 流式编程

​	举个例子，假如你要随机展示 5 至 20 之间不重复的整数并进行排序。你要对它们进行排序的事实，会使你首先关注选用哪个有序集合，然后围绕这个集合进行后续的操作。但是使用流式编程，你就可以简单陈述你要做什么：

```java
// streams/Randoms.java
import java.util.*;
public class Randoms {
    public static void main(String[] args) {
        new Random(47)
            .ints(5, 20)
            .distinct()
            .limit(7)
            .sorted()
            .forEach(System.out::println);
    }
}
```

输出

```java
6
10
13
16
17
18
```

​	首先，我们给 **Random** 对象一个种子值47（以便程序再次运行时产生相同的输出）。`ints()` 方法产生一个流并且 `ints()` 方法有多种方式的重载 —— 两个参数限定了产生的数值的边界。这将生成一个随机整数流。我们用 *流的中间操作*（intermediate stream operation） `distinct()` 使流中的整数不重复，然后使用 `limit()` 方法获取前 7 个元素。接下来使用 `sorted()` 方法排序。最终使用 `forEach()` 方法遍历输出，它根据传递给它的函数对流中的每个对象执行操作。在这里，我们传递了一个可以在控制台显示每个元素的方法引用：`System.out::println` 。

​	*声明式编程*（Declarative programming）是一种编程风格——它声明了要做什么，而不是指明（每一步）如何做。而这正是我们在函数式编程中所看到的（编程风格）。你会注意到，命令式（Imperative）编程的形式（指明每一步如何做）会更难理解：

```java
// streams/ImperativeRandoms.java
import java.util.*;
public class ImperativeRandoms {
    public static void main(String[] args) {
        Random rand = new Random(47);
        SortedSet<Integer> rints = new TreeSet<>();
        while(rints.size() < 7) {
            int r = rand.nextInt(20);
            if(r < 5) continue;
            rints.add(r);
        }
        System.out.println(rints);
    }
}
```

出结果：

```
[7, 8, 9, 11, 13, 15, 18]
```

在 `Randoms.java` 中，我们无需定义任何变量，但在这里我们定义了 3 个变量： `rand`，`rints` 和 `r`。由于 `nextInt()` 方法没有下限的原因（其内置的下限永远为 0），这段代码实现起来更复杂。所以我们要生成额外的值来过滤小于 5 的结果。

注意，你必须研究代码才能搞清楚`ImperativeRandoms.java`程序在做什么。而在 `Randoms.java` 中，代码会直接告诉你它在做什么。这种语义的清晰性是使用Java 8 流式编程的重要原因之一。

像在 `ImperativeRandoms.java` 中那样显式地编写迭代过程的方式称为*外部迭代（external iteration）*。而在 `Randoms.java` 中，你看不到任何上述的迭代过程，所以它被称为*内部迭代（internal iteration）*，这是流式编程的一个核心特征。内部迭代产生的代码可读性更强，而且能更简单的使用多核处理器。通过放弃对迭代过程的控制，可以把控制权交给并行化机制。我们将在[并发编程](24-Concurrent-Programming.md)一章中学习这部分内容。

另一个重要方面，流是懒加载的。这代表着它只在绝对必要时才计算。你可以将流看作“延迟列表”。由于计算延迟，流使我们能够表示非常大（甚至无限）的序列，而不需要考虑内存问题。

<!-- Java 8 Stream Support -->

### 流支持

Java 设计者面临着这样一个难题：现存的大量类库不仅为 Java 所用，同时也被应用在整个 Java 生态圈数百万行的代码中。如何将一个全新的流的概念融入到现有类库中呢？

比如在 **Random** 中添加更多的方法。只要不改变原有的方法，现有代码就不会受到干扰。

一个大的挑战来自于使用接口的库。集合类是其中关键的一部分，因为你想把集合转为流。但是如果你将一个新方法添加到接口，那就破坏了每一个实现接口的类，因为这些类都没有实现你添加的新方法。

Java 8 采用的解决方案是：在[接口](10-Interfaces.md)中添加被 `default`（`默认`）修饰的方法。通过这种方案，设计者们可以将流式（*stream*）方法平滑地嵌入到现有类中。流方法预置的操作几乎已满足了我们平常所有的需求。流操作的类型有三种：创建流，修改流元素（中间操作， Intermediate Operations），消费流元素（终端操作， Terminal Operations）。最后一种类型通常意味着收集流元素（通常是汇入一个集合）。

下面我们来看下每种类型的流操作。

<!-- Stream Creation -->

### 流创建

你可以通过 `Stream.of()` 很容易地将一组元素转化成为流（`Bubble` 类在本章的后面定义）：

```java
// streams/StreamOf.java
import java.util.stream.*;
public class StreamOf {
    public static void main(String[] args) {
        Stream.of(new Bubble(1), new Bubble(2), new Bubble(3))
            .forEach(System.out::println);
        Stream.of("It's ", "a ", "wonderful ", "day ", "for ", "pie!")
            .forEach(System.out::print);
        System.out.println();
        Stream.of(3.14159, 2.718, 1.618)
            .forEach(System.out::println);
    }
}
```

输出结果：

```
Bubble(1)
Bubble(2)
Bubble(3)
It's a wonderful day for pie!
3.14159
2.718
1.618
```

除此之外，每个集合都可以通过调用 `stream()` 方法来产生一个流。代码示例：

```java
// streams/CollectionToStream.java
import java.util.*;
import java.util.stream.*;
public class CollectionToStream {
    public static void main(String[] args) {
        List<Bubble> bubbles = Arrays.asList(new Bubble(1), new Bubble(2), new Bubble(3));
        System.out.println(bubbles.stream()
            .mapToInt(b -> b.i)
            .sum());
        
        Set<String> w = new HashSet<>(Arrays.asList("It's a wonderful day for pie!".split(" ")));
        w.stream()
         .map(x -> x + " ")
         .forEach(System.out::print);
        System.out.println();
        
        Map<String, Double> m = new HashMap<>();
        m.put("pi", 3.14159);
        m.put("e", 2.718);
        m.put("phi", 1.618);
        m.entrySet().stream()
                    .map(e -> e.getKey() + ": " + e.getValue())
                    .forEach(System.out::println);
    }
}
```

输出结果：

```
6
a pie! It's for wonderful day
phi: 1.618
e: 2.718
pi: 3.14159
```

在创建 `List<Bubble>` 对象之后，我们只需要简单地调用所有集合中都有的 `stream()`。中间操作 `map()` 会获取流中的所有元素，并且对流中元素应用操作从而产生新的元素，并将其传递到后续的流中。通常 `map()` 会获取对象并产生新的对象，但在这里产生了特殊的用于数值类型的流。例如，`mapToInt()` 方法将一个对象流（object stream）转换成为包含整型数字的 `IntStream`。同样，针对 `Float` 和 `Double` 也有类似名字的操作。

我们通过调用字符串的 `split()`（该方法会根据参数来拆分字符串）来获取元素用于定义变量 `w`。稍后你会知道 `split()` 参数可以是十分复杂，但在这里我们只是根据空格来分割字符串。

为了从 **Map** 集合中产生流数据，我们首先调用 `entrySet()` 产生一个对象流，每个对象都包含一个 `key` 键以及与其相关联的 `value` 值。然后分别调用 `getKey()` 和 `getValue()` 获取值。

#### 随机数流

`Random` 类被一组生成流的方法增强了。代码示例：

```java
// streams/RandomGenerators.java
import java.util.*;
import java.util.stream.*;
public class RandomGenerators {
    public static <T> void show(Stream<T> stream) {
        stream
        .limit(4)
        .forEach(System.out::println);
        System.out.println("++++++++");
    }
    
    public static void main(String[] args) {
        Random rand = new Random(47);
        show(rand.ints().boxed());
        show(rand.longs().boxed());
        show(rand.doubles().boxed());
        // 控制上限和下限：
        show(rand.ints(10, 20).boxed());
        show(rand.longs(50, 100).boxed());
        show(rand.doubles(20, 30).boxed());
        // 控制流大小：
        show(rand.ints(2).boxed());
        show(rand.longs(2).boxed());
        show(rand.doubles(2).boxed());
        // 控制流的大小和界限
        show(rand.ints(3, 3, 9).boxed());
        show(rand.longs(3, 12, 22).boxed());
        show(rand.doubles(3, 11.5, 12.3).boxed());
    }
}
```

输出结果：

```
-1172028779
1717241110
-2014573909
229403722
++++++++
2955289354441303771
3476817843704654257
-8917117694134521474
4941259272818818752
++++++++
0.2613610344283964
0.0508673570556899
0.8037155449603999
0.7620665811558285
++++++++
16
10
11
12
++++++++
65
99
54
58
++++++++
29.86777681078574
24.83968447804611
20.09247112332014
24.046793846338723
++++++++
1169976606
1947946283
++++++++
2970202997824602425
-2325326920272830366
++++++++
0.7024254510631527
0.6648552384607359
++++++++
6
7
7
++++++++
17
12
20
++++++++
12.27872414236691
11.732085449736195
12.196509449817267
++++++++
```

为了消除冗余代码，我创建了一个泛型方法 `show(Stream<T> stream)` （在讲解泛型之前就使用这个特性，确实有点作弊，但是回报是值得的）。类型参数 `T` 可以是任何类型，所以这个方法对 **Integer**、**Long** 和 **Double** 类型都生效。但是 **Random** 类只能生成基本类型 **int**， **long**， **double** 的流。幸运的是， `boxed()` 流操作将会自动地把基本类型包装成为对应的装箱类型，从而使得 `show()` 能够接受流。

我们可以使用 **Random** 为任意对象集合创建 **Supplier**。从文本文件提供字符串对象的例子如下

Cheese.dat 文件内容：

```
// streams/Cheese.dat
Not much of a cheese shop really, is it?
Finest in the district, sir.
And what leads you to that conclusion?
Well, it's so clean.
It's certainly uncontaminated by cheese.
```

我们通过 **File** 类将 Cheese.dat 文件的所有行读取到 `List<String>` 中。代码示例：

```java
// streams/RandomWords.java
import java.util.*;
import java.util.stream.*;
import java.util.function.*;
import java.io.*;
import java.nio.file.*;
public class RandomWords implements Supplier<String> {
    List<String> words = new ArrayList<>();
    Random rand = new Random(47);
    RandomWords(String fname) throws IOException {
        List<String> lines = Files.readAllLines(Paths.get(fname));
        // 略过第一行
        for (String line : lines.subList(1, lines.size())) {
            for (String word : line.split("[ .?,]+"))
                words.add(word.toLowerCase());
        }
    }
    public String get() {
        return words.get(rand.nextInt(words.size()));
    }
    @Override
    public String toString() {
        return words.stream()
            .collect(Collectors.joining(" "));
    }
    public static void main(String[] args) throws Exception {
        System.out.println(
            Stream.generate(new RandomWords("Cheese.dat"))
                .limit(10)
                .collect(Collectors.joining(" ")));
    }
}
```

输出结果：

```
it shop sir the much cheese by conclusion district is
```

在这里可以看到 `split()` 更复杂的运用。在构造器里，每一行都被 `split()` 通过方括号内的空格或其它标点符号分割。在方括号后面的 `+` 表示 `+` 前面的东西可以出现一次或者多次（正则表达式）。

你会发现构造函数使用命令式编程（外部迭代）进行循环。在以后的例子中，你会看到我们是如何去除命令式编程。这种旧的形式虽不是特别糟糕，但使用流会让人感觉更好。

在`toString()` 和`main()`方法中你看到了 `collect()` 操作，它根据参数来结合所有的流元素。当你用 `Collectors.joining()`作为 `collect()` 的参数时，将得到一个`String` 类型的结果，该结果是流中的所有元素被`joining()`的参数隔开。还有很多不同的 `Collectors` 用于产生不同的结果。

在主方法中，我们提前看到了 **Stream.**`generate()` 的用法，它可以把任意  `Supplier<T>` 用于生成 `T` 类型的流。

#### int 类型的范围

`IntStream` 类提供了  `range()` 方法用于生成整型序列的流。编写循环时，这个方法会更加便利：

```java
// streams/Ranges.java
import static java.util.stream.IntStream.*;
public class Ranges {
    public static void main(String[] args) {
        // 传统方法:
        int result = 0;
        for (int i = 10; i < 20; i++)
            result += i;
        System.out.println(result);
        // for-in 循环:
        result = 0;
        for (int i : range(10, 20).toArray())
            result += i;
        System.out.println(result);
        // 使用流:
        System.out.println(range(10, 20).sum());
    }
}
```

输出结果：

```
145
145
145
```

在主方法中的第一种方式是我们传统编写 `for` 循环的方式；第二种方式，我们使用 `range()` 创建了流并将其转化为数组，然后在 `for-in` 代码块中使用。但是，如果你能像第三种方法那样全程使用流是更好的。我们对范围中的数字进行求和。在流中可以很方便的使用 `sum()` 操作求和。

注意 **IntStream.**`range()` 相比 `onjava.Range.range()` 受更多限制。这是由于其可选的第三个参数，后者允许步长大于 1，并且可以从大到小来生成。

实用小功能 `repeat()` 可以用来替换简单的 `for` 循环。代码示例：

```java
// onjava/Repeat.java
package onjava;
import static java.util.stream.IntStream.*;
public class Repeat {
    public static void repeat(int n, Runnable action) {
        range(0, n).forEach(i -> action.run());
    }
}
```

其产生的循环更加清晰：

```java
// streams/Looping.java
import static onjava.Repeat.*;
public class Looping {
    static void hi() {
        System.out.println("Hi!");
    }
    public static void main(String[] args) {
        repeat(3, () -> System.out.println("Looping!"));
        repeat(2, Looping::hi);
    }
}
```

输出结果：

```
Looping!
Looping!
Looping!
Hi!
Hi!
```

原则上，在代码中包含和解释 `repeat()` 并不值得。诚然它是一个相当透明的工具，但这取决于你的团队和公司的运作方式。

#### generate()

参照 `RandomWords.java` 中 **Stream.**`generate()` 搭配 `Supplier<T>` 使用的例子。代码示例：

```java
// streams/Generator.java
import java.util.*;
import java.util.function.*;
import java.util.stream.*;

public class Generator implements Supplier<String> {
    Random rand = new Random(47);
    char[] letters = "ABCDEFGHIJKLMNOPQRSTUVWXYZ".toCharArray();
    
    public String get() {
        return "" + letters[rand.nextInt(letters.length)];
    }
    
    public static void main(String[] args) {
        String word = Stream.generate(new Generator())
                            .limit(30)
                            .collect(Collectors.joining());
        System.out.println(word);
    }
}
```

输出结果：

```
YNZBRNYGCFOWZNTCQRGSEGZMMJMROE
```

使用 `Random.nextInt()` 方法来挑选字母表中的大写字母。`Random.nextInt()` 的参数代表可以接受的最大的随机数范围，所以使用数组边界是经过慎重考虑的。

如果要创建包含相同对象的流，只需要传递一个生成那些对象的 `lambda` 到 `generate()` 中：

```java
// streams/Duplicator.java
import java.util.stream.*;
public class Duplicator {
    public static void main(String[] args) {
        Stream.generate(() -> "duplicate")
              .limit(3)
              .forEach(System.out::println);
    }
}
```

输出结果：

```
duplicate
duplicate
duplicate
```

如下是在本章之前例子中使用过的 `Bubble` 类。**注意**它包含了自己的静态生成器（Static generator）方法。

```java
// streams/Bubble.java
import java.util.function.*;
public class Bubble {
    public final int i;
    
    public Bubble(int n) {
        i = n;
    }
    
    @Override
    public String toString() {
        return "Bubble(" + i + ")";
    }
    
    private static int count = 0;
    public static Bubble bubbler() {
        return new Bubble(count++);
    }
}
```

由于 `bubbler()` 与 `Supplier<Bubble>` 是接口兼容的，我们可以将其方法引用直接传递给 **Stream.**`generate()`：

```java
// streams/Bubbles.java
import java.util.stream.*;
public class Bubbles {
    public static void main(String[] args) {
        Stream.generate(Bubble::bubbler)
              .limit(5)
              .forEach(System.out::println);
    }
}
```

输出结果：

```
Bubble(0)
Bubble(1)
Bubble(2)
Bubble(3)
Bubble(4)
```

这是创建单独工厂类（Separate Factory class）的另一种方式。在很多方面它更加整洁，但是这是一个关于代码组织和品味的问题——你总是可以创建一个完全不同的工厂类。

#### iterate()

`Stream.iterate()` 产生的流的第一个元素是种子（iterate方法的第一个参数），然后将种子传递给方法（iterate方法的第二个参数）。方法运行的结果被添加到流（作为流的下一个元素），并被存储起来，作为下次调用 `iterate()`方法时的第一个参数，以此类推。我们可以利用 `iterate()` 生成一个斐波那契数列（上一章已经遇到过Fibonacci）。代码示例：

```java
// streams/Fibonacci.java
import java.util.stream.*;
public class Fibonacci {
    int x = 1;
    
    Stream<Integer> numbers() {
        return Stream.iterate(0, i -> {
            int result = x + i;
            x = i;
            return result;
        });
    }
    
    public static void main(String[] args) {
        new Fibonacci().numbers()
                       .skip(20) // 跳过前 20 个
                       .limit(10) // 然后取 10 个
                       .forEach(System.out::println);
    }
}
```

输出结果：

```
6765
10946
17711
28657
46368
75025
121393
196418
317811
514229
```

斐波那契数列将数列中最后两个元素进行求和以产生下一个元素。`iterate()` 只能记忆结果，因此我们需要利用一个变量 `x` 追踪另外一个元素。

在主方法中，我们使用了一个之前没有见过的 `skip()` 操作。它根据参数丢弃指定数量的流元素。在这里，我们丢弃了前 20 个元素。

#### 流的建造者模式

在*建造者模式*（Builder design pattern）中，首先创建一个 `builder` 对象，然后将创建流所需的多个信息传递给它，最后`builder` 对象执行”创建“流的操作。**Stream** 库提供了这样的 `Builder`。在这里，我们重新审视文件读取并将其转换成为单词流的过程。代码示例：

```java
// streams/FileToWordsBuilder.java
import java.io.*;
import java.nio.file.*;
import java.util.stream.*;

public class FileToWordsBuilder {
    Stream.Builder<String> builder = Stream.builder();
    
    public FileToWordsBuilder(String filePath) throws Exception {
        Files.lines(Paths.get(filePath))
             .skip(1) // 略过开头的注释行
             .forEach(line -> {
                  for (String w : line.split("[ .?,]+"))
                      builder.add(w);
              });
    }
    
    Stream<String> stream() {
        return builder.build();
    }
    
    public static void main(String[] args) throws Exception {
        new FileToWordsBuilder("Cheese.dat")
            .stream()
            .limit(7)
            .map(w -> w + " ")
            .forEach(System.out::print);
    }
}
```

输出结果：

```
Not much of a cheese shop really
```

**注意**，构造器会添加文件中的所有单词（除了第一行，它是包含文件路径信息的注释），但是其并没有调用 `build()`。只要你不调用 `stream()` 方法，就可以继续向 `builder` 对象中添加单词。

在该类的更完整形式中，你可以添加一个标志位用于查看 `build()` 是否被调用，并且可能的话增加一个可以添加更多单词的方法。在 `Stream.Builder` 调用 `build()` 方法后继续尝试添加单词会产生一个异常。

#### Arrays

`Arrays` 类中含有一个名为 `stream()` 的静态方法用于把数组转换成为流。我们可以重写 `interfaces/Machine.java` 中的主方法用于创建一个流，并将 `execute()` 应用于每一个元素。代码示例：

```java
// streams/Machine2.java
import java.util.*;
import onjava.Operations;
public class Machine2 {
    public static void main(String[] args) {
        Arrays.stream(new Operations[] {
            () -> Operations.show("Bing"),
            () -> Operations.show("Crack"),
            () -> Operations.show("Twist"),
            () -> Operations.show("Pop")
        }).forEach(Operations::execute);
    }
}
```

输出结果：

```
Bing
Crack
Twist
Pop
```

`new Operations[]` 表达式动态创建了 `Operations` 对象的数组。

`stream()` 同样可以产生 **IntStream**，**LongStream** 和 **DoubleStream**。

```java
// streams/ArrayStreams.java
import java.util.*;
import java.util.stream.*;

public class ArrayStreams {
    public static void main(String[] args) {
        Arrays.stream(new double[] { 3.14159, 2.718, 1.618 })
            .forEach(n -> System.out.format("%f ", n));
        System.out.println();
        
        Arrays.stream(new int[] { 1, 3, 5 })
            .forEach(n -> System.out.format("%d ", n));
        System.out.println();
        
        Arrays.stream(new long[] { 11, 22, 44, 66 })
            .forEach(n -> System.out.format("%d ", n));
        System.out.println();
        
        // 选择一个子域:
        Arrays.stream(new int[] { 1, 3, 5, 7, 15, 28, 37 }, 3, 6)
            .forEach(n -> System.out.format("%d ", n));
    }
}
```

输出结果：

```
3.141590 2.718000 1.618000
1 3 5
11 22 44 66
7 15 28
```

最后一次 `stream()` 的调用有两个额外的参数。第一个参数告诉 `stream()` 从数组的哪个位置开始选择元素，第二个参数用于告知在哪里停止。每种不同类型的 `stream()` 都有类似的操作。

#### 正则表达式

Java 的正则表达式将在[字符串](18-Strings.md)这一章节详细介绍。Java 8 在 `java.util.regex.Pattern` 中增加了一个新的方法 `splitAsStream()`。这个方法可以根据传入的公式将字符序列转化为流。但是有一个限制，输入只能是 **CharSequence**，因此不能将流作为 `splitAsStream()` 的参数。

我们再一次查看将文件转换为单词的过程。这一次，我们使用流将文件转换为一个字符串，接着使用正则表达式将字符串转化为单词流。

```java
// streams/FileToWordsRegexp.java
import java.io.*;
import java.nio.file.*;
import java.util.stream.*;
import java.util.regex.Pattern;
public class FileToWordsRegexp {
    private String all;
    public FileToWordsRegexp(String filePath) throws Exception {
        all = Files.lines(Paths.get(filePath))
        .skip(1) // First (comment) line
        .collect(Collectors.joining(" "));
    }
    public Stream<String> stream() {
        return Pattern
        .compile("[ .,?]+").splitAsStream(all);
    }
    public static void
    main(String[] args) throws Exception {
        FileToWordsRegexp fw = new FileToWordsRegexp("Cheese.dat");
        fw.stream()
          .limit(7)
          .map(w -> w + " ")
          .forEach(System.out::print);
        fw.stream()
          .skip(7)
          .limit(2)
          .map(w -> w + " ")
          .forEach(System.out::print);
    }
}
```

输出结果：

```
Not much of a cheese shop really is it
```

在构造器中我们读取了文件中的所有内容（跳过第一行注释，并将其转化成为单行字符串）。现在，当你调用 `stream()` 的时候，可以像往常一样获取一个流，但这回你可以多次调用 `stream()` ，每次从已存储的字符串中创建一个新的流。这里有个限制，整个文件必须存储在内存中；在大多数情况下这并不是什么问题，但是这丢掉了流操作非常重要的优势：

1. “不需要把流存储起来。”当然，流确实需要一些内部存储，但存储的只是序列的一小部分，和存储整个序列不同。
2. 它们是懒加载计算的。

幸运的是，我们稍后就会知道如何解决这个问题。

<!-- Intermediate Operations -->

### 中间操作

中间操作用于从一个流中获取对象，并将对象作为另一个流从后端输出，以连接到其他操作。

#### 跟踪和调试

`peek()` 操作的目的是帮助调试。它允许你无修改地查看流中的元素。代码示例：

```java
// streams/Peeking.java
class Peeking {
    public static void main(String[] args) throws Exception {
        FileToWords.stream("Cheese.dat")
        .skip(21)
        .limit(4)
        .map(w -> w + " ")
        .peek(System.out::print)
        .map(String::toUpperCase)
        .peek(System.out::print)
        .map(String::toLowerCase)
        .forEach(System.out::print);
    }
}
```

输出结果：

```
Well WELL well it IT it s S s so SO so
```

`FileToWords` 稍后定义，但它的功能实现貌似和之前我们看到的差不多：产生字符串对象的流。之后在其通过管道时调用 `peek()` 进行处理。

因为 `peek()` 符合无返回值的 **Consumer** 函数式接口，所以我们只能观察，无法使用不同的元素来替换流中的对象。

#### 流元素排序

在 `Randoms.java` 中，我们熟识了 `sorted()` 的默认比较器实现。其实它还有另一种形式的实现：传入一个 **Comparator** 参数。代码示例：

```java
// streams/SortedComparator.java
import java.util.*;
public class SortedComparator {
    public static void main(String[] args) throws Exception {
        FileToWords.stream("Cheese.dat")
        .skip(10)
        .limit(10)
        .sorted(Comparator.reverseOrder())
        .map(w -> w + " ")
        .forEach(System.out::print);
    }
}
```

输出结果：

```
you what to the that sir leads in district And
```

`sorted()` 预设了一些默认的比较器。这里我们使用的是反转“自然排序”。当然你也可以把 Lambda 函数作为参数传递给 `sorted()`。

#### 移除元素

* `distinct()`：在 `Randoms.java` 类中的 `distinct()` 可用于消除流中的重复元素。相比创建一个 **Set** 集合来消除重复，该方法的工作量要少得多。

* `filter(Predicate)`：过滤操作，保留如下元素：若元素传递给过滤函数产生的结果为`true` 。

在下例中，`isPrime()` 作为过滤函数，用于检测质数。

```java
// streams/Prime.java
import java.util.stream.*;
import static java.util.stream.LongStream.*;
public class Prime {
    public static Boolean isPrime(long n) {
        return rangeClosed(2, (long)Math.sqrt(n))
        .noneMatch(i -> n % i == 0);
    }
    public LongStream numbers() {
        return iterate(2, i -> i + 1)
        .filter(Prime::isPrime);
    }
    public static void main(String[] args) {
        new Prime().numbers()
        .limit(10)
        .forEach(n -> System.out.format("%d ", n));
        System.out.println();
        new Prime().numbers()
        .skip(90)
        .limit(10)
        .forEach(n -> System.out.format("%d ", n));
    }
}
```

输出结果：

```
2 3 5 7 11 13 17 19 23 29
467 479 487 491 499 503 509 521 523 541
```

`rangeClosed()` 包含了上限值。如果不能整除，即余数不等于 0，则 `noneMatch()` 操作返回 `true`，如果出现任何等于 0 的结果则返回 `false`。 `noneMatch()` 操作一旦有失败就会退出。

#### 应用函数到元素

- `map(Function)`：将函数操作应用在输入流的元素中，并将返回值传递到输出流中。

- `mapToInt(ToIntFunction)`：操作同上，但结果是 **IntStream**。

- `mapToLong(ToLongFunction)`：操作同上，但结果是 **LongStream**。

- `mapToDouble(ToDoubleFunction)`：操作同上，但结果是 **DoubleStream**。

在这里，我们使用 `map()` 映射多种函数到一个字符串流中。代码示例：

```java
// streams/FunctionMap.java
import java.util.*;
import java.util.stream.*;
import java.util.function.*;
class FunctionMap {
    static String[] elements = { "12", "", "23", "45" };
    static Stream<String>
    testStream() {
        return Arrays.stream(elements);
    }
    static void test(String descr, Function<String, String> func) {
        System.out.println(" ---( " + descr + " )---");
        testStream()
        .map(func)
        .forEach(System.out::println);
    }
    public static void main(String[] args) {
        test("add brackets", s -> "[" + s + "]");
        test("Increment", s -> {
            try {
                return Integer.parseInt(s) + 1 + "";
            }
            catch(NumberFormatException e) {
                return s;
            }
        }
        );
        test("Replace", s -> s.replace("2", "9"));
        test("Take last digit", s -> s.length() > 0 ?
        s.charAt(s.length() - 1) + "" : s);
    }
}
```

输出结果：

```
 ---( add brackets )---
[12]
[]
[23]
[45]
 ---( Increment )---
13

24
46
 ---( Replace )---
19

93
45
 ---( Take last digit )---
2

3
5

```

在上面的自增示例中，我们用 `Integer.parseInt()` 尝试将一个字符串转化为整数。如果字符串不能被转化成为整数就会抛出 `NumberFormatException` 异常，此时我们就回过头来把原始字符串放到输出流中。

在以上例子中，`map()` 将一个字符串映射为另一个字符串，但是我们完全可以产生和接收类型完全不同的类型，从而改变流的数据类型。下面代码示例：

```java
// streams/FunctionMap2.java
// Different input and output types （不同的输入输出类型）
import java.util.*;
import java.util.stream.*;
class Numbered {
    final int n;
    Numbered(int n) {
        this.n = n;
    }
    @Override
    public String toString() {
        return "Numbered(" + n + ")";
    }
}
class FunctionMap2 {
    public static void main(String[] args) {
        Stream.of(1, 5, 7, 9, 11, 13)
        .map(Numbered::new)
        .forEach(System.out::println);
    }
}
```

输出结果：

```
Numbered(1)
Numbered(5)
Numbered(7)
Numbered(9)
Numbered(11)
Numbered(13)
```

我们将获取到的整数通过构造器 `Numbered::new` 转化成为 `Numbered` 类型。

如果使用 **Function** 返回的结果是数值类型的一种，我们必须使用合适的 `mapTo数值类型` 进行替代。代码示例：

```java
// streams/FunctionMap3.java
// Producing numeric output streams（ 产生数值输出流）
import java.util.*;
import java.util.stream.*;
class FunctionMap3 {
    public static void main(String[] args) {
        Stream.of("5", "7", "9")
        .mapToInt(Integer::parseInt)
        .forEach(n -> System.out.format("%d ", n));
        System.out.println();
        Stream.of("17", "19", "23")
        .mapToLong(Long::parseLong)
        .forEach(n -> System.out.format("%d ", n));
        System.out.println();
        Stream.of("17", "1.9", ".23")
        .mapToDouble(Double::parseDouble)
        .forEach(n -> System.out.format("%f ", n));
    }
}
```

输出结果：

```
5 7 9
17 19 23
17.000000 1.900000 0.230000
```

遗憾的是，Java 设计者并没有尽最大努力去消除基本类型。

#### 在 `map()` 中组合流

假设我们现在有了一个传入的元素流，并且打算对流元素使用 `map()` 函数。现在你已经找到了一些可爱并独一无二的函数功能，但是问题来了：这个函数功能是产生一个流。我们想要产生一个元素流，而实际却产生了一个元素流的流。

`flatMap()` 做了两件事：将产生流的函数应用在每个元素上（与 `map()` 所做的相同），然后将每个流都扁平化为元素，因而最终产生的仅仅是元素。

`flatMap(Function)`：当 `Function` 产生流时使用。

`flatMapToInt(Function)`：当 `Function` 产生 `IntStream` 时使用。

`flatMapToLong(Function)`：当 `Function` 产生 `LongStream` 时使用。

`flatMapToDouble(Function)`：当 `Function` 产生 `DoubleStream` 时使用。

为了弄清它的工作原理，我们从传入一个刻意设计的函数给  `map()` 开始。该函数接受一个整数并产生一个字符串流：

```java
// streams/StreamOfStreams.java
import java.util.stream.*;
public class StreamOfStreams {
    public static void main(String[] args) {
        Stream.of(1, 2, 3)
        .map(i -> Stream.of("Gonzo", "Kermit", "Beaker"))
        .map(e-> e.getClass().getName())
        .forEach(System.out::println);
    }
}
```

输出结果：

```
java.util.stream.ReferencePipeline$Head
java.util.stream.ReferencePipeline$Head
java.util.stream.ReferencePipeline$Head
```

我们天真地希望能够得到字符串流，但实际得到的却是“Head”流的流。我们可以使用 `flatMap()` 解决这个问题：

```java
// streams/FlatMap.java
import java.util.stream.*;
public class FlatMap {
    public static void main(String[] args) {
        Stream.of(1, 2, 3)
        .flatMap(i -> Stream.of("Gonzo", "Fozzie", "Beaker"))
        .forEach(System.out::println);
    }
}
```

输出结果：

```
Gonzo
Fozzie
Beaker
Gonzo
Fozzie
Beaker
Gonzo
Fozzie
Beaker
```

从映射返回的每个流都会自动扁平为组成它的字符串。

下面是另一个演示，我们从一个整数流开始，然后使用每一个整数去创建更多的随机数。

```java
// streams/StreamOfRandoms.java
import java.util.*;
import java.util.stream.*;
public class StreamOfRandoms {
    static Random rand = new Random(47);
    public static void main(String[] args) {
        Stream.of(1, 2, 3, 4, 5)
            .flatMapToInt(i -> IntStream.concat(
        rand.ints(0, 100).limit(i), IntStream.of(-1)))
            .forEach(n -> System.out.format("%d ", n));
    }
}
```

输出结果：

```
58 -1 55 93 -1 61 61 29 -1 68 0 22 7 -1 88 28 51 89 9 -1
```

在这里我们引入了 `concat()`，它以参数顺序组合两个流。 如此，我们在每个随机 `Integer` 流的末尾添加一个 -1 作为标记。你可以看到最终流确实是从一组扁平流中创建的。

因为 `rand.ints()` 产生的是一个 `IntStream`，所以我必须使用 `flatMap()`、`concat()` 和 `of()` 的特定整数形式。

让我们再看一下将文件划分为单词流的任务。我们最后使用到的是 **FileToWordsRegexp.java**，它的问题是需要将整个文件读入行列表中 —— 显然需要存储该列表。而我们真正想要的是创建一个不需要中间存储层的单词流。

下面，我们再使用 ` flatMap()` 来解决这个问题：

```java
// streams/FileToWords.java
import java.nio.file.*;
import java.util.stream.*;
import java.util.regex.Pattern;
public class FileToWords {
    public static Stream<String> stream(String filePath) throws Exception {
        return Files.lines(Paths.get(filePath))
        .skip(1) // First (comment) line
        .flatMap(line ->
        Pattern.compile("\\W+").splitAsStream(line));
    }
}
```

`stream()` 现在是一个静态方法，因为它可以自己完成整个流创建过程。

注意：`\\W+` 是一个正则表达式。表示“非单词字符”，`+` 表示“可以出现一次或者多次”。小写形式的 `\\w` 表示“单词字符”。

我们之前遇到的问题是 `Pattern.compile().splitAsStream()` 产生的结果为流，这意味着当我们只是想要一个简单的单词流时，在传入的行流（stream of lines）上调用 `map()` 会产生一个单词流的流。幸运的是，`flatMap()`  可以将元素流的流扁平化为一个简单的元素流。或者，我们可以使用 `String.split()` 生成一个数组，其可以被 `Arrays.stream()` 转化成为流：

```java
.flatMap(line -> Arrays.stream(line.split("\\W+"))))
```

因为有了真正的流（而不是`FileToWordsRegexp.java` 中基于集合存储的流），所以每次需要一个新的流时，我们都必须从头开始创建，因为流不能被复用：

```java
// streams/FileToWordsTest.java
import java.util.stream.*;
public class FileToWordsTest {
    public static void main(String[] args) throws Exception {
        FileToWords.stream("Cheese.dat")
        .limit(7)
        .forEach(s -> System.out.format("%s ", s));
        System.out.println();
        FileToWords.stream("Cheese.dat")
        .skip(7)
        .limit(2)
        .forEach(s -> System.out.format("%s ", s));
    }
}
```

输出结果：

```
Not much of a cheese shop really
is it
```

在 `System.out.format()` 中的 `%s` 表明参数为 **String** 类型。

<!-- Optional -->

### Optional类

在我们学习终端操作（Terminal Operations）之前，我们必须考虑在一个空流中获取元素会发生什么。我们喜欢沿着“快乐路径”[^1]把流连接起来，同时假设流不会中断。然而，在流中放置 `null` 却会轻易令其中断。那么是否存在某种对象，可以在持有流元素的同时，即使在我们查找的元素不存在时，也能友好地对我们进行提示（也就是说，不会产生异常）？

**Optional** 可以实现这样的功能。一些标准流操作返回 **Optional** 对象，因为它们并不能保证预期结果一定存在。包括：

- `findFirst()` 返回一个包含第一个元素的 **Optional** 对象，如果流为空则返回 **Optional.empty**
- `findAny()` 返回包含任意元素的 **Optional** 对象，如果流为空则返回 **Optional.empty**
- `max()` 和 `min()` 返回一个包含最大值或者最小值的 **Optional** 对象，如果流为空则返回 **Optional.empty**

 `reduce()` 不再以 `identity` 形式开头，而是将其返回值包装在 **Optional** 中。（`identity` 对象成为其他形式的 `reduce()` 的默认结果，因此不存在空结果的风险）

对于数字流 **IntStream**、**LongStream** 和 **DoubleStream**，`average()` 会将结果包装在 **Optional** 以防止流为空。

以下是对空流进行所有这些操作的简单测试：

```java
// streams/OptionalsFromEmptyStreams.java
import java.util.*;
import java.util.stream.*;
class OptionalsFromEmptyStreams {
    public static void main(String[] args) {
        System.out.println(Stream.<String>empty()
             .findFirst());
        System.out.println(Stream.<String>empty()
             .findAny());
        System.out.println(Stream.<String>empty()
             .max(String.CASE_INSENSITIVE_ORDER));
        System.out.println(Stream.<String>empty()
             .min(String.CASE_INSENSITIVE_ORDER));
        System.out.println(Stream.<String>empty()
             .reduce((s1, s2) -> s1 + s2));
        System.out.println(IntStream.empty()
             .average());
    }
}
```

输出结果：

```
Optional.empty
Optional.empty
Optional.empty
Optional.empty
Optional.empty
OptionalDouble.empty
```

当流为空的时候你会获得一个 **Optional.empty** 对象，而不是抛出异常。**Optional** 拥有 `toString()` 方法可以用于展示有用信息。

注意，空流是通过 `Stream.<String>empty()` 创建的。如果你在没有任何上下文环境的情况下调用 `Stream.empty()`，Java 并不知道它的数据类型；这个语法解决了这个问题。如果编译器拥有了足够的上下文信息，比如：

```java
Stream<String> s = Stream.empty();
```

就可以在调用 `empty()` 时推断类型。

这个示例展示了 **Optional** 的两个基本用法：

```java
// streams/OptionalBasics.java
import java.util.*;
import java.util.stream.*;
class OptionalBasics {
    static void test(Optional<String> optString) {
        if(optString.isPresent())
            System.out.println(optString.get()); 
        else
            System.out.println("Nothing inside!");
    }
    public static void main(String[] args) {
        test(Stream.of("Epithets").findFirst());
        test(Stream.<String>empty().findFirst());
    }
}
```

输出结果：

```
Epithets
Nothing inside!
```

当你接收到 **Optional** 对象时，应首先调用 `isPresent()` 检查其中是否包含元素。如果存在，可使用 `get()` 获取。

<!-- Convenience Functions -->

#### 便利函数

有许多便利函数可以解包 **Optional** ，这简化了上述“对所包含的对象的检查和执行操作”的过程：

- `ifPresent(Consumer)`：当值存在时调用 **Consumer**，否则什么也不做。
- `orElse(otherObject)`：如果值存在则直接返回，否则生成 **otherObject**。
- `orElseGet(Supplier)`：如果值存在则直接返回，否则使用 **Supplier** 函数生成一个可替代对象。
- `orElseThrow(Supplier)`：如果值存在直接返回，否则使用 **Supplier** 函数生成一个异常。

如下是针对不同便利函数的简单演示：

```java
// streams/Optionals.java
import java.util.*;
import java.util.stream.*;
import java.util.function.*;
public class Optionals {
    static void basics(Optional<String> optString) {
        if(optString.isPresent())
            System.out.println(optString.get()); 
        else
            System.out.println("Nothing inside!");
    }
    static void ifPresent(Optional<String> optString) {
        optString.ifPresent(System.out::println);
    }
    static void orElse(Optional<String> optString) {
        System.out.println(optString.orElse("Nada"));
    }
    static void orElseGet(Optional<String> optString) {
        System.out.println(
        optString.orElseGet(() -> "Generated"));
    }
    static void orElseThrow(Optional<String> optString) {
        try {
            System.out.println(optString.orElseThrow(
            () -> new Exception("Supplied")));
        } catch(Exception e) {
            System.out.println("Caught " + e);
        }
    }
    static void test(String testName, Consumer<Optional<String>> cos) {
        System.out.println(" === " + testName + " === ");
        cos.accept(Stream.of("Epithets").findFirst());
        cos.accept(Stream.<String>empty().findFirst());
    }
    public static void main(String[] args) {
        test("basics", Optionals::basics);
        test("ifPresent", Optionals::ifPresent);
        test("orElse", Optionals::orElse);
        test("orElseGet", Optionals::orElseGet);
        test("orElseThrow", Optionals::orElseThrow);
    }
}
```

输出结果：

```
=== basics ===
Epithets
Nothing inside!
=== ifPresent ===
Epithets
=== orElse ===
Epithets
Nada
=== orElseGet ===
Epithets
Generated
=== orElseThrow ===
Epithets
Caught java.lang.Exception: Supplied
```

`test()` 通过传入所有方法都适用的 **Consumer** 来避免重复代码。

`orElseThrow()` 通过 **catch** 关键字来捕获抛出的异常。更多细节，将在 [异常](./15-Exceptions.md) 这一章节中学习。

<!-- Creating Optionals -->

#### 创建 Optional

当我们在自己的代码中加入 **Optional** 时，可以使用下面 3 个静态方法：

- `empty()`：生成一个空 **Optional**。
- `of(value)`：将一个非空值包装到 **Optional** 里。
- `ofNullable(value)`：针对一个可能为空的值，为空时自动生成 **Optional.empty**，否则将值包装在 **Optional** 中。

下面来看看它是如何工作的。代码示例：

```java
// streams/CreatingOptionals.java
import java.util.*;
import java.util.stream.*;
import java.util.function.*;
class CreatingOptionals {
    static void test(String testName, Optional<String> opt) {
        System.out.println(" === " + testName + " === ");
        System.out.println(opt.orElse("Null"));
    }
    public static void main(String[] args) {
        test("empty", Optional.empty());
        test("of", Optional.of("Howdy"));
        try {
            test("of", Optional.of(null));
        } catch(Exception e) {
            System.out.println(e);
        }
        test("ofNullable", Optional.ofNullable("Hi"));
        test("ofNullable", Optional.ofNullable(null));
    }
}
```

输出结果：

```
=== empty ===
Null
=== of ===
Howdy
java.lang.NullPointerException
=== ofNullable ===
Hi
=== ofNullable ===
Null
```

我们不能通过传递 `null` 到 `of()` 来创建 `Optional` 对象。最安全的方法是， 使用 `ofNullable()` 来优雅地处理 `null`。

#### Optional 对象操作

当我们的流管道生成了 **Optional** 对象，下面 3 个方法可使得 **Optional** 的后续能做更多的操作：

- `filter(Predicate)`：对 **Optional** 中的内容应用**Predicate** 并将结果返回。如果 **Optional** 不满足 **Predicate** ，将 **Optional** 转化为空 **Optional** 。如果 **Optional** 已经为空，则直接返回空**Optional** 。

- `map(Function)`：如果 **Optional** 不为空，应用 **Function**  于 **Optional** 中的内容，并返回结果。否则直接返回 **Optional.empty**。

- `flatMap(Function)`：同 `map()`，但是提供的映射函数将结果包装在 **Optional** 对象中，因此 `flatMap()` 不会在最后进行任何包装。

以上方法都不适用于数值型 **Optional**。一般来说，流的 `filter()` 会在 **Predicate** 返回 `false` 时移除流元素。而 `Optional.filter()` 在失败时不会删除 **Optional**，而是将其保留下来，并转化为空。下面请看代码示例：


```java
// streams/OptionalFilter.java
import java.util.*;
import java.util.stream.*;
import java.util.function.*;
class OptionalFilter {
    static String[] elements = {
            "Foo", "", "Bar", "Baz", "Bingo"
    };
    static Stream<String> testStream() {
        return Arrays.stream(elements);
    }
    static void test(String descr, Predicate<String> pred) {
        System.out.println(" ---( " + descr + " )---");
        for(int i = 0; i <= elements.length; i++) {
            System.out.println(
                    testStream()
                            .skip(i)
                            .findFirst()
                            .filter(pred));
        }
    }
    public static void main(String[] args) {
        test("true", str -> true);
        test("false", str -> false);
        test("str != \"\"", str -> str != "");
        test("str.length() == 3", str -> str.length() == 3);
        test("startsWith(\"B\")",
                str -> str.startsWith("B"));
    }
}
```

输出结果：

```
---( true )---
Optional[Foo]
Optional[]
Optional[Bar]
Optional[Baz]
Optional[Bingo]
Optional.empty
---( false )---
Optional.empty
Optional.empty
Optional.empty
Optional.empty
Optional.empty
Optional.empty
---( str != "" )---
Optional[Foo]
Optional.empty
Optional[Bar]
Optional[Baz]
Optional[Bingo]
Optional.empty
---( str.length() == 3 )---
Optional[Foo]
Optional.empty
Optional[Bar]
Optional[Baz]
Optional.empty
Optional.empty
---( startsWith("B") )---
Optional.empty
Optional.empty
Optional[Bar]
Optional[Baz]
Optional[Bingo]
Optional.empty
```

即使输出看起来像流，要特别注意 `test()` 中的 for 循环。每一次的for循环都重新启动流，然后跳过for循环索引指定的数量的元素，这就是流只剩后续元素的原因。然后调用`findFirst()` 获取剩余元素中的第一个元素，并包装在一个 `Optional`对象中。

**注意**，不同于普通 for 循环，这里的索引值范围并不是 `i < elements.length`， 而是 `i <= elements.length`。所以最后一个元素实际上超出了流。方便的是，这将自动成为 **Optional.empty**，你可以在每一个测试的结尾中看到。

同 `map()` 一样 ， `Optional.map()` 执行一个函数。它仅在 **Optional** 不为空时才执行这个映射函数。并将 **Optional** 的内容提取出来，传递给映射函数。代码示例：

```java
// streams/OptionalMap.java
import java.util.Arrays;
import java.util.function.Function;
import java.util.stream.Stream;

class OptionalMap {
    static String[] elements = {"12", "", "23", "45"};

    static Stream<String> testStream() {
        return Arrays.stream(elements);
    }

    static void test(String descr, Function<String, String> func) {
        System.out.println(" ---( " + descr + " )---");
        for (int i = 0; i <= elements.length; i++) {
            System.out.println(
                    testStream()
                            .skip(i)
                            .findFirst() // Produces an Optional
                            .map(func));
        }
    }

    public static void main(String[] args) {
        // If Optional is not empty, map() first extracts
        // the contents which it then passes
        // to the function:
        test("Add brackets", s -> "[" + s + "]");
        test("Increment", s -> {
            try {
                return Integer.parseInt(s) + 1 + "";
            } catch (NumberFormatException e) {
                return s;
            }
        });
        test("Replace", s -> s.replace("2", "9"));
        test("Take last digit", s -> s.length() > 0 ?
                s.charAt(s.length() - 1) + "" : s);
    }
    // After the function is finished, map() wraps the
    // result in an Optional before returning it:
}
```

输出结果：

```
---( Add brackets )---
Optional[[12]]
Optional[[]]
Optional[[23]]
Optional[[45]]
Optional.empty
---( Increment )---
Optional[13]
Optional[]
Optional[24]
Optional[46]
Optional.empty
---( Replace )---
Optional[19]
Optional[]
Optional[93]
Optional[45]
Optional.empty
---( Take last digit )---
Optional[2]
Optional[]
Optional[3]
Optional[5]
Optional.empty
```

映射函数的返回结果会自动包装成为 **Optional**。**Optional.empty** 会被直接跳过。

**Optional** 的 `flatMap()` 应用于已生成 **Optional** 的映射函数，所以 `flatMap()` 不会像 `map()` 那样将结果封装在 **Optional** 中。代码示例：

```java
// streams/OptionalFlatMap.java
import java.util.Arrays;
import java.util.Optional;
import java.util.function.Function;
import java.util.stream.Stream;

class OptionalFlatMap {
    static String[] elements = {"12", "", "23", "45"};

    static Stream<String> testStream() {
        return Arrays.stream(elements);
    }

    static void test(String descr,
                     Function<String, Optional<String>> func) {
        System.out.println(" ---( " + descr + " )---");
        for (int i = 0; i <= elements.length; i++) {
            System.out.println(
                    testStream()
                            .skip(i)
                            .findFirst()
                            .flatMap(func));
        }
    }

    public static void main(String[] args) {
        test("Add brackets",
                s -> Optional.of("[" + s + "]"));
        test("Increment", s -> {
            try {
                return Optional.of(
                        Integer.parseInt(s) + 1 + "");
            } catch (NumberFormatException e) {
                return Optional.of(s);
            }
        });
        test("Replace",
                s -> Optional.of(s.replace("2", "9")));
        test("Take last digit",
                s -> Optional.of(s.length() > 0 ?
                        s.charAt(s.length() - 1) + ""
                        : s));
    }
}
```

输出结果：

```
---( Add brackets )---
Optional[[12]]
Optional[[]]
Optional[[23]]
Optional[[45]]
Optional.empty
 ---( Increment )---
Optional[13]
Optional[]
Optional[24]
Optional[46]
Optional.empty
 ---( Replace )---
Optional[19]
Optional[]
Optional[93]
Optional[45]
Optional.empty
 ---( Take last digit )---
Optional[2]
Optional[]
Optional[3]
Optional[5]
Optional.empty
```

同 `map()`，`flatMap()` 将提取非空 **Optional** 的内容并将其应用在映射函数。唯一的区别就是 `flatMap()` 不会把结果包装在 **Optional** 中，因为映射函数已经被包装过了。在如上示例中，我们已经在每一个映射函数中显式地完成了包装，但是很显然 `Optional.flatMap()` 是为那些自己已经生成 **Optional** 的函数而设计的。

<!-- Streams of Optionals -->

#### Optional 流

假设你的生成器可能产生 `null` 值，那么当用它来创建流时，你会自然地想到用  **Optional** 来包装元素。如下是它的样子，代码示例：

```java
// streams/Signal.java
import java.util.*;
import java.util.stream.*;
import java.util.function.*;
public class Signal {
    private final String msg;
    public Signal(String msg) { this.msg = msg; }
    public String getMsg() { return msg; }
    @Override
    public String toString() {
        return "Signal(" + msg + ")";
    }
    static Random rand = new Random(47);
    public static Signal morse() {
        switch(rand.nextInt(4)) {
            case 1: return new Signal("dot");
            case 2: return new Signal("dash");
            default: return null;
        }
    }
    public static Stream<Optional<Signal>> stream() {
        return Stream.generate(Signal::morse)
                .map(signal -> Optional.ofNullable(signal));
    }
}
```

当我们使用这个流的时候，必须要弄清楚如何解包 **Optional**。代码示例：

```java
// streams/StreamOfOptionals.java
import java.util.*;
import java.util.stream.*;
public class StreamOfOptionals {
    public static void main(String[] args) {
        Signal.stream()
                .limit(10)
                .forEach(System.out::println);
        System.out.println(" ---");
        Signal.stream()
                .limit(10)
                .filter(Optional::isPresent)
                .map(Optional::get)
                .forEach(System.out::println);
    }
}
```

输出结果：

```java
Optional[Signal(dash)]
Optional[Signal(dot)]
Optional[Signal(dash)]
Optional.empty
Optional.empty
Optional[Signal(dash)]
Optional.empty
Optional[Signal(dot)]
Optional[Signal(dash)]
Optional[Signal(dash)]
---
Signal(dot)
Signal(dot)
Signal(dash)
Signal(dash)
```

在这里，我们使用 `filter()` 来保留那些非空 **Optional**，然后在 `map()` 中使用 `get()` 获取元素。由于每种情况都需要定义“空值”的含义，所以通常我们要为每个应用程序采用不同的方法。

<!-- Terminal Operations -->

### 终端操作


以下操作将会获取流的最终结果。至此我们无法再继续往后传递流。可以说，终端操作（Terminal Operations）总是我们在流管道中所做的最后一件事。

<!-- Convert to an Array -->

#### 数组

- `toArray()`：将流转换成适当类型的数组。
- `toArray(generator)`：在特殊情况下，生成自定义类型的数组。

当我们需要得到数组类型的数据以便于后续操作时，上面的方法就很有用。假设我们需要复用流产生的随机数时，就可以这么使用。代码示例:

```java
// streams/RandInts.java
package streams;
import java.util.*;
import java.util.stream.*;
public class RandInts {
    private static int[] rints = new Random(47).ints(0, 1000).limit(100).toArray();
    public static IntStream rands() {
        return Arrays.stream(rints);
    }
}
```

上例将100个数值范围在 0 到 1000 之间的随机数流转换成为数组并将其存储在 `rints` 中。这样一来，每次调用 `rands()` 的时候可以重复获取相同的整数流。

<!-- Apply a Final Operation to Every Element -->

#### 循环

- `forEach(Consumer)`常见如 `System.out::println` 作为 **Consumer** 函数。
- `forEachOrdered(Consumer)`： 保证 `forEach` 按照原始流顺序操作。

第一种形式：无序操作，仅在引入并行流时才有意义。在 [并发编程](24-Concurrent-Programming.md) 章节之前我们不会深入研究这个问题。这里简单介绍下 `parallel()`：可实现多处理器并行操作。实现原理为将流分割为多个（通常数目为 CPU 核心数）并在不同处理器上分别执行操作。因为我们采用的是内部迭代，而不是外部迭代，所以这是可能实现的。

`parallel()` 看似简单，实则棘手。更多内容将在稍后的 [并发编程](24-Concurrent-Programming.md) 章节中学习。

下例引入 `parallel()` 来帮助理解 `forEachOrdered(Consumer)` 的作用和使用场景。代码示例：

```java
// streams/ForEach.java
import java.util.*;
import java.util.stream.*;
import static streams.RandInts.*;
public class ForEach {
    static final int SZ = 14;
    public static void main(String[] args) {
        rands().limit(SZ)
                .forEach(n -> System.out.format("%d ", n));
        System.out.println();
        rands().limit(SZ)
                .parallel()
                .forEach(n -> System.out.format("%d ", n));
        System.out.println();
        rands().limit(SZ)
                .parallel()
                .forEachOrdered(n -> System.out.format("%d ", n));
    }
}
```

输出结果：

```
258 555 693 861 961 429 868 200 522 207 288 128 551 589
551 861 429 589 200 522 555 693 258 128 868 288 961 207
258 555 693 861 961 429 868 200 522 207 288 128 551 589
```

为了方便测试不同大小的流，我们抽离出了 `SZ` 变量。然而即使 `SZ` 值为14也产生了有趣的结果。在第一个流中，未使用 `parallel()` ，因此以元素从 `rands()`出来的顺序输出结果。在第二个流中，引入`parallel()` ，即便流很小，输出的结果的顺序也和前面不一样。这是由于多处理器并行操作的原因，如果你将程序多运行几次，你会发现输出都不相同，这是多处理器并行操作的不确定性造成的结果。

在最后一个流中，同时使用了 `parallel()` 和 `forEachOrdered()` 来强制保持原始流顺序。因此，对非并行流使用 `forEachOrdered()` 是没有任何影响的。

<!-- Collecting -->

#### 集合

- `collect(Collector)`：使用 **Collector** 收集流元素到结果集合中。
- `collect(Supplier, BiConsumer, BiConsumer)`：同上，第一个参数 **Supplier** 创建了一个新的结果集合，第二个参数 **BiConsumer** 将下一个元素收集到结果集合中，第三个参数 **BiConsumer** 用于将两个结果集合合并起来。

在这里我们只是简单介绍了几个 **Collectors** 的运用示例。实际上，它还有一些非常复杂的操作实现，可通过查看 `java.util.stream.Collectors` 的 API 文档了解。例如，我们可以将元素收集到任意一种特定的集合中。

假设我们现在为了保证元素有序，将元素存储在 **TreeSet** 中。**Collectors** 里面没有特定的 `toTreeSet()`，但是我们可以通过将集合的构造函数引用传递给 `Collectors.toCollection()`，从而构建任何类型的集合。下面我们来将一个文件中的单词收集到 **TreeSet** 集合中。代码示例：

```java
// streams/TreeSetOfWords.java
import java.util.*;
import java.nio.file.*;
import java.util.stream.*;
public class TreeSetOfWords {
    public static void
    main(String[] args) throws Exception {
        Set<String> words2 =
                Files.lines(Paths.get("TreeSetOfWords.java"))
                        .flatMap(s -> Arrays.stream(s.split("\\W+")))
                        .filter(s -> !s.matches("\\d+")) // No numbers
                        .map(String::trim)
                        .filter(s -> s.length() > 2)
                        .limit(100)
                        .collect(Collectors.toCollection(TreeSet::new));
        System.out.println(words2);
    }
}
```

输出结果：

```
[Arrays, Collectors, Exception, Files, Output, Paths,
Set, String, System, TreeSet, TreeSetOfWords, args,
class, collect, file, filter, flatMap, get, import,
java, length, limit, lines, main, map, matches, new,
nio, numbers, out, println, public, split, static,
stream, streams, throws, toCollection, trim, util,
void, words2]
```

**Files.**`lines()` 打开 **Path** 并将其转换成为由行组成的流。下一行代码以一个或多个非单词字符（`\\W+`）为分界，对每一行进行分割，结果是产生一个数组，然后使用 **Arrays.**`stream()` 将数组转化成为流，最后`flatMap()`将各行形成的多个单词流，扁平映射为一个单词流。使用 `matches(\\d+)` 查找并移除全部是数字的字符串（注意,`words2` 是通过的）。然后用 **String.**`trim()` 去除单词两边的空白，`filter()` 过滤所有长度小于3的单词，并只获取前100个单词，最后将其保存到 **TreeSet** 中。

我们也可以在流中生成 **Map**。代码示例：

```java
// streams/MapCollector.java
import java.util.*;
import java.util.stream.*;
class Pair {
    public final Character c;
    public final Integer i;
    Pair(Character c, Integer i) {
        this.c = c;
        this.i = i;
    }
    public Character getC() { return c; }
    public Integer getI() { return i; }
    @Override
    public String toString() {
        return "Pair(" + c + ", " + i + ")";
    }
}
class RandomPair {
    Random rand = new Random(47);
    // An infinite iterator of random capital letters:
    Iterator<Character> capChars = rand.ints(65,91)
            .mapToObj(i -> (char)i)
            .iterator();
    public Stream<Pair> stream() {
        return rand.ints(100, 1000).distinct()
                .mapToObj(i -> new Pair(capChars.next(), i));
    }
}
public class MapCollector {
    public static void main(String[] args) {
        Map<Integer, Character> map =
                new RandomPair().stream()
                        .limit(8)
                        .collect(
                                Collectors.toMap(Pair::getI, Pair::getC));
        System.out.println(map);
    }
}
```

输出结果：

```
{688=W, 309=C, 293=B, 761=N, 858=N, 668=G, 622=F, 751=N}
```

**Pair** 只是一个基础的数据对象。**RandomPair** 创建了随机生成的 **Pair** 对象流。在 Java 中，我们不能直接以某种方式组合两个流。所以我创建了一个整数流，并且使用 `mapToObj()` 将整数流转化成为 **Pair** 流。 **capChars**的随机大写字母迭代器创建了流，然后`next()`让我们可以在`stream()`中使用这个流。就我所知，这是将多个流组合成新的对象流的唯一方法。

在这里，我们只使用最简单形式的 `Collectors.toMap()`，这个方法只需要两个从流中获取键和值的函数。还有其他重载形式，其中一种当是键发生冲突时，使用一个函数来处理冲突。

大多数情况下，`java.util.stream.Collectors` 中预设的 **Collector** 就能满足我们的要求。除此之外，你还可以使用第二种形式的 `collect()`。 我把它留作更高级的练习，下例给出基本用法：

```java
// streams/SpecialCollector.java
import java.util.*;
import java.util.stream.*;
public class SpecialCollector {
    public static void main(String[] args) throws Exception {
        ArrayList<String> words =
                FileToWords.stream("Cheese.dat")
                        .collect(ArrayList::new,
                                ArrayList::add,
                                ArrayList::addAll);
        words.stream()
                .filter(s -> s.equals("cheese"))
                .forEach(System.out::println);
    }
}
```

输出结果：

```
cheese
cheese
```

在这里， **ArrayList** 的方法已经做了你所需要的操作，但更有可能的是，如果你必须使用这种形式的 `collect()`，就要自己创建特定的定义。

<!-- Combining All Stream Elements -->

#### 组合

- `reduce(BinaryOperator)`：使用 **BinaryOperator** 来组合所有流中的元素。因为流可能为空，其返回值为 **Optional**。
- `reduce(identity, BinaryOperator)`：功能同上，但是使用 **identity** 作为其组合的初始值。因此如果流为空，**identity** 就是结果。
- `reduce(identity, BiFunction, BinaryOperator)`：更复杂的使用形式（暂不介绍），这里把它包含在内，因为它可以提高效率。通常，我们可以显式地组合 `map()` 和 `reduce()` 来更简单的表达它。

下面来看下 `reduce` 的代码示例：

```java
// streams/Reduce.java
import java.util.*;
import java.util.stream.*;
class Frobnitz {
    int size;
    Frobnitz(int sz) { size = sz; }
    @Override
    public String toString() {
        return "Frobnitz(" + size + ")";
    }
    // Generator:
    static Random rand = new Random(47);
    static final int BOUND = 100;
    static Frobnitz supply() {
        return new Frobnitz(rand.nextInt(BOUND));
    }
}
public class Reduce {
    public static void main(String[] args) {
        Stream.generate(Frobnitz::supply)
                .limit(10)
                .peek(System.out::println)
                .reduce((fr0, fr1) -> fr0.size < 50 ? fr0 : fr1)
                .ifPresent(System.out::println);
    }
}
```

输出结果：

```
Frobnitz(58)
Frobnitz(55)
Frobnitz(93)
Frobnitz(61)
Frobnitz(61)
Frobnitz(29)
Frobnitz(68)
Frobnitz(0)
Frobnitz(22)
Frobnitz(7)
Frobnitz(29)
```

**Frobnitz** 包含一个可生成自身的生成器 `supply()` ；因为 `supply()` 方法作为一个 `Supplier<Frobnitz>` 是签名兼容的，我们可以把 `supply()` 作为一个方法引用传递给 `Stream.generate()` （这种签名兼容性被称作结构一致性）。我们使用了没有“初始值”作为第一个参数的 `reduce()`方法，所以产生的结果是 **Optional** 类型。`Optional.ifPresent()` 方法只有在结果非空的时候才会调用 `Consumer<Frobnitz>` （`println` 方法可以被调用是因为 **Frobnitz** 可以通过 `toString()` 方法转换成 **String**）。

Lambda 表达式中的第一个参数 `fr0` 是 `reduce()` 中上一次调用的结果。而第二个参数 `fr1` 是从流传递过来的值。

`reduce()` 中的 Lambda 表达式使用了三元表达式来获取结果，当 `fr0` 的 `size` 值小于 50 的时候，将 `fr0` 作为结果，否则将序列中的下一个元素即 `fr1`作为结果。当取得第一个 `size` 值小于 50 的 `Frobnitz`，只要得到这个结果就会忽略流中其他元素。这是个非常奇怪的限制， 但也确实让我们对 `reduce()` 有了更多的了解。

<!-- Matching -->

#### 匹配

- `allMatch(Predicate)` ：如果流的每个元素提供给 **Predicate** 都返回 true ，结果返回为 true。在第一个 false 时，则停止执行计算。
- `anyMatch(Predicate)`：如果流的任意一个元素提供给 **Predicate** 返回 true ，结果返回为 true。在第一个 true 是停止执行计算。
- `noneMatch(Predicate)`：如果流的每个元素提供给 **Predicate** 都返回 false 时，结果返回为 true。在第一个 true 时停止执行计算。

我们已经在 `Prime.java` 中看到了 `noneMatch()` 的示例；`allMatch()` 和 `anyMatch()` 的用法基本上是等同的。下面我们来探究一下短路行为。为了消除冗余代码，我们创建了 `show()`。首先我们必须知道如何统一地描述这三个匹配器的操作，然后再将其转换为 **Matcher** 接口。代码示例：

```java
// streams/Matching.java
// Demonstrates short-circuiting of *Match() operations
import java.util.stream.*;
import java.util.function.*;
import static streams.RandInts.*;

interface Matcher extends BiPredicate<Stream<Integer>, Predicate<Integer>> {}
        
public class Matching {
    static void show(Matcher match, int val) {
        System.out.println(
                match.test(
                        IntStream.rangeClosed(1, 9)
                                .boxed()
                                .peek(n -> System.out.format("%d ", n)),
                        n -> n < val));
    }
    public static void main(String[] args) {
        show(Stream::allMatch, 10);
        show(Stream::allMatch, 4);
        show(Stream::anyMatch, 2);
        show(Stream::anyMatch, 0);
        show(Stream::noneMatch, 5);
        show(Stream::noneMatch, 0);
    }
}
```

输出结果：

```
1 2 3 4 5 6 7 8 9 true
1 2 3 4 false
1 true
1 2 3 4 5 6 7 8 9 false
1 false
1 2 3 4 5 6 7 8 9 true
```

**BiPredicate** 是一个二元谓词，它接受两个参数并返回 true 或者 false。第一个参数是我们要测试的流，第二个参数是一个谓词 **Predicate**。**Matcher** 可以匹配所有的 **Stream::\*Match** 方法，所以可以将每一个**Stream::\*Match**方法引用传递到 `show()` 中。对`match.test()` 的调用会被转换成 对方法引用**Stream::\*Match** 的调用。

`show()` 接受一个**Matcher**和一个 `val` 参数，`val` 在判断测试 `n < val`中指定了最大值。`show()` 方法生成了整数1-9组成的一个流。`peek()`用来展示在测试短路之前测试进行到了哪一步。从输出中可以看到每次都发生了短路。

#### 查找

- `findFirst()`：返回第一个流元素的 **Optional**，如果流为空返回 **Optional.empty**。
- `findAny(`：返回含有任意流元素的 **Optional**，如果流为空返回 **Optional.empty**。

代码示例：

```java
// streams/SelectElement.java
import java.util.*;
import java.util.stream.*;
import static streams.RandInts.*;
public class SelectElement {
    public static void main(String[] args) {
        System.out.println(rands().findFirst().getAsInt());
        System.out.println(
                rands().parallel().findFirst().getAsInt());
        System.out.println(rands().findAny().getAsInt());
        System.out.println(
                rands().parallel().findAny().getAsInt());
    }
}
```

输出结果：

```
258
258
258
242
```

无论流是否为并行化，`findFirst()` 总是会选择流中的第一个元素。对于非并行流，`findAny()`会选择流中的第一个元素（即使从定义上来看是选择任意元素）。在这个例子中，用 `parallel()` 将流并行化，以展示 `findAny()` 不选择流的第一个元素的可能性。

如果必须选择流中最后一个元素，那就使用 `reduce()`。代码示例：

```java
// streams/LastElement.java
import java.util.*;
import java.util.stream.*;
public class LastElement {
    public static void main(String[] args) {
        OptionalInt last = IntStream.range(10, 20)
                .reduce((n1, n2) -> n2);
        System.out.println(last.orElse(-1));
        // Non-numeric object:
        Optional<String> lastobj =
                Stream.of("one", "two", "three")
                        .reduce((n1, n2) -> n2);
        System.out.println(
                lastobj.orElse("Nothing there!"));
    }
}
```

输出结果：

```
19
three
```

`reduce()`  的参数只是用最后一个元素替换了最后两个元素，最终只生成最后一个元素。如果为数字流，你必须使用相近的数字 **Optional** 类型（ numeric optional type），否则使用 **Optional** 类型，就像上例中的 `Optional<String>`。

<!-- Informational -->

#### 信息

- `count()`：流中的元素个数。
- `max(Comparator)`：根据所传入的 **Comparator** 所决定的“最大”元素。
- `min(Comparator)`：根据所传入的 **Comparator** 所决定的“最小”元素。

**String** 类型有预设的 **Comparator** 实现。代码示例：

```java
// streams/Informational.java
import java.util.stream.*;
import java.util.function.*;
public class Informational {
    public static void
    main(String[] args) throws Exception {
        System.out.println(
                FileToWords.stream("Cheese.dat").count());
        System.out.println(
                FileToWords.stream("Cheese.dat")
                        .min(String.CASE_INSENSITIVE_ORDER)
                        .orElse("NONE"));
        System.out.println(
                FileToWords.stream("Cheese.dat")
                        .max(String.CASE_INSENSITIVE_ORDER)
                        .orElse("NONE"));
    }
}
```

输出结果：

```
32
a
you
```

`min()` 和 `max()` 的返回类型为 **Optional**，这需要我们使用 `orElse()`来解包。

<!-- Information for Numeric Streams -->

#### 数字流信息

- `average()` ：求取流元素平均值。
- `max()` 和 `min()`：数值流操作无需 **Comparator**。
- `sum()`：对所有流元素进行求和。
- `summaryStatistics()`：生成可能有用的数据。目前并不太清楚这个方法存在的必要性，因为我们其实可以用更直接的方法获得需要的数据。

```java
// streams/NumericStreamInfo.java
import java.util.stream.*;
import static streams.RandInts.*;
public class NumericStreamInfo {
    public static void main(String[] args) {
        System.out.println(rands().average().getAsDouble());
        System.out.println(rands().max().getAsInt());
        System.out.println(rands().min().getAsInt());
        System.out.println(rands().sum());
        System.out.println(rands().summaryStatistics());
    }
}
```

输出结果：

```
507.94
998
8
50794
IntSummaryStatistics{count=100, sum=50794, min=8, average=507.940000, max=998}
```

上例操作对于 **LongStream** 和 **DoubleStream** 同样适用。

#### 本章小结

流式操作改变并极大地提升了 Java 语言的可编程性，并可能极大地阻止了 Java 编程人员向诸如 Scala 这种函数式语言的流转。在本书的剩余部分，我们将尽可能地使用流。

[^1]: 在软件或信息建模的上下文中，快乐路径(有时称为快乐流)是没有异常或错误条件的默认场景。例如，验证信用卡号的函数的快乐路径应该是任何验证规则都不会出现错误的地方，从而让执行成功地继续到最后，生成一个积极的响应。[见 wikipedia: happy path](https://en.wikipedia.org/wiki/Happy_path)

<!-- 分页 -->

<div style="page-break-after: always;"></div>

## 正则表达式(todo)

## 类型信息

> RTTI（RunTime Type Information，运行时类型信息）能够在程序运行时发现和使用类型信息

RTTI 把我们从只能在编译期进行面向类型操作的禁锢中解脱了出来，并且让我们可以使用某些非常强大的程序。对 RTTI 的需要，揭示了面向对象设计中许多有趣（并且复杂）的特性，同时也带来了关于如何组织程序的基本问题。

本章将讨论 Java 是如何在运行时识别对象和类信息的。主要有两种方式：

1. “传统的” RTTI：假定我们在编译时已经知道了所有的类型；
2. “反射”机制：允许我们在运行时发现和使用类的信息。

### Class 对象

​	要理解 RTTI 在 Java 中的工作原理，首先必须知道类型信息在运行时是如何表示的。这项工作是由称为 **`Class`对象** 的特殊对象完成的，它包含了与类有关的信息。实际上，`Class` 对象就是用来创建该类所有"常规"对象的。Java 使用 `Class` 对象来实现 RTTI，即便是类型转换这样的操作都是用 `Class` 对象实现的。不仅如此，`Class` 类还提供了很多使用 RTTI 的其它方式。

​	所有的类都是第一次使用时动态加载到 JVM 中的，当程序创建第一个对类的静态成员的引用时，就会加载这个类。

> 其实构造器也是类的静态方法，虽然构造器前面并没有 `static` 关键字。所以，使用 `new` 操作符创建类的新对象，这个操作也算作对类的静态成员引用。

​	因此，Java 程序在它开始运行之前并没有被完全加载，很多部分是在需要时才会加载。这一点与许多传统编程语言不同，动态加载使得 Java 具有一些静态加载语言（如 C++）很难或者根本不可能实现的特性。



```java
Class.forName("Gum");
```

​	所有 `Class` 对象都属于 `Class` 类，而且它跟其他普通对象一样，我们可以获取和操控它的引用(这也是类加载器的工作)。`forName()` 是 `Class` 类的一个静态方法，我们可以使用 `forName()` 根据目标类的类名（`String`）得到该类的 `Class` 对象。上面的代码忽略了 `forName()` 的返回值，因为那个调用是为了得到它产生的“副作用”。从结果可以看出，`forName()` 执行的副作用是如果 `Gum` 类没有被加载就加载它，而在加载的过程中，`Gum` 的 `static` 初始化块被执行了。

#### 类字面常量

​	Java 还提供了另一种方法来生成类对象的引用：**类字面常量**。对上述程序来说，就像这样：`FancyToy.class;`。这样做不仅更简单，而且更安全，因为它在编译时就会受到检查（因此不必放在 `try` 语句块中）。并且它根除了对 `forName()` 方法的调用，所以效率更高。

类字面常量不仅可以应用于普通类，也可以应用于接口、数组以及基本数据类型。另外，对于基本数据类型的包装类，还有一个标准字段 `TYPE`。`TYPE` 字段是一个引用，指向对应的基本数据类型的 `Class` 对象，如下所示：

<figure>
<table style="text-align:center;">
  <thead>
    <tr>
      <th colspan="2">...等价于...</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>boolean.class</td>
      <td>Boolean.TYPE</td>
    </tr>
    <tr>
      <td>char.class</td>
      <td>Character.TYPE</td>
    </tr>
    <tr>
      <td>byte.class</td>
      <td>Byte.TYPE</td>
    </tr>
    <tr>
      <td>short.class</td>
      <td>Short.TYPE</td>
    </tr>
    <tr>
      <td>int.class</td>
      <td>Integer.TYPE</td>
    </tr>
    <tr>
      <td>long.class</td>
      <td>Long.TYPE</td>
    </tr>
    <tr>
      <td>float.class</td>
      <td>Float.TYPE</td>
    </tr>
    <tr>
      <td>double.class</td>
      <td>Double.TYPE</td>
    </tr>
    <tr>
      <td>void.class</td>
      <td>Void.TYPE</td>
    </tr>
  </tbody>
</table>
</figure>

​	我的建议是使用 `.class` 的形式，以保持与普通类的一致性。

为了使用类而做的准备工作实际包含三个步骤：

1. **加载**，这是由类加载器执行的。该步骤将查找字节码（通常在 classpath 所指定的路径中查找，但这并非是必须的），并从这些字节码中创建一个 `Class` 对象。
2. **链接**。在链接阶段将验证类中的字节码，为 `static` 字段分配存储空间，并且如果需要的话，将解析这个类创建的对其他类的所有引用。
3. **初始化**。如果该类具有超类，则先初始化超类，执行 `static` 初始化器和 `static` 初始化块。



​	初始化有效地实现了尽可能的“惰性”，从对 `initable` 引用的创建中可以看到，仅使用 `.class` 语法来获得对类对象的引用不会引发初始化。但与此相反，使用 `Class.forName()` 来产生 `Class` 引用会立即就进行初始化，如 `initable3`。

#### 泛化的 `Class` 引用

​	`Class` 引用总是指向某个 `Class` 对象，而 `Class` 对象可以用于产生类的实例，并且包含可作用于这些实例的所有方法代码。它还包含该类的 `static` 成员，因此 `Class` 引用表明了它所指向对象的确切类型，而该对象便是 `Class` 类的一个对象。

```java
// typeinfo/GenericClassReferences.java

public class GenericClassReferences {
    public static void main(String[] args) {
        Class intClass = int.class;
        Class<Integer> genericIntClass = int.class;
        genericIntClass = Integer.class; // 同一个东西
        intClass = double.class;
        // genericIntClass = double.class; // 非法
    }
}
```

​	为了在使用 `Class` 引用时放松限制，我们使用了通配符，它是 Java 泛型中的一部分。通配符就是 `?`，表示“任何事物”。因此，我们可以在上例的普通 `Class` 引用中添加通配符，并产生相同的结果：

```java
// typeinfo/WildcardClassReferences.java

public class WildcardClassReferences {
    public static void main(String[] args) {
        Class<?> intClass = int.class;
        intClass = double.class;
    }
}
```

​	使用 `Class<?>` 比单纯使用 `Class` 要好，虽然它们是等价的，并且单纯使用 `Class` 不会产生编译器警告信息。使用 `Class<?>` 的好处是它表示你并非是碰巧或者由于疏忽才使用了一个非具体的类引用，而是特意为之。

​	为了创建一个限定指向某种类型或其子类的 `Class` 引用，我们需要将通配符与 `extends` 关键字配合使用，创建一个范围限定。这与仅仅声明 `Class<Number>` 不同，现在做如下声明：

```java
// typeinfo/BoundedClassReferences.java

public class BoundedClassReferences {
    public static void main(String[] args) {
        Class<? extends Number> bounded = int.class;
        bounded = double.class;
        bounded = Number.class;
        // Or anything else derived from Number.
    }
}
```

向 `Class` 引用添加泛型语法的原因只是为了提供编译期类型检查，因此如果你操作有误，稍后就会发现这点。使用普通的 `Class` 引用你要确保自己不会犯错，因为一旦你犯了错误，就要等到运行时才能发现它，很不方便。

下面的示例使用了泛型语法，它保存了一个类引用，稍后又用 `newInstance()` 方法产生类的对象：

#### `cast()` 方法

Java 中还有用于 `Class` 引用的转型语法，即 `cast()` 方法：

```java
// typeinfo/ClassCasts.java

class Building {}
class House extends Building {}

public class ClassCasts {
    public static void main(String[] args) {
        Building b = new House();
        Class<House> houseType = House.class;
        House h = houseType.cast(b);
        h = (House)b; // ... 或者这样做.
    }
}
```

`cast()` 方法接受参数对象，并将其类型转换为 `Class` 引用的类型。但是，如果观察上面的代码，你就会发现，与实现了相同功能的 `main` 方法中最后一行相比，这种转型好像做了很多额外的工作。

`cast()` 在无法使用普通类型转换的情况下会显得非常有用，在你编写泛型代码（你将在[泛型](./20-Generics)这一章学习到）时，如果你保存了 `Class` 引用，并希望以后通过这个引用来执行转型，你就需要用到 `cast()`。但事实却是这种情况非常少见，我发现整个 Java 类库中，只有一处使用了 `cast()`（在 `com.sun.mirror.util.DeclarationFilter` 中）。

Java 类库中另一个没有任何用处的特性就是 `Class.asSubclass()`，该方法允许你将一个 `Class` 对象转型为更加具体的类型。

### 类型转换检测

​	直到现在，我们已知的 RTTI 类型包括：

1.  传统的类型转换，如 “`(Shape)`”，由 RTTI 确保转换的正确性，如果执行了一个错误的类型转换，就会抛出一个 `ClassCastException` 异常。
2.  代表对象类型的 `Class` 对象. 通过查询 `Class` 对象可以获取运行时所需的信息.



​	RTTI 在 Java 中还有第三种形式，那就是关键字 `instanceof`。它返回一个布尔值，告诉我们对象是不是某个特定类型的实例，可以用提问的方式使用它，就像这个样子：

```java
if(x instanceof Dog)
    ((Dog)x).bark();
```

#### 一个动态 `instanceof` 函数

```java
		 

 public void count(Pet pet) {
            // Class.isInstance() 替换 instanceof:
            entrySet().stream()
                    .filter(pair -> pair.getKey().isInstance(pet))
                    .forEach(pair ->
                            put(pair.getKey(), pair.getValue() + 1));
        }
```



#### 递归计数

.isAssignableFrom()方法与instanceof关键字的区别总结为以下两个点：

isAssignableFrom()方法是从类继承的角度去判断，instanceof关键字是从实例继承的角度去判断。
isAssignableFrom()方法是判断是否为某个类的父类，instanceof关键字是判断是否某个类的子类。
使用方法：

```java
父类.class.isAssignableFrom(子类.class)

子类实例 instanceof 父类类型
```



isAssignableFrom()方法的调用者和参数都是Class对象，调用者为父类，参数为本身或者其子类。

### 注册工厂

​	略。

### 类的等价比较

​	当你查询类型信息时，需要注意：instanceof 的形式(即 `instanceof` 或 `isInstance()` ，这两者产生的结果相同) 和 与 Class 对象直接比较 这两者间存在重要区别。下面的例子展示了这种区别：

```java
// typeinfo/FamilyVsExactType.java
// instanceof 与 class 的差别
// {java typeinfo.FamilyVsExactType}
package typeinfo;

class Base {}
class Derived extends Base {}

public class FamilyVsExactType {
    static void test(Object x) {
        System.out.println(
              "Testing x of type " + x.getClass());
        System.out.println(
              "x instanceof Base " + (x instanceof Base));
        System.out.println(
              "x instanceof Derived " + (x instanceof Derived));
        System.out.println(
              "Base.isInstance(x) " + Base.class.isInstance(x));
        System.out.println(
              "Derived.isInstance(x) " +
              Derived.class.isInstance(x));
        System.out.println(
              "x.getClass() == Base.class " +
              (x.getClass() == Base.class));
        System.out.println(
              "x.getClass() == Derived.class " +
              (x.getClass() == Derived.class));
        System.out.println(
              "x.getClass().equals(Base.class)) "+
              (x.getClass().equals(Base.class)));
        System.out.println(
              "x.getClass().equals(Derived.class)) " +
              (x.getClass().equals(Derived.class)));
    }

    public static void main(String[] args) {
        test(new Base());
        test(new Derived());
    }
}
```

输出结果：

```
Testing x of type class typeinfo.Base
x instanceof Base true
x instanceof Derived false
Base.isInstance(x) true
Derived.isInstance(x) false
x.getClass() == Base.class true
x.getClass() == Derived.class false
x.getClass().equals(Base.class)) true
x.getClass().equals(Derived.class)) false
Testing x of type class typeinfo.Derived
x instanceof Base true
x instanceof Derived true
Base.isInstance(x) true
Derived.isInstance(x) true
x.getClass() == Base.class false
x.getClass() == Derived.class true
x.getClass().equals(Base.class)) false
x.getClass().equals(Derived.class)) true
```

`test()` 方法使用两种形式的 `instanceof` 对其参数执行类型检查。然后，它获取 `Class` 引用，并使用 `==` 和 `equals()` 测试 `Class` 对象的相等性。令人放心的是，`instanceof` 和 `isInstance()` 产生的结果相同， `equals()` 和 `==` 产生的结果也相同。但测试本身得出了不同的结论。与类型的概念一致，`instanceof` 说的是“你是这个类，还是从这个类派生的类？”。而如果使用 `==` 比较实际的`Class` 对象，则与继承无关 —— 它要么是确切的类型，要么不是。

<!-- Reflection: Runtime Class Information -->

### 反射：运行时类信息

#### 类方法提取器

```java
void func() {
    Method[] methods = c.getMethods(); // 公共方法
	Constructor[] ctors = c.getConstructors();
	Method[] allMethods = c.getDeclaredMethods(); // 所有方法
    Field[] fields = getDeclaredFields(searchType);
    Field field = a.getField("stream");
	Method method = a.getMethod("getName", String.class);
}
```

`Class` 方法 `getmethods()` 和 `getconstructors()`  分别返回 `Method` 数组和 `Constructor` 数组。



### 动态代理

​	一个简单的示例，显示代理的结构：

```java
// typeinfo/SimpleProxyDemo.java

interface Interface {
    void doSomething();

    void somethingElse(String arg);
}

class RealObject implements Interface {
    @Override
    public void doSomething() {
        System.out.println("doSomething");
    }

    @Override
    public void somethingElse(String arg) {
        System.out.println("somethingElse " + arg);
    }
}

class SimpleProxy implements Interface {
    private Interface proxied;

    SimpleProxy(Interface proxied) {
        this.proxied = proxied;
    }

    @Override
    public void doSomething() {
        System.out.println("SimpleProxy doSomething");
        proxied.doSomething();
    }

    @Override
    public void somethingElse(String arg) {
        System.out.println(
                "SimpleProxy somethingElse " + arg);
        proxied.somethingElse(arg);
    }
}

class SimpleProxyDemo {
    public static void consumer(Interface iface) {
        iface.doSomething();
        iface.somethingElse("bonobo");
    }

    public static void main(String[] args) {
        consumer(new RealObject());
        consumer(new SimpleProxy(new RealObject()));
    }
}
```

​	因为 `consumer()` 接受 `Interface`，所以它不知道获得的是 `RealObject` 还是 `SimpleProxy`，因为两者都实现了 `Interface`。
但是，在客户端和 `RealObject` 之间插入的 `SimpleProxy` 执行操作，然后在 `RealObject` 上调用相同的方法。

​	Java 的*动态代理*更进一步，不仅动态创建代理对象而且动态处理对代理方法的调用。在动态代理上进行的所有调用都被重定向到单个*调用处理程序*，该处理程序负责发现调用的内容并决定如何处理。这是 `SimpleProxyDemo.java` 使用动态代理重写的例子：

```java
// typeinfo/SimpleDynamicProxy.java

import java.lang.reflect.*;

class DynamicProxyHandler implements InvocationHandler {
    private Object proxied;

    DynamicProxyHandler(Object proxied) {
        this.proxied = proxied;
    }

    @Override
    public Object
    invoke(Object proxy, Method method, Object[] args)
            throws Throwable {
        System.out.println(
                "**** proxy: " + proxy.getClass() +
                        ", method: " + method + ", args: " + args);
        if (args != null)
            for (Object arg : args)
                System.out.println("  " + arg);
        return method.invoke(proxied, args);
    }
}

class SimpleDynamicProxy {
    public static void consumer(Interface iface) {
        iface.doSomething();
        iface.somethingElse("bonobo");
    }

    public static void main(String[] args) {
        RealObject real = new RealObject();
        consumer(real);
        // Insert a proxy and call again:
        Interface proxy = (Interface) Proxy.newProxyInstance(
                Interface.class.getClassLoader(),
                new Class[]{Interface.class},
                new DynamicProxyHandler(real));
        consumer(proxy);
    }
}
```

输出结果：

```
doSomething
somethingElse bonobo
**** proxy: class $Proxy0, method: public abstract void
Interface.doSomething(), args: null
doSomething
**** proxy: class $Proxy0, method: public abstract void
Interface.somethingElse(java.lang.String), args:
[Ljava.lang.Object;@6bc7c054
  bonobo
somethingElse bonobo
```

​	可以通过调用静态方法 `Proxy.newProxyInstance()` 来创建动态代理，该方法需要一个类加载器（通常可以从已加载的对象中获取），希望代理实现的接口列表（不是类或抽象类），以及接口  `InvocationHandler` 的一个实现。动态代理会将所有调用重定向到调用处理程序，因此通常为调用处理程序的构造函数提供对“真实”对象的引用，以便一旦执行中介任务便可以转发请求。

### Optional类

```java
public void func() {
    this.address = Optional.ofNullable(address);
// Throws EmptyTitleException if newTitle is null:
    title = Optional.ofNullable(newTitle)
       .orElseThrow(EmptyTitleException::new);
    
    person = Optional.ofNullable(newPerson)
                .orElse(new Person());
}
```

#### 标记接口

​	有时候使用一个**标记接口**来表示空值会更方便。标记接口里边什么都没有，你只要把它的名字当做标签来用就可以。

```java
// onjava/Null.java
package onjava;
public interface Null {}
```

#### Mock 对象和桩

**Mock 对象**和 **桩（Stub）**在逻辑上都是 `Optional` 的变体。他们都是最终程序中所使用的“实际”对象的代理。不过，Mock 对象和桩都是假扮成那些可以传递实际信息的实际对象，而不是像 `Optional` 那样把包含潜在 `null` 值的对象隐藏。

## 泛型

### 接口和类型

​	略。

```java
// generics/GenericHolder.java

public class GenericHolder<T> {
    private T a;
    public GenericHolder() {}
    public void set(T a) { this.a = a; }
    public T get() { return a; }
    
    public static void main(String[] args) {
        GenericHolder<Automobile> h3 = new GenericHolder<Automobile>();
        h3.set(new Automobile()); // 此处有类型校验
        Automobile a = h3.get();  // 无需类型转换
        //- h3.set("Not an Automobile"); // 报错
        //- h3.set(1);  // 报错
    }
}
```

#### 泛型方法

```java
	public <T> void f(T x) {
        System.out.println(x.getClass().getName());
    }
```

#### 变长参数和泛型方法

```java
 @SafeVarargs
    public static <T> List<T> makeList(T... args) {
        List<T> result = new ArrayList<>();
        for (T item : args)
            result.add(item);
        return result;
    }
```

### 泛型擦除

​	在泛型代码内部，无法获取任何有关泛型参数类型的信息。

​	注意，对于在泛型中创建数组，使用 `Array.newInstance()` 是推荐的方式。

### 补偿擦除

​	有时，我们可以对这些问题进行编程，但是有时必须通过引入类型标签来补偿擦除。这意味着为所需的类型显式传递一个 **Class** 对象，以在类型表达式中使用它。

#### 创建类型的实例

​	Java 中的解决方案是传入一个工厂对象，并使用该对象创建新实例。方便的工厂对象只是 **Class** 对象，因此，如果使用类型标记，则可以使用 `newInstance()` 创建该类型的新对象：

```java
class ClassAsFactory<T> implements Supplier<T> {
    Class<T> kind;

    ClassAsFactory(Class<T> kind) {
        this.kind = kind;
    }

    @Override
    public T get() {
        try {
            return kind.newInstance();
        } catch (InstantiationException |
                IllegalAccessException e) {
            throw new RuntimeException(e);
        }
    }
}
```

​	另一种方法是模板方法设计模式。在以下示例中，`create()` 是模板方法，在子类中被重写以生成该类型的对象：

```java
// generics/CreatorGeneric.java

abstract class GenericWithCreate<T> {
    final T element;

    GenericWithCreate() {
        element = create();
    }

    abstract T create();
}

class X {
}

class XCreator extends GenericWithCreate<X> {
    @Override
    X create() {
        return new X();
    }

    void f() {
        System.out.println(
                element.getClass().getSimpleName());
    }
}

public class CreatorGeneric {
    public static void main(String[] args) {
        XCreator xc = new XCreator();
        xc.f();
    }
}
/* Output:
X
*/
```

#### 泛型数组

​	正如在 **Erased.java** 中所看到的，我们无法创建泛型数组。通用解决方案是在试图创建泛型数组的时候使用 **ArrayList** ：

```java
// generics/ListOfGenerics.java

import java.util.ArrayList;
import java.util.List;

public class ListOfGenerics<T> {
    private List<T> array = new ArrayList<>();

    public void add(T item) {
        array.add(item);
    }

    public T get(int index) {
        return array.get(index);
    }
}
```

​	这样做可以获得数组的行为，并且还具有泛型提供的编译时类型安全性。

​	有时，仍然会创建泛型类型的数组（例如， **ArrayList** 在内部使用数组）。可以通过使编译器满意的方式定义对数组的通用引用：

```java
// generics/ArrayOfGenericReference.java

class Generic<T> {
}

public class ArrayOfGenericReference {
    static Generic<Integer>[] gia;
}
```

### 边界

```java
class WithColorCoord<T extends Coord & HasColor> {}

// Bounds in generic methods:
    static <POWER extends SuperHearing>
    void useSuperHearing(SuperHero<POWER> hero) {
        hero.getPower().hearSubtleNoises();
    }
```

### 通配符

```java
List<? extends Fruit> flist = new ArrayList<>();
```

#### 逆变

```java
static void writeTo(List<? super Apple> apples) {}
```

#### 无界通配符

​	无界通配符 `<?>` 看起来意味着“任何事物”，因此使用无界通配符好像等价于使用原生类型。

​	即编译器很少关心使用的是原生类型还是 `<?>` 。在这些情况中，`<?>` 可以被认为是一种装饰，但是它仍旧是很有价值的，因为，实际上它是在声明：“我是想用 Java 的泛型来编写这段代码，我在这里并不是要用原生类型，但是在当前这种情况下，泛型参数可以持有任何类型。”

#### 捕获转换

有一种特殊情况需要使用 `<?>` 而不是原生类型。如果向一个使用 `<?>` 的方法传递原生类型，那么对编译器来说，可能会推断出实际的类型参数，使得这个方法可以回转并调用另一个使用这个确切类型的方法。下面的示例演示了这种技术，它被称为捕获转换，因为未指定的通配符类型被捕获，并被转换为确切类型。这里，有关警告的注释只有在 `@SuppressWarnings` 注解被移除之后才能起作用：

```java
// generics/CaptureConversion.java

public class CaptureConversion {
    static <T> void f1(Holder<T> holder) {
        T t = holder.get();
        System.out.println(t.getClass().getSimpleName());
    }
  
    static void f2(Holder<?> holder) {
        f1(holder); // Call with captured type
    }
    
    @SuppressWarnings("unchecked")
    public static void main(String[] args) {
        Holder raw = new Holder<>(1);
        f1(raw);
        // warning: [unchecked] unchecked method invocation:
        // method f1 in class CaptureConversion
        // is applied to given types
        //     f1(raw);
        //       ^
        //   required: Holder<T>
        //   found: Holder
        //   where T is a type-variable:
        //     T extends Object declared in
        //     method <T>f1(Holder<T>)
        // warning: [unchecked] unchecked conversion
        //     f1(raw);
        //        ^
        //   required: Holder<T>
        //   found:    Holder
        //   where T is a type-variable:
        //     T extends Object declared in
        //     method <T>f1(Holder<T>)
        // 2 warnings
        f2(raw); // No warnings
        
        Holder rawBasic = new Holder();
        rawBasic.set(new Object());
        // warning: [unchecked] unchecked call to set(T)
        // as a member of the raw type Holder
        //     rawBasic.set(new Object());
        //                 ^
        //   where T is a type-variable:
        //     T extends Object declared in class Holder
        // 1 warning
        f2(rawBasic); // No warnings
        
        // Upcast to Holder<?>, still figures it out:
        Holder<?> wildcarded = new Holder<>(1.0);
        f2(wildcarded);
    }
}
/* Output:
Integer
Integer
Object
Double
```

### 自限定

```java
class A extends SelfBounded<A>{}
```

#### 参数协变

​	自限定类型的价值在于它们可以产生*协变参数类型*——方法参数类型会随子类而变化。尽管自限定类型还可以产生与子类类型相同的返回类型，但是这并不十分重要，因为*协变返回类型*是在 Java 5 引入：

```java
// generics/CovariantReturnTypes.java

class Base {}
class Derived extends Base {}

interface OrdinaryGetter {
    Base get();
}

interface DerivedGetter extends OrdinaryGetter {
    // Overridden method return type can vary:
    @Override
    Derived get();
}

public class CovariantReturnTypes {
    void test(DerivedGetter d) {
        Derived d2 = d.get();
    }
}
```

​	自限定泛型事实上将产生确切的导出类型作为其返回值，就像在 `get()` 中所看到的一样：

```java
// generics/GenericsAndReturnTypes.java

interface GenericGetter<T extends GenericGetter<T>> {
    T get();
}

interface Getter extends GenericGetter<Getter> {}

public class GenericsAndReturnTypes {
    void test(Getter g) {
        Getter result = g.get();
        GenericGetter gg = g.get(); // Also the base type
    }
}
```

### 动态类型安全

​	Java 5 的 **java.util.Collections** 中有一组便利工具，可以解决在这种情况下的类型检查问题，它们是：静态方法 `checkedCollection()` 、`checkedList()`、 `checkedMap()` 、 `checkedSet()` 、`checkedSortedMap()`和 `checkedSortedSet()`。这些方法每一个都会将你希望动态检查的集合当作第一个参数接受，并将你希望强制要求的类型作为第二个参数接受。

​	受检查的集合在你试图插入类型不正确的对象时抛出 **ClassCastException** ，这与泛型之前的（原生）集合形成了对比，对于后者来说，当你将对象从集合中取出时，才会通知你出现了问题。

### 泛型异常

​	由于擦除的原因，**catch** 语句不能捕获泛型类型的异常，因为在编译期和运行时都必须知道异常的确切类型。泛型类也不能直接或间接继承自 **Throwable**（这将进一步阻止你去定义不能捕获的泛型异常）。
​	但是，类型参数可能会在一个方法的 **throws** 子句中用到。这使得你可以编写随检查型异常类型变化的泛型代码：

```java
// generics/ThrowGenericException.java

import java.util.*;

interface Processor<T, E extends Exception> {
    void process(List<T> resultCollector) throws E;
}

class ProcessRunner<T, E extends Exception>
extends ArrayList<Processor<T, E>> {
    List<T> processAll() throws E {
        List<T> resultCollector = new ArrayList<>();
        for(Processor<T, E> processor : this)
            processor.process(resultCollector);
        return resultCollector;
    }
}

class Failure1 extends Exception {}

class Processor1
implements Processor<String, Failure1> {
    static int count = 3;
    @Override
    public void process(List<String> resultCollector)
    throws Failure1 {
        if(count-- > 1)
            resultCollector.add("Hep!");
        else
            resultCollector.add("Ho!");
        if(count < 0)
            throw new Failure1();
    }
}

class Failure2 extends Exception {}

class Processor2
implements Processor<Integer, Failure2> {
    static int count = 2;
    @Override
    public void process(List<Integer> resultCollector)
    throws Failure2 {
        if(count-- == 0)
            resultCollector.add(47);
        else {
            resultCollector.add(11);
        }
        if(count < 0)
            throw new Failure2();
    }
}

public class ThrowGenericException {
    public static void main(String[] args) {
        ProcessRunner<String, Failure1> runner =
            new ProcessRunner<>();
        for(int i = 0; i < 3; i++)
            runner.add(new Processor1());
        try {
            System.out.println(runner.processAll());
        } catch(Failure1 e) {
            System.out.println(e);
        }

        ProcessRunner<Integer, Failure2> runner2 =
            new ProcessRunner<>();
        for(int i = 0; i < 3; i++)
            runner2.add(new Processor2());
        try {
            System.out.println(runner2.processAll());
        } catch(Failure2 e) {
            System.out.println(e);
        }
    }
}
/* Output:
[Hep!, Hep!, Ho!]
Failure2
*/
```

​	**Processor** 执行 `process()` 方法，并且可能会抛出具有类型 **E** 的异常。`process()` 的结果存储在 `List<T>resultCollector` 中（这被称为*收集参数*）。**ProcessRunner** 有一个 `processAll()` 方法，它会在所持有的每个 **Process** 对象执行，并返回 **resultCollector** 。
如果不能参数化所抛出的异常，那么由于检查型异常的缘故，将不能编写出这种泛化的代码。

### 混型

​	术语*混型*随时间的推移好像拥有了无数的含义，但是其最基本的概念是混合多个类的能力，以产生一个可以表示混型中所有类型的类。这往往是你最后的手段，它将使组装多个类变得简单易行。

​	其他略。

### 潜在类型机制

​	某些编程语言提供的一种解决方案称为*潜在类型机制*或*结构化类型机制*，而更古怪的术语称为*鸭子类型机制*，即“如果它走起来像鸭子，并且叫起来也像鸭子，那么你就可以将它当作鸭子对待。”鸭子类型机制变成了一种相当流行的术语，可能是因为它不像其他的术语那样承载着历史的包袱。

#### java中的直接潜在类型

​	因为泛型是在这场竞赛的后期才添加到 Java 中，因此没有任何机会可以去实现任何类型的潜在类型机制，因此 Java 没有对这种特性的支持。所以，初看起来，Java 的泛型机制比支持潜在类型机制的语言更“缺乏泛化性”。（使用擦除来实现 Java 泛型的实现有时称为第二类泛型类型）例如，在 Java 8  之前如果我们试图用 Java 实现上面 dogs-and-robots 的示例，那么就会被强制要求使用一个类或接口，并在边界表达式中指定它。

### 对缺乏潜在类型机制的补偿

尽管 Java 不直接支持潜在类型机制，但是这并不意味着泛型代码不能在不同的类型层次结构之间应用。也就是说，我们仍旧可以创建真正的泛型代码，但是这需要付出一些额外的努力。

* 反射
* 可变参数

### Java8 中的辅助潜在类型

​	先前声明关于 Java 缺乏对潜在类型的支持在 Java 8 之前是完全正确的。但是，Java 8 中的非绑定方法引用使我们能够产生一种潜在类型的形式，以满足创建一段可工作在不相干类型上的代码。

```java
// generics/DogsAndRobotMethodReferences.java

// "Assisted Latent Typing"
import typeinfo.pets.*;
import java.util.function.*;

class PerformingDogA extends Dog {
    public void speak() { System.out.println("Woof!"); }
    public void sit() { System.out.println("Sitting"); }
    public void reproduce() {}
}

class RobotA {
    public void speak() { System.out.println("Click!"); }
    public void sit() { System.out.println("Clank!"); }
    public void oilChange() {}
}

class CommunicateA {
    public static <P> void perform(P performer,
      Consumer<P> action1, Consumer<P> action2) {
        action1.accept(performer);
        action2.accept(performer);
    }
}

public class DogsAndRobotMethodReferences {
    public static void main(String[] args) {
        CommunicateA.perform(new PerformingDogA(),
          PerformingDogA::speak, PerformingDogA::sit);
        CommunicateA.perform(new RobotA(),
          RobotA::speak, RobotA::sit);
        CommunicateA.perform(new Mime(),
          Mime::walkAgainstTheWind,
          Mime::pushInvisibleWalls);
    }
}
/* Output:
Woof!
Sitting
Click!
Clank!
*/
```

### 使用**Suppliers**类的通用方法

​	略



## 枚举	

## 注解

来自

http://hollischuang.gitee.io/tobetopjavaer/#/basics/java-basic/meta-annotation 

### 元注解

说简单点，就是 定义其他注解的注解 。 比如Override这个注解，就不是一个元注解。而是通过元注解定义出来的。

```java
@Target(ElementType.METHOD)
@Retention(RetentionPolicy.SOURCE)
public @interface Override {
}
```

这里面的 @Target @Retention 就是元注解。

元注解有六个:@Target（表示该注解可以用于什么地方）、@Retention（表示再什么级别保存该注解信息）、@Documented（将此注解包含再javadoc中）、@Inherited（允许子类继承父类中的注解）、@Repeatable（1.8新增，允许一个注解在一个元素上使用多次）、@Native（1.8新增，修饰成员变量，表示这个变量可以被本地代码引用，常常被代码生成工具使用）。

### 自定义注解

除了元注解，都是自定义注解。通过元注解定义出来的注解。 如我们常用的Override 、Autowire等。 日常开发中也可以自定义一个注解，这些都是自定义注解。

### Java常用注解

@Override 表示当前方法覆盖了父类的方法

@Deprecated 表示方法已经过时,方法上有横线，使用时会有警告。

@SuppressWarnings 表示关闭一些警告信息(通知java编译器忽略特定的编译警告)

@SafeVarargs (jdk1.7更新) 表示：专门为抑制“堆污染”警告提供的。

@FunctionalInterface (jdk1.8更新) 表示：用来指定某个接口必须是函数式接口，否则就会编译出错。

#### Spring常用注解

@Configuration把一个类作为一个IoC容器，它的某个方法头上如果注册了@Bean，就会作为这个Spring容器中的Bean。

@Scope注解 作用域

@Lazy(true) 表示延迟初始化

@Service用于标注业务层组件

@Controller用于标注控制层组件@Repository用于标注数据访问组件，即DAO组件。

@Component泛指组件，当组件不好归类的时候，我们可以使用这个注解进行标注。

@Scope用于指定scope作用域的（用在类上）

@PostConstruct用于指定初始化方法（用在方法上）

@PreDestory用于指定销毁方法（用在方法上）

@DependsOn：定义Bean初始化及销毁时的顺序

@Primary：自动装配时当出现多个Bean候选者时，被注解为@Primary的Bean将作为首选者，否则将抛出异常

@Autowired 默认按类型装配，如果我们想使用按名称装配，可以结合@Qualifier注解一起使用。如下： @Autowired @Qualifier("personDaoBean") 存在多个实例配合使用

@Resource默认按名称装配，当找不到与名称匹配的bean才会按类型装配。

@PostConstruct 初始化注解

@PreDestroy 摧毁注解 默认 单例 启动就加载

### 注解与反射结合

注解和反射经常结合在一起使用，在很多框架的代码中都能看到他们结合使用的影子

可以通过反射来判断类，方法，字段上是否有某个注解以及获取注解中的值, 获取某个类中方法上的注解代码示例如下：

```java
Class<?> clz = bean.getClass();
Method[] methods = clz.getMethods();
for (Method method : methods) {
    if (method.isAnnotationPresent(EnableAuth.class)) {
        String name = method.getAnnotation(EnableAuth.class).name();
    }

```

通过isAnnotationPresent判断是否存在某个注解，通过getAnnotation获取注解对象，然后获取值。

**示例**

示例参考：https://blog.csdn.net/KKALL1314/article/details/96481557

自己写了一个例子，实现功能如下：

一个类的某些字段上被注解标识，在读取该属性时，将注解中的默认值赋给这些属性，没有标记的属性不赋值

```java
@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.FIELD)
@Documented
@Inherited
public @interface MyAnno {
    String value() default "有注解";
}
```

定义一个类

```java
@Data
@ToString
public class Person {
    @MyAnno
    private String stra;
    private String strb;
    private String strc;

    public Person(String str1,String str2,String str3){
        super();
        this.stra = str1;
        this.strb = str2;
        this.strc = str3;
    }

}
```

这里给str1加了注解，并利用反射解析并赋值：

```java
public class MyTest {
    public static void main(String[] args) {
        //初始化全都赋值无注解
        Person person = new Person("无注解","无注解","无注解");
        //解析注解
        doAnnoTest(person);
        System.out.println(person.toString());
    }

  private static void doAnnoTest(Object obj) {
        Class clazz = obj.getClass();
        Field[] declareFields = clazz.getDeclaredFields();
        for (Field field:declareFields) {
            //检查该字段是否使用了某个注解
            if(field.isAnnotationPresent(MyAnno.class)){
                MyAnno anno = field.getAnnotation(MyAnno.class);
                if(anno!=null){
                    String fieldName = field.getName();
                    try {
                        Method setMethod = clazz.getDeclaredMethod("set" + fieldName.substring(0, 1).toUpperCase() + fieldName.substring(1),String.class);
                        //获取注解的属性
                        String annoValue = anno.value();
                        //将注解的属性值赋给对应的属性
                        setMethod.invoke(obj,annoValue);
                    }catch (NoSuchMethodException e){
                        e.printStackTrace();
                    } catch (IllegalAccessException e) {
                        e.printStackTrace();
                    } catch (InvocationTargetException e) {
                        e.printStackTrace();
                    }

                }
            }
            
        }
    }

}
```

运行结果：

```java
Person(stra=有注解, strb=无注解, strc=无注解)

``

当开发者使用了Annotation 修饰了类、方法、Field 等成员之后，这些 Annotation 不会自己生效，必须由开发者提供相应的代码来提取并处理 Annotation 信息。这些处理提取和处理 Annotation 的代码统称为 APT（Annotation Processing Tool)。

注解的提取需要借助于 Java 的反射技术，反射比较慢，所以注解使用时也需要谨慎计较时间成本。
```

### 如何自定义一个注解？

在Java中，类使用class定义，接口使用interface定义，注解和接口的定义差不多，增加了一个@符号，即@interface，代码如下：

```java
public @interface EnableAuth {

}
```

注解中可以定义成员变量，用于信息的描述，跟接口中方法的定义类似，代码如下：

```java
public @interface EnableAuth {
    String name();
}
```

还可以添加默认值：

```java
public @interface EnableAuth {
    String name() default "猿天地";
}
```

上面的介绍只是完成了自定义注解的第一步，开发中日常使用注解大部分是用在类上，方法上，字段上，示列代码如下：

```java
@Target(ElementType.METHOD)
@Retention(RetentionPolicy.RUNTIME)
@Documented
public @interface EnableAuth {

}
```

Target

用于指定被修饰的注解修饰哪些程序单元，也就是上面说的类，方法，字段

Retention

用于指定被修饰的注解被保留多长时间，分别SOURCE（注解仅存在于源码中，在class字节码文件中不包含）,CLASS（默认的保留策略，注解会在class字节码文件中存在，但运行时无法获取）,RUNTIME（注解会在class字节码文件中存在，在运行时可以通过反射获取到）三种类型，如果想要在程序运行过程中通过反射来获取注解的信息需要将Retention设置为RUNTIME

Documented

用于指定被修饰的注解类将被javadoc工具提取成文档

Inherited

用于指定被修饰的注解类将具有继承性

### Spring常用注解

@Configuration把一个类作为一个IoC容器，它的某个方法头上如果注册了@Bean，就会作为这个Spring容器中的Bean。

@Scope注解 作用域

@Lazy(true) 表示延迟初始化

@Service用于标注业务层组件、

@Controller用于标注控制层组件@Repository用于标注数据访问组件，即DAO组件。

@Component泛指组件，当组件不好归类的时候，我们可以使用这个注解进行标注。

@Scope用于指定scope作用域的（用在类上）

@PostConstruct用于指定初始化方法（用在方法上）

@PreDestory用于指定销毁方法（用在方法上）

@DependsOn：定义Bean初始化及销毁时的顺序

@Primary：自动装配时当出现多个Bean候选者时，被注解为@Primary的Bean将作为首选者，否则将抛出异常

@Autowired 默认按类型装配，如果我们想使用按名称装配，可以结合@Qualifier注解一起使用。如下：

@Autowired @Qualifier("personDaoBean") 存在多个实例配合使用

@Resource默认按名称装配，当找不到与名称匹配的bean才会按类型装配。

### Spring中的这几个注解有什么区别：@Component 、@Repository、@Service、@Controller

1. @Component指的是组件，@Controller，@Repository和@Service 注解都被@Component修饰，用于代码中区分表现层，持久层和业务层的组件，代码中组件不好归类时可以使用@Component来标注

2. 当前版本只有区分的作用，未来版本可能会添加更丰富的功能

## 序列化

​	如果能够将一个对象声明为是“持久性”的，并为我们处理掉所有细节，那将会显得十分方便。

​	Java 的对象序列化将那些实现了 Serializable 接口的对象转换成一个字节序列，并能够在以后将这个字节序列完全恢复为原来的对象。这一过程甚至可通过网络进行，这意味着序列化机制能自动弥补不同操作系统之间的差异。也就是说，可以在运行 Windows 系统的计算机上创建一个对象，将其序列化，通过网络将它发送给一台运行 Unix 系统的计算机，然后在那里准确地重新组装，而却不必担心数据在不同机器上的表示会不同，也不必关心字节的顺序或者其他任何细节。

​	对象序列化可以实现轻量级持久性（lightweight persistence），“持久性”意味着一个对象的生存周期并不取决于程序是否正在执行它可以生存于程序的调用之间。通过将一个序列化对象写入磁盘，然后在重新调用程序时恢复该对象，就能够实现持久性的效果。之所以称其为“轻量级”，是因为不能用某种"persistent"（持久）关键字来简单地定义一个对象，并让系统自动维护其他细节问题（尽管将来有可能实现）。

​	对象序列化的概念加入到语言中是为了支持两种主要特性。一是 Java 的远程方法调用（Remote Method Invocation，RMI），它使存活于其他计算机上的对象使用起来就像是存活于本机上一样。当向远程对象发送消息时，需要通过对象序列化来传输参数和返回值。

​	再者，对 Java Beans 来说，对象的序列化也是必需的（在撰写本文时被视为失败的技术），使用一个 Bean 时，一般情况下是在设计阶段对它的状态信息进行配置。这种状态信息必须保存下来，并在程序启动时进行后期恢复，这种具体工作就是由对象序列化完成的。

​	要序列化一个对象，首先要创建某些 OutputStream 对象，然后将其封装在一个 ObjectOutputStream 对象内。这时，只需调用 writeObject() 即可将对象序列化，并将其发送给 OutputStream（对象化序列是基于字节的，因要使用 InputStream 和 OutputStream 继承层次结构）。要反向进行该过程（即将一个序列还原为一个对象），需要将一个 InputStream 封装在 ObjectInputStream 内，然后调用 readObject()。和往常一样，我们最后获得的是一个引用，它指向一个向上转型的 Object，所以必须向下转型才能直接设置它们。

​	对象序列化特别“聪明”的一个地方是它不仅保存了对象的“全景图”，而且能追踪对象内所包含的所有引用，并保存那些对象；接着又能对对象内包含的每个这样的引用进行追踪，依此类推。这种情况有时被称为“对象网”，单个对象可与之建立连接，而且它还包含了对象的引用数组以及成员对象。如果必须保持一套自己的对象序列化机制，那么维护那些可追踪到所有链接的代码可能会显得非常麻烦。然而，由于 Java 的对象序列化似乎找不出什么缺点，所以请尽量不要自己动手，让它用优化的算法自动维护整个对象网。下面这个例子通过对链接的对象生成一个 worm（蠕虫）对序列化机制进行了测试。每个对象都与 worm 中的下一段链接，同时又与属于不同类（Data）的对象引用数组链接：

```java
// serialization/Worm.java
// Demonstrates object serialization
import java.io.*;
import java.util.*;
class Data implements Serializable {
    private int n;
    Data(int n) { this.n = n; }
    @Override
    public String toString() {
        return Integer.toString(n);
    }
}
public class Worm implements Serializable {
    private static Random rand = new Random(47);
    private Data[] d = {
            new Data(rand.nextInt(10)),
            new Data(rand.nextInt(10)),
            new Data(rand.nextInt(10))
    };
    private Worm next;
    private char c;
    // Value of i == number of segments
    public Worm(int i, char x) {
        System.out.println("Worm constructor: " + i);
        c = x;
        if(--i > 0)
            next = new Worm(i, (char)(x + 1));
    }
    public Worm() {
        System.out.println("No-arg constructor");
    }
    @Override
    public String toString() {
        StringBuilder result = new StringBuilder(":");
        result.append(c);
        result.append("(");
        for(Data dat : d)
            result.append(dat);
        result.append(")");
        if(next != null)
            result.append(next);
        return result.toString();
    }
    public static void
    main(String[] args) throws ClassNotFoundException,
            IOException {
        Worm w = new Worm(6, 'a');
        System.out.println("w = " + w);
        try(
                ObjectOutputStream out = new ObjectOutputStream(
                        new FileOutputStream("worm.dat"))
        ) {
            out.writeObject("Worm storage\n");
            out.writeObject(w);
        }
        try(
                ObjectInputStream in = new ObjectInputStream(
                        new FileInputStream("worm.dat"))
        ) {
            String s = (String)in.readObject();
            Worm w2 = (Worm)in.readObject();
            System.out.println(s + "w2 = " + w2);
        }
        try(
                ByteArrayOutputStream bout =
                        new ByteArrayOutputStream();
                ObjectOutputStream out2 =
                        new ObjectOutputStream(bout)
        ) {
            out2.writeObject("Worm storage\n");
            out2.writeObject(w);
            out2.flush();
            try(
                    ObjectInputStream in2 = new ObjectInputStream(
                            new ByteArrayInputStream(
                                    bout.toByteArray()))
            ) {
                String s = (String)in2.readObject();
                Worm w3 = (Worm)in2.readObject();
                System.out.println(s + "w3 = " + w3);
            }
        }
    }
}
```

输出为：

```
Worm constructor: 6
Worm constructor: 5
Worm constructor: 4
Worm constructor: 3
Worm constructor: 2
Worm constructor: 1
w = :a(853):b(119):c(802):d(788):e(199):f(881)
Worm storage
w2 = :a(853):b(119):c(802):d(788):e(199):f(881)
Worm storage
w3 = :a(853):b(119):c(802):d(788):e(199):f(881)
```

​	Worm 内的 Data 对象数组是用随机数初始化的（这样就不用怀疑编译器保留了某种原始信息），每个 Worm 段都用一个 char 加以标记。该 char 是在递归生成链接的 Worm 列表时自动产生的。要创建一个 Worm，必须告诉构造器你所希望的它的长度。在产生下一个引用时，要调用 Worm 构造器，并将长度减 1，以此类推。最后一个 next 引用则为 null（空），表示已到达 Worm 的尾部

​	以上这些操作都使得事情变得更加复杂，从而加大了对象序列化的难度。然而，真正的序列化过程却是非常简单的。一旦从另外某个流创建了 ObjectOutputstream，writeObject() 就会将对象序列化。注意也可以为一个 String 调用 writeObject() 也可以用与 DataOutputStream 相同的方法写人所有基本数据类型（它们具有同样的接口）。

​	有两段看起来相似的独立的代码。一个读写的是文件，而另一个读写的是字节数组（ByteArray），可利用序列化将对象读写到任何 DatalnputStream 或者 DataOutputStream。

​	从输出中可以看出，被还原后的对象确实包含了原对象中的所有链接。

​	注意在对一个 Serializable 对象进行还原的过程中，没有调用任何构造器，包括默认的构造器。整个对象都是通过从 InputStream 中取得数据恢复而来的。

#### 查找类

​	将一个对象从它的序列化状态中恢复出来，有哪些工作是必须的呢？举个例子来说，假如我们将一个对象序列化，并通过网络将其作为文件传送给另一台计算机，那么，另一台计算机上的程序可以只利用该文件内容来还原这个对象吗？

回答这个问题的最好方法就是做一个实验。下面这个文件位于本章的子目录下：

```java
// serialization/Alien.java
// A serializable class
import java.io.*;
public class Alien implements Serializable {}
```

而用于创建和序列化一个 Alien 对象的文件也位于相同的目录下：

```java
// serialization/FreezeAlien.java
// Create a serialized output file
import java.io.*;
public class FreezeAlien {
    public static void main(String[] args) throws Exception {
        try(
                ObjectOutputStream out = new ObjectOutputStream(
                        new FileOutputStream("X.file"));
        ) {
            Alien quellek = new Alien();
            out.writeObject(quellek);
        }
    }
}
```

一旦该程序被编译和运行，它就会在 c12 目录下产生一个名为 X.file 的文件。以下代码位于一个名为 xiles 的子目录下：

```java
// serialization/xfiles/ThawAlien.java
// Recover a serialized file
// {java serialization.xfiles.ThawAlien}
// {RunFirst: FreezeAlien}
package serialization.xfiles;
import java.io.*;
public class ThawAlien {
    public static void main(String[] args) throws Exception {
        ObjectInputStream in = new ObjectInputStream(
                new FileInputStream(new File("X.file")));
        Object mystery = in.readObject();
        System.out.println(mystery.getClass());
    }
}
```

输出为：

```java
class Alien
```

为了正常运行，必须保证 Java 虚拟机能找到相关的.class 文件。

<!-- Controlling Serialization -->

#### 控制序列化

​	也许要考虑特殊的安全问题，而且你不希望对象的某一部分被序列化；或者一个对象被还原以后，某子对象需要重新创建，从而不必将该子对象序列化

​	在这些特殊情况下，可通过实现 Externalizable 接口——代替实现 Serializable 接口-来对序列化过程进行控制。这个 Externalizable 接口继承了 Serializable 接口，同时增添了两个方法：writeExternal() 和 readExternal()。这两个方法会在序列化和反序列化还原的过程中被自动调用，以便执行一些特殊操作。

```java
// serialization/Blips.java
// Simple use of Externalizable & a pitfall
import java.io.*;
class Blip1 implements Externalizable {
    public Blip1() {
        System.out.println("Blip1 Constructor");
    }
    @Override
    public void writeExternal(ObjectOutput out)
            throws IOException {
        System.out.println("Blip1.writeExternal");
    }
    @Override
    public void readExternal(ObjectInput in)
            throws IOException, ClassNotFoundException {
        System.out.println("Blip1.readExternal");
    }
}
class Blip2 implements Externalizable {
    Blip2() {
        System.out.println("Blip2 Constructor");
    }
    @Override
    public void writeExternal(ObjectOutput out)
            throws IOException {
        System.out.println("Blip2.writeExternal");
    }
    @Override
    public void readExternal(ObjectInput in)
            throws IOException, ClassNotFoundException {
        System.out.println("Blip2.readExternal");
    }
}
public class Blips {
    public static void main(String[] args) {
        System.out.println("Constructing objects:");
        Blip1 b1 = new Blip1();
        Blip2 b2 = new Blip2();
        try(
                ObjectOutputStream o = new ObjectOutputStream(
                        new FileOutputStream("Blips.serialized"))
        ) {
            System.out.println("Saving objects:");
            o.writeObject(b1);
            o.writeObject(b2);
        } catch(IOException e) {
            throw new RuntimeException(e);
        }
        // Now get them back:
        System.out.println("Recovering b1:");
        try(
                ObjectInputStream in = new ObjectInputStream(
                        new FileInputStream("Blips.serialized"))
        ) {
            b1 = (Blip1)in.readObject();
        } catch(IOException | ClassNotFoundException e) {
            throw new RuntimeException(e);
        }
        // OOPS! Throws an exception:
        //- System.out.println("Recovering b2:");
        //- b2 = (Blip2)in.readObject();
    }
}
```

输出为：

```
Constructing objects:
Blip1 Constructor
Blip2 Constructor
Saving objects:
Blip1.writeExternal
Blip2.writeExternal
Recovering b1:
Blip1 Constructor
Blip1.readExternal
```

​	没有恢复 Blip2 对象的原因是那样做会导致一个异常。你找出 Blip1 和 Blip2 之间的区别了吗？Blipl 的构造器是“公共的”（public），Blip2 的构造器却不是，这样就会在恢复时造成异常。试试将 Blip2 的构造器变成 public 的，然后删除//注释标记，看看是否能得到正确的结果。

​	恢复 b1 后，会调用 Blip1 默认构造器。这与恢复一个 Serializable 对象不同。对于 Serializable 对象，对象完全以它存储的二进制位为基础来构造，而不调用构造器。而对于一个 Externalizable 对象，所有普通的默认构造器都会被调用（包括在字段定义时的初始化），然后调用 readExternal() 必须注意这一点--所有默认的构造器都会被调用，才能使 Externalizable 对象产生正确的行为。

下面这个例子示范了如何完整保存和恢复一个 Externalizable 对象：

```java
// serialization/Blip3.java
// Reconstructing an externalizable object
import java.io.*;
public class Blip3 implements Externalizable {
    private int i;
    private String s; // No initialization
    public Blip3() {
        System.out.println("Blip3 Constructor");
// s, i not initialized
    }
    public Blip3(String x, int a) {
        System.out.println("Blip3(String x, int a)");
        s = x;
        i = a;
// s & i initialized only in non-no-arg constructor.
    }
    @Override
    public String toString() { return s + i; }
    @Override
    public void writeExternal(ObjectOutput out)
            throws IOException {
        System.out.println("Blip3.writeExternal");
// You must do this:
        out.writeObject(s);
        out.writeInt(i);
    }
    @Override
    public void readExternal(ObjectInput in)
            throws IOException, ClassNotFoundException {
        System.out.println("Blip3.readExternal");
// You must do this:
        s = (String)in.readObject();
        i = in.readInt();
    }
    public static void main(String[] args) {
        System.out.println("Constructing objects:");
        Blip3 b3 = new Blip3("A String ", 47);
        System.out.println(b3);
        try(
                ObjectOutputStream o = new ObjectOutputStream(
                        new FileOutputStream("Blip3.serialized"))
        ) {
            System.out.println("Saving object:");
            o.writeObject(b3);
        } catch(IOException e) {
            throw new RuntimeException(e);
        }
// Now get it back:
        System.out.println("Recovering b3:");
        try(
                ObjectInputStream in = new ObjectInputStream(
                        new FileInputStream("Blip3.serialized"))
        ) {
            b3 = (Blip3)in.readObject();
        } catch(IOException | ClassNotFoundException e) {
            throw new RuntimeException(e);
        }
        System.out.println(b3);
    }
}
```

输出为：

```
Constructing objects:
Blip3(String x, int a)
A String 47
Saving object:
Blip3.writeExternal
Recovering b3:
Blip3 Constructor
Blip3.readExternal
A String 47
```

​	其中，字段 s 和 i 只在第二个构造器中初始化，而不是在默认的构造器中初始化。这意味着假如不在 readExternal() 中初始化 s 和 i，s 就会为 null，而 i 就会为零（因为在创建对象的第一步中将对象的存储空间清理为 0）。如果注释掉跟随于"You must do this”后面的两行代码，然后运行程序，就会发现当对象被还原后，s 是 null，而 i 是零。

​	我们如果从一个 Externalizable 对象继承，通常需要调用基类版本的 writeExternal() 和 readExternal() 来为基类组件提供恰当的存储和恢复功能。

​	因此，为了正常运行，我们不仅需要在 writeExternal() 方法（没有任何默认行为来为 Externalizable 对象写入任何成员对象）中将来自对象的重要信息写入，还必须在 readExternal() 方法中恢复数据。起先，可能会有一点迷惑，因为 Externalizable 对象的默认构造行为使其看起来似乎像某种自动发生的存储与恢复操作。但实际上并非如此。

##### transient 关键字

​	当我们对序列化进行控制时，可能某个特定子对象不想让 Java 的序列化机制自动保存与恢复。如果子对象表示的是我们不希望将其序列化的敏感信息（如密码），通常就会面临这种情况。即使对象中的这些信息是 private（私有）属性，一经序列化处理，人们就可以通过读取文件或者拦截网络传输的方式来访问到它。

​	有一种办法可防止对象的敏感部分被序列化，就是将类实现为 Externalizable，如前面所示。这样一来，没有任何东西可以自动序列化，并且可以在 writeExternal() 内部只对所需部分进行显式的序列化。

然而，如果我们正在操作的是一个 Seralizable 对象，那么所有序列化操作都会自动进行。为了能够予以控制，可以用 transient（瞬时）关键字逐个字段地关闭序列化，它的意思是“不用麻烦你保存或恢复数据——我自己会处理的"。

例如，假设某个 Logon 对象保存某个特定的登录会话信息，登录的合法性通过校验之后，我们想把数据保存下来，但不包括密码。为做到这一点，最简单的办法是实现 Serializable，并将 password 字段标志为 transient，下面是具体的代码：

```java
// serialization/Logon.java
// Demonstrates the "transient" keyword
import java.util.concurrent.*;
import java.io.*;
import java.util.*;
import onjava.Nap;
public class Logon implements Serializable {
    private Date date = new Date();
    private String username;
    private transient String password;
    public Logon(String name, String pwd) {
        username = name;
        password = pwd;
    }
    @Override
    public String toString() {
        return "logon info: \n username: " +
                username + "\n date: " + date +
                "\n password: " + password;
    }
    public static void main(String[] args) {
        Logon a = new Logon("Hulk", "myLittlePony");
        System.out.println("logon a = " + a);
        try(
                ObjectOutputStream o =
                        new ObjectOutputStream(
                                new FileOutputStream("Logon.dat"))
        ) {
            o.writeObject(a);
        } catch(IOException e) {
            throw new RuntimeException(e);
        }
        new Nap(1);
// Now get them back:
        try(
                ObjectInputStream in = new ObjectInputStream(
                        new FileInputStream("Logon.dat"))
        ) {
            System.out.println(
                    "Recovering object at " + new Date());
            a = (Logon)in.readObject();
        } catch(IOException | ClassNotFoundException e) {
            throw new RuntimeException(e);
        }
        System.out.println("logon a = " + a);
    }
}
```

输出为：

```
logon a = logon info:
username: Hulk
date: Tue May 09 06:07:47 MDT 2017
password: myLittlePony
Recovering object at Tue May 09 06:07:49 MDT 2017
logon a = logon info:
username: Hulk
date: Tue May 09 06:07:47 MDT 2017
password: null
```

​	可以看到，其中的 date 和 username 是一般的（不是 transient 的），所以它们会被自动序列化。而 password 是 transient 的，所以不会被自动保存到磁盘；另外，自动序列化机制也不会尝试去恢复它。当对象被恢复时，password 就会变成 null。注意，虽然 toString() 是用重载后的+运算符来连接 String 对象，但是 null 引用会被自动转换成字符串 null。

​	我们还可以发现：date 字段被存储了到磁盘并从磁盘上被恢复了出来，而且没有再重新生成。由于 Externalizable 对象在默认情况下不保存它们的任何字段，所以 transient 关键字只能和 Serializable 对象一起使用。

##### Externalizable的替代方法

如果不是特别坚持实现 Externalizable 接口，那么还有另一种方法。我们可以实现 Serializable 接口，并添加（注意我说的是“添加”，而非“覆盖”或者“实现”）名为 writeObject() 和 readObject() 的方法。这样一旦对象被序列化或者被反序列化还原，就会自动地分别调用这两个方法。也就是说，只要我们提供了这两个方法，就会使用它们而不是默认的序列化机制。

这些方法必须具有准确的方法特征签名：

```java
private void writeObject(ObjectOutputStream stream) throws IOException

private void readObject(ObjectInputStream stream) throws IOException, ClassNotFoundException
```

​	从设计的观点来看，现在事情变得真是不可思议。首先，我们可能会认为由于这些方法不是基类或者 Serializable 接口的一部分，所以应该在它们自己的接口中进行定义。但是注意它们被定义成了 private，这意味着它们仅能被这个类的其他成员调用。然而，实际上我们并没有从这个类的其他方法中调用它们，而是 ObjectOutputStream 和 ObjectInputStream 对象的 writeObject() 和 readobject() 方法调用你的对象的 writeObject() 和 readObject() 方法（注意关于这里用到的相同方法名，我尽量抑制住不去谩骂。一句话：混乱）。读者可能想知道 ObjectOutputStream 和 ObjectInputStream 对象是怎样访问你的类中的 private 方法的。我们只能假设这正是序列化神奇的一部分。

​	还有另外一个技巧。在你的 writeObject() 内部，可以调用 defaultWriteObject() 来选择执行默认的 writeObject()。类似地，在 readObject() 内部，我们可以调用 defaultReadObject()，下面这个简单的例子演示了如何对一个 Serializable 对象的存储与恢复进行控制：

```java
// serialization/SerialCtl.java
// Controlling serialization by adding your own
// writeObject() and readObject() methods
import java.io.*;
public class SerialCtl implements Serializable {
    private String a;
    private transient String b;
    public SerialCtl(String aa, String bb) {
        a = "Not Transient: " + aa;
        b = "Transient: " + bb;
    }
    @Override
    public String toString() { return a + "\n" + b; }
    private void writeObject(ObjectOutputStream stream)
            throws IOException {
        stream.defaultWriteObject();
        stream.writeObject(b);
    }
    private void readObject(ObjectInputStream stream)
            throws IOException, ClassNotFoundException {
        stream.defaultReadObject();
        b = (String)stream.readObject();
    }
    public static void main(String[] args) {
        SerialCtl sc = new SerialCtl("Test1", "Test2");
        System.out.println("Before:\n" + sc);
        try (
                ByteArrayOutputStream buf =
                        new ByteArrayOutputStream();
                ObjectOutputStream o =
                        new ObjectOutputStream(buf);
        ) {
            o.writeObject(sc);
// Now get it back:
            try (
                    ObjectInputStream in =
                            new ObjectInputStream(
                                    new ByteArrayInputStream(
                                            buf.toByteArray()));
            ) {
                SerialCtl sc2 = (SerialCtl)in.readObject();
                System.out.println("After:\n" + sc2);
            }
        } catch(IOException | ClassNotFoundException e) {
            throw new RuntimeException(e);
        }
    }
}
```

输出为：

```
Before:
Not Transient: Test1
Transient: Test2
After:
Not Transient: Test1
Transient: Test2
```

​	在这个例子中，有一个 String 字段是普通字段，而另一个是 transient 字段，用来证明非 transient 字段由 defaultWriteObject() 方法保存，而 transient 字段必须在程序中明确保存和恢复。字段是在构造器内部而不是在定义处进行初始化的，以此可以证实它们在反序列化还原期间没有被一些自动化机制初始化。





##### 版本控制

​	有时可能想要改变可序列化类的版本（比如源类的对象可能保存在数据库中）。虽然 Java 支持这种做法，但是你可能只在特殊的情况下才这样做，此外，还需要对它有相当深程度的了解（在这里我们就不再试图达到这一点）。从 http://java.oracle.com 下的 JDK 文档中对这一主题进行了非常彻底的论述。

#### 使用持久化

​	一个比较诱人的使用序列化技术的想法是：存储程序的一些状态，以便我们随后可以很容易地将程序恢复到当前状态。但是在我们能够这样做之前，必须回答几个问题。如果我们将两个对象-它们都具有指向第三个对象的引用-进行序列化，会发生什么情况？当我们从它们的序列化状态恢复这两个对象时，第三个对象会只出现一次吗？如果将这两个对象序列化成独立的文件，然后在代码的不同部分对它们进行反序列化还原，又会怎样呢？

下面这个例子说明了上述问题：

```java
// serialization/MyWorld.java
import java.io.*;
import java.util.*;
class House implements Serializable {}
class Animal implements Serializable {
    private String name;
    private House preferredHouse;
    Animal(String nm, House h) {
        name = nm;
        preferredHouse = h;
    }
    @Override
    public String toString() {
        return name + "[" + super.toString() +
                "], " + preferredHouse + "\n";
    }
}
public class MyWorld {
    public static void main(String[] args) {
        House house = new House();
        List<Animal> animals = new ArrayList<>();
        animals.add(
                new Animal("Bosco the dog", house));
        animals.add(
                new Animal("Ralph the hamster", house));
        animals.add(
                new Animal("Molly the cat", house));
        System.out.println("animals: " + animals);
        try(
                ByteArrayOutputStream buf1 =
                        new ByteArrayOutputStream();
                ObjectOutputStream o1 =
                        new ObjectOutputStream(buf1)
        ) {
            o1.writeObject(animals);
            o1.writeObject(animals); // Write a 2nd set
// Write to a different stream:
            try(
                    ByteArrayOutputStream buf2 = new ByteArrayOutputStream();
                    ObjectOutputStream o2 = new ObjectOutputStream(buf2)
            ) {
                o2.writeObject(animals);
// Now get them back:
                try(
                        ObjectInputStream in1 =
                                new ObjectInputStream(
                                        new ByteArrayInputStream(
                                                buf1.toByteArray()));
                        ObjectInputStream in2 =
                                new ObjectInputStream(
                                        new ByteArrayInputStream(
                                                buf2.toByteArray()))
                ) {
                    List
                            animals1 = (List)in1.readObject(),
                            animals2 = (List)in1.readObject(),
                            animals3 = (List)in2.readObject();
                    System.out.println(
                            "animals1: " + animals1);
                    System.out.println(
                            "animals2: " + animals2);
                    System.out.println(
                            "animals3: " + animals3);
                }
            }
        } catch(IOException | ClassNotFoundException e) {
            throw new RuntimeException(e);
        }
    }
}
```

输出为：

```
animals: [Bosco the dog[Animal@15db9742],
House@6d06d69c
, Ralph the hamster[Animal@7852e922], House@6d06d69c
, Molly the cat[Animal@4e25154f], House@6d06d69c
]
animals1: [Bosco the dog[Animal@7ba4f24f],
House@3b9a45b3
, Ralph the hamster[Animal@7699a589], House@3b9a45b3
, Molly the cat[Animal@58372a00], House@3b9a45b3
]
animals2: [Bosco the dog[Animal@7ba4f24f],
House@3b9a45b3
, Ralph the hamster[Animal@7699a589], House@3b9a45b3
, Molly the cat[Animal@58372a00], House@3b9a45b3
]
animals3: [Bosco the dog[Animal@4dd8dc3],
House@6d03e736
, Ralph the hamster[Animal@568db2f2], House@6d03e736
, Molly the cat[Animal@378bf509], House@6d03e736
]
```



这里有一件有趣的事：我们可以通过一个字节数组来使用对象序列化，从而实现对任何可 Serializable 对象的“深度复制"（deep copy）—— 深度复制意味着我们复制的是整个对象网，而不仅仅是基本对象及其引用。复制对象将在本书的 [附录：传递和返回对象 ]() 一章中进行深入地探讨。

​	当然，我们期望这些反序列化还原后的对象地址与原来的地址不同。但请注意，在 animals1 和 animals2 中却出现了相同的地址，包括二者共享的那个指向 House 对象的引用。另一方面，当恢复 animals3 时，系统无法知道另一个流内的对象是第一个流内的对象的别名，因此它会产生出完全不同的对象网。

只要将任何对象序列化到单一流中，就可以恢复出与我们写出时一样的对象网，并且没有任何意外重复复制出的对象。当然，我们可以在写出第一个对象和写出最后一个对象期间改变这些对象的状态，但是这是我们自己的事，无论对象在被序列化时处于什么状态（无论它们和其他对象有什么样的连接关系），它们都可以被写出。

​	最安全的做法是将其作为“原子”操作进行序列化。如果我们序列化了某些东西，再去做其他一些工作，再来序列化更多的东西，如此等等，那么将无法安全地保存系统状态。取而代之的是，将构成系统状态的所有对象都置入单一容器内，并在一个操作中将该容器直接写出。然后同样只需一次方法调用，即可以将其恢复。

```java
// serialization/AStoreCADState.java
// Saving the state of a fictitious CAD system
import java.io.*;
import java.util.*;
import java.util.stream.*;
enum Color { RED, BLUE, GREEN }
abstract class Shape implements Serializable {
    private int xPos, yPos, dimension;
    private static Random rand = new Random(47);
    private static int counter = 0;
    public abstract void setColor(Color newColor);
    public abstract Color getColor();
    Shape(int xVal, int yVal, int dim) {
        xPos = xVal;
        yPos = yVal;
        dimension = dim;
    }
    public String toString() {
        return getClass() + "color[" + getColor() +
                "] xPos[" + xPos + "] yPos[" + yPos +
                "] dim[" + dimension + "]\n";
    }
    public static Shape randomFactory() {
        int xVal = rand.nextInt(100);
        int yVal = rand.nextInt(100);
        int dim = rand.nextInt(100);
        switch(counter++ % 3) {
            default:
            case 0: return new Circle(xVal, yVal, dim);
            case 1: return new Square(xVal, yVal, dim);
            case 2: return new Line(xVal, yVal, dim);
        }
    }
}
class Circle extends Shape {
    private static Color color = Color.RED;
    Circle(int xVal, int yVal, int dim) {
        super(xVal, yVal, dim);
    }
    public void setColor(Color newColor) {
        color = newColor;
    }
    public Color getColor() { return color; }
}
class Square extends Shape {
    private static Color color = Color.RED;
    Square(int xVal, int yVal, int dim) {
        super(xVal, yVal, dim);
    }
    public void setColor(Color newColor) {
        color = newColor;
    }
    public Color getColor() { return color; }
}
class Line extends Shape {
    private static Color color = Color.RED;
    public static void
    serializeStaticState(ObjectOutputStream os)
            throws IOException { os.writeObject(color); }
    public static void
    deserializeStaticState(ObjectInputStream os)
            throws IOException, ClassNotFoundException {
        color = (Color)os.readObject();
    }
    Line(int xVal, int yVal, int dim) {
        super(xVal, yVal, dim);
    }
    public void setColor(Color newColor) {
        color = newColor;
    }
    public Color getColor() { return color; }
}
public class AStoreCADState {
    public static void main(String[] args) {
        List<Class<? extends Shape>> shapeTypes =
                Arrays.asList(
                        Circle.class, Square.class, Line.class);
        List<Shape> shapes = IntStream.range(0, 10)
                .mapToObj(i -> Shape.randomFactory())
                .collect(Collectors.toList());
        // Set all the static colors to GREEN:
        shapes.forEach(s -> s.setColor(Color.GREEN));
        // Save the state vector:
        try(
                ObjectOutputStream out =
                        new ObjectOutputStream(
                                new FileOutputStream("CADState.dat"))
        ) {
            out.writeObject(shapeTypes);
            Line.serializeStaticState(out);
            out.writeObject(shapes);
        } catch(IOException e) {
            throw new RuntimeException(e);
        }
        // Display the shapes:
        System.out.println(shapes);
    }
}
```

输出为：

```java
[class Circlecolor[GREEN] xPos[58] yPos[55] dim[93]
, class Squarecolor[GREEN] xPos[61] yPos[61] dim[29]
, class Linecolor[GREEN] xPos[68] yPos[0] dim[22]
, class Circlecolor[GREEN] xPos[7] yPos[88] dim[28]
, class Squarecolor[GREEN] xPos[51] yPos[89] dim[9]
, class Linecolor[GREEN] xPos[78] yPos[98] dim[61]
, class Circlecolor[GREEN] xPos[20] yPos[58] dim[16]
, class Squarecolor[GREEN] xPos[40] yPos[11] dim[22]
, class Linecolor[GREEN] xPos[4] yPos[83] dim[6]
, class Circlecolor[GREEN] xPos[75] yPos[10] dim[42]
]
```

Shape 类实现了 Serializable，所以任何自 Shape 继承的类也都会自动是 Serializable 的。每个 Shape 都含有数据，而且每个派生自 Shape 的类都包含一个 static 字段，用来确定各种 Shape 类型的颜色（如果将 static 字段置入基类，只会产生一个 static 字段，因为 static 字段不能在派生类中复制）。可对基类中的方法进行重载，以便为不同的类型设置颜色（static 方法不会动态绑定，所以这些都是普通的方法）。每次调用 randomFactory() 方法时，它都会使用不同的随机数作为 Shape 的数据，从而创建不同的 Shape。

在 main() 中，一个 ArrayList 用于保存 Class 对象，而另一个用于保存几何形状。

恢复对象相当直观：

```java
// serialization/RecoverCADState.java
// Restoring the state of the fictitious CAD system
// {RunFirst: AStoreCADState}
import java.io.*;
import java.util.*;
public class RecoverCADState {
    @SuppressWarnings("unchecked")
    public static void main(String[] args) {
        try(
                ObjectInputStream in =
                        new ObjectInputStream(
                                new FileInputStream("CADState.dat"))
        ) {
// Read in the same order they were written:
            List<Class<? extends Shape>> shapeTypes =
                    (List<Class<? extends Shape>>)in.readObject();
            Line.deserializeStaticState(in);
            List<Shape> shapes =
                    (List<Shape>)in.readObject();
            System.out.println(shapes);
        } catch(IOException | ClassNotFoundException e) {
            throw new RuntimeException(e);
        }
    }
}
```

输出为：

```java
[class Circlecolor[RED] xPos[58] yPos[55] dim[93]
, class Squarecolor[RED] xPos[61] yPos[61] dim[29]
, class Linecolor[GREEN] xPos[68] yPos[0] dim[22]
, class Circlecolor[RED] xPos[7] yPos[88] dim[28]
, class Squarecolor[RED] xPos[51] yPos[89] dim[9]
, class Linecolor[GREEN] xPos[78] yPos[98] dim[61]
, class Circlecolor[RED] xPos[20] yPos[58] dim[16]
, class Squarecolor[RED] xPos[40] yPos[11] dim[22]
, class Linecolor[GREEN] xPos[4] yPos[83] dim[6]
, class Circlecolor[RED] xPos[75] yPos[10] dim[42]
]
```

可以看到，xPos，yPos 以及 dim 的值都被成功地保存和恢复了，但是对 static 信息的读取却出现了问题。所有读回的颜色应该都是“3”，但是真实情况却并非如此。Circle 的值为 1（定义为 RED），而 Square 的值为 0（记住，它们是在构造器中被初始化的）。看上去似乎 static 数据根本没有被序列化！确实如此——尽管 Class 类是 Serializable 的，但它却不能按我们所期望的方式运行。所以假如想序列化 static 值，必须自己动手去实现。

这正是 Line 中的 serializeStaticState() 和 deserializeStaticState() 两个 static 方法的用途。可以看到，它们是作为存储和读取过程的一部分被显式地调用的。（注意必须维护写入序列化文件和从该文件中读回的顺序。）因此，为了使 CADStatejava 正确运转起来，你必须：

1. 为几何形状添加 serializeStaticState() 和 deserializeStaticState()
2. 移除 ArrayList shapeTypes 以及与之有关的所有代码。
3. 在几何形状内添加对新的序列化和反序列化还原静态方法的调用。

另一个要注意的问题是安全，因为序列化也会将 private 数据保存下来。如果你关心安全问题，那么应将其标记成 transient，但是这之后，还必须设计一种安全的保存信息的方法，以便在执行恢复时可以复位那些 private 变量。

#### XML

对象序列化的一个重要限制是它只是 Java 的解决方案：只有 Java 程序才能反序列化这种对象。一种更具互操作性的解决方案是将数据转换为 XML 格式，这可以使其被各种各样的平台和语言使用。

因为 XML 十分流行，所以用它来编程时的各种选择不胜枚举，包括随 JDK 发布的 javax.xml.*类库。我选择使用 Elliotte Rusty Harold 的开源 XOM 类库（可从 www.xom.nu 下载并获得文档），因为它看起来最简单，同时也是最直观的用 Java 产生和修改 XML 的方式。另外，XOM 还强调了 XML 的正确性。

作为一个示例，假设有一个 APerson 对象，它包含姓和名，你想将它们序列化到 XML 中。下面的 APerson 类有一个 getXML() 方法，它使用 XOM 来产生被转换为 XML 的 Element 对象的 APerson 数据；还有一个构造器，接受 Element 并从中抽取恰当的 APerson 数据（注意，XML 示例都在它们自己的子目录中）：

```java
// serialization/APerson.java
// Use the XOM library to write and read XML
// nu.xom.Node comes from http://www.xom.nu
import nu.xom.*;
import java.io.*;
import java.util.*;
public class APerson {
    private String first, last;
    public APerson(String first, String last) {
        this.first = first;
        this.last = last;
    }
    // Produce an XML Element from this APerson object:
    public Element getXML() {
        Element person = new Element("person");
        Element firstName = new Element("first");
        firstName.appendChild(first);
        Element lastName = new Element("last");
        lastName.appendChild(last);
        person.appendChild(firstName);
        person.appendChild(lastName);
        return person;
    }
    // Constructor restores a APerson from XML:
    public APerson(Element person) {
        first = person
                .getFirstChildElement("first").getValue();
        last = person
                .getFirstChildElement("last").getValue();
    }
    @Override
    public String toString() {
        return first + " " + last;
    }
    // Make it human-readable:
    public static void
    format(OutputStream os, Document doc)
            throws Exception {
        Serializer serializer =
                new Serializer(os,"ISO-8859-1");
        serializer.setIndent(4);
        serializer.setMaxLength(60);
        serializer.write(doc);
        serializer.flush();
    }
    public static void main(String[] args) throws Exception {
        List<APerson> people = Arrays.asList(
                new APerson("Dr. Bunsen", "Honeydew"),
                new APerson("Gonzo", "The Great"),
                new APerson("Phillip J.", "Fry"));
        System.out.println(people);
        Element root = new Element("people");
        for(APerson p : people)
            root.appendChild(p.getXML());
        Document doc = new Document(root);
        format(System.out, doc);
        format(new BufferedOutputStream(
                new FileOutputStream("People.xml")), doc);
    }
}
```

输出为：

```xml
[Dr. Bunsen Honeydew, Gonzo The Great, Phillip J. Fry]
<?xml version="1.0" encoding="ISO-8859-1"?>
<people>
    <person>
        <first>Dr. Bunsen</first>
        <last>Honeydew</last>
    </person>
    <person>
        <first>Gonzo</first>
        <last>The Great</last>
    </person>
    <person>
        <first>Phillip J.</first>
        <last>Fry</last>
    </person>
</people>
```

XOM 的方法都具有相当的自解释性，可以在 XOM 文档中找到它们。XOM 还包含一个 Serializer 类，你可以在 format() 方法中看到它被用来将 XML 转换为更具可读性的格式。如果只调用 toXML()，那么所有东西都会混在一起，因此 Serializer 是一种便利工具。

从 XML 文件中反序列化 Person 对象也很简单：


```java
// serialization/People.java
// nu.xom.Node comes from http://www.xom.nu
// {RunFirst: APerson}
import nu.xom.*;
import java.io.File;
import java.util.*;
public class People extends ArrayList<APerson> {
    public People(String fileName) throws Exception {
        Document doc =
                new Builder().build(new File(fileName));
        Elements elements =
                doc.getRootElement().getChildElements();
        for(int i = 0; i < elements.size(); i++)
            add(new APerson(elements.get(i)));
    }
    public static void main(String[] args) throws Exception {
        People p = new People("People.xml");
        System.out.println(p);
    }
}
/* Output:
[Dr. Bunsen Honeydew, Gonzo The Great, Phillip J. Fry]
*/
```

People 构造器使用 XOM 的 Builder.build() 方法打开并读取一个文件，而 getChildElements() 方法产生了一个 Elements 列表（不是标准的 Java List，只是一个拥有 size() 和 get() 方法的对象，因为 Harold 不想强制人们使用特定版本的 Java，但是仍旧希望使用类型安全的容器）。在这个列表中的每个 Element 都表示一个 Person 对象，因此它可以传递给第二个 Person 构造器。注意，这要求你提前知道 XML 文件的确切结构，但是这经常会有些问题。如果文件结构与你预期的结构不匹配，那么 XOM 将抛出异常。对你来说，如果你缺乏有关将来的 XML 结构的信息，那么就有可能会编写更复杂的代码去探测 XML 文档，而不是只对其做出假设。

为了获取这些示例去编译它们，你必须将 XOM 发布包中的 JAR 文件放置到你的类路径中。

这里只给出了用 Java 和 XOM 类库进行 XML 编程的简介，更详细的信息可以浏览 www.xom.nu 。



<!-- 分页 -->



# 设计模式



# RocketMQ

https://www.bilibili.com/video/BV1RE411r75d

## MQ介绍

消息队列是一种“先进先出”的数据结构。

![](https://pic.imgdb.cn/item/5ed46cb3c2a9a83be56c0d15.jpg)



应用场景主要包括以下3个方面

* 应用解耦

  替代RPC，降低耦合

* 流量削峰

  缓存请求，避免请求压垮服务器

* 数据分发

### 1.2 MQ的优点和缺点

优点：解耦、削峰、数据分发。

缺点：

* 系统可用性降低

  外部依赖越多，稳定性越差，一旦MQ宕机，就会对业务造成影响。

* 系统复杂度提高

* 一致性问题

### 1.3 MQ产品比较

https://www.cnblogs.com/nov5026/archive/2018/08/22/9518520.html

## RocketMq快速入门

### 2.1准备工作

#### 下载RocketMq

http://rocketmq.apache.org/

版本4.7.0

#### 系统环境

* Linux64位环境
* JDK1.8
* 源码安装需要安装Maven 3.2.x

### 2.2 安装RocketMQ

#### 安装步骤

以二进制包进行安装

1. 解压安装包
2. 进入安装目录

```sh
unzip rocketmq-all-4.7.0-bin-release.zip
```

如果是source

```sh
 unzip rocketmq-all-4.8.0-source-release.zip
 cd rocketmq-all-4.8.0/
 mvn -Prelease-all -DskipTests clean install -U
 cd distribution/target/rocketmq-4.8.0/rocketmq-4.8.0
```



#### 目录介绍

* bin: 启动脚本，包括shell脚本和cmd脚本
* conf：实例配置文件，包括broker配置文件，logback配置文件等
* lib：依赖jar包，包括netty，commons-lang,FastJson等

### 2.3启动RocketMQ

1. 启动NameServer

```sh
# 1.启动NameServer
nohup sh bin/mqnamesrv &
# 2.查看启动日志
tail -f ~/logs/rocketmqlogs/namesrv.log
```

2. 启动broker

```sh
# 1.启动broker
nohup sh bin/mqbroker -n localhost:9876 &
# 2.查看启动日志
tail -f ~/logs/rocketmqlogs/broker.log
```

* 问题描述

  RocketMQ默认的虚拟机内存较大，启动Broker如果因为内存不足失败，需要编辑如下两个配置文件，修改JVM内存大小

```sh
vim runbroker.sh
vim runserver.sh
```

* 参考设置

```sh
JAVA_OPT="${JAVA_OPT} -server -Xms128m -Xmx256m -Xmn256m -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m"
```

### 2.4测试RocketMQ

#### 消息发送

```sh
# 1.设置环境变量
export NAMESRV_ADDR=localhost:9876
# 2.使用安装包的Demo发送信息
sh bin/tools.sh org.apache.rocketmq.example.quickstart.Producer
```

#### 接受消息

```sh
# 1.设置环境变量
export NAMESRV_ADDR=localhost:9876
# 2.接受消息
sh bin/tools.sh org.apache.rocketmq.example.quickstart.Consumer
```



### 2.5 关闭RocketMQ

```sh
# 1.关闭NameServer
sh bin/mqshutdown namesrv
# 2.关闭Broker
sh bin/mqshutdown broker
```

## RocketMQ集群搭建

### 3.1各角色介绍

* Producer：消息的发送者。
* Consumer：消息的接收者。
* Broker：暂存和传递消息。
* NameServer：管理Broker。
* Topic：区分消息的分类；一个发送者发送消息给一个或者多个Topic；一个消息的接收者可以订阅一个或多个Topic消息。
* MessageQueue：相当于是Topic的分区；用于并行发送和接收消息。

![](https://pic.imgdb.cn/item/5ed6606cc2a9a83be5e5e6c1.jpg)

### 3.2 集群搭建方式

#### 集群特点

* NameServer是一个几乎无状态节点，可集群部署，节点之间无任何信息同步。
* Broker部署相对复杂，Broker分为Master和Slave，一个Master可以对应多个Slave，但是一个Slave只能对应一个Master，Master和Slave的对应关系通过指定相同的BrokerName，不同的BrokerId来定义，BrokerId为0代表Master，非0代表Slave。Master也可以部署多个。每个Broker与NameServer集群中的所有节点建立长连接，定时注册Topic信息到所有NameServer。
* Producer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从NameServer取Topic路由信息，并向提供Topic服务的Master建立长连接，且定时向Master发送心跳。Producer完全无状态，可集群部署。
* Consumer与NameServer集群中其中一个节点（随机选择）建立长连接，定期从NameServer取路由信息，并向提供Topic服务器的Master和Slave建立长连接，且定时向Master和Slave发送心跳。Consumer既可以从Master订阅信息，也可以从Slave订阅信息，订阅规则由Broker决定。

#### 集群模式

**单Master模式**

这种方式风险较大，一旦Broker重启或者宕机时，会导致整个服务不可用。不建议线上环境使用，可用于本地测试。

**多Master模式**

一个集群无Slave，全是Master，例如2个或者3个Master。

* 优点：配置简单，单个Master宕机或重启维护对应用无影响，在磁盘配置为RAID10时，即使机器宕机不可恢复情况下，由于RAID10磁盘非常可靠，消息也不会丢失（异步刷盘丢失少了信息，同步刷盘一条不丢），性能最高。
* 缺点：单台机器宕机期间，这台机器上未被消费的消息在机器恢复之前不可订阅，消息实时性会受到影响。

**多Master多Slave模式（异步）**

每个Master配置一个Slave，有多对Master-Slave，HA采用异步复制方式，主备有短暂消息延迟（毫秒级），这种模式的优缺点如下：

* 优点：即使磁盘损坏，消息丢失非常少，且消息实时性不受影响，同时Master宕机后，消费者仍然从Slave消费，而且此过程对应用透明，不需要人工干预，性能同多Master模式几乎一样。
* 缺点：Master宕机，磁盘损坏情况下会丢失少量信息。

**多Master多Slave模式（同步）**

每个Master配置一个Slave，有多对Master-Slave，HA采用同步双写方式，即主备都写成功，才向应用返回成功。

* 优点：数据与服务都无单点故障，Master宕机情况下，消息无延迟，服务可用性与数据可用性都非常高。
* 缺点：性能比异步复制模式略低（大约低10%左右），发送单个消息的RT略高，且在目前版本主节点宕机后，备机不能自动切换为主机。

### 3.3 双主双从集群搭建

#### 总体架构

![](https://pic.imgdb.cn/item/5ed8f998c2a9a83be58351d9.jpg)



#### 集群工作流程

1.启动NameServer，NameServer起来后监听端口，等待Broker、Producer、Consumer连上来，相当于一个路由控制中心。

2.Broker启动，跟所有的NameServer保持长连接，定时发送心跳包。心跳包中包含当前Broker信息（IP+端口等）以及存储所有Topic信息。注册成功后，NameServer集群中就有Topic跟Broker的映射关系。

3.收发消息前，先创建Topic，创建Topic时需要指定该Topic要存储在哪些Broker上，也可以在发送消息时自动创建Topic。

4.Producer发送消息，启动时先跟NameServer集群中的其中一台建立长连接，并从NameServer中获取当前发送的Topic存在哪些Broker上，轮询从队列列表中选择一个队列，然后与队列所在的Broker建立长连接从而向Broker发送消息。

5.Consumer跟Producer类似，跟其中一台NameServer建立长连接，获取当前订阅Topic存在哪些Broker上，然后直接跟Broker建立连接通道，开始消费消息。

#### 服务器环境

| 序号 | IP             | 角色              | 架构模式        |
| ---- | -------------- | ----------------- | --------------- |
| 1    | 192.168.25.xxx | nameserver,broker | Master1,Slave2  |
| 2    | 192.168.25.yyy | nameserver,broker | Master2，Salve1 |

#### Host添加信息

```sh
vim /etc/hosts
```

配置如下

```sh
# nameserver
192.168.1.6 rocketmq-nameserver1
192.168.1.7 rocketmq-nameserver2
# broker
192.168.1.6 rocketmq-master1
192.168.1.6 rocketmq-slave2
192.168.1.7 rocketmq-master2
192.168.1.7 rocketmq-slave1
```

完成后重启网卡,centos8是

```sh
nmcli c reload
```

#### 防火墙配置

宿主机需要远程访问虚拟机的rocketmq服务和web服务，需要开放相关的端口号，简单粗暴的方式是直接关闭防火墙。

```sh
# 关闭防火墙
systemclt stop firewalld.service
# 查看防火墙状态
firewall-cmd --state
# 禁止firewall开机启动
systemctl disable firewalld.service
```

或者为了安全，只开放特定端口，RocketMQ默认使用3个端口:9876、10911、11011。如果防火墙没有关闭的话，那么防火墙必须开放这些端口。

* nameserver 默认使用9876端口
* master 默认使用10911端口
* slave 默认使用11011端口

执行以下命令：

```sh
# 开放name server默认端口
firewall-cmd --remove-port=9876/tcp --permanet
# 开放master默认端口
firewall-cmd --remove-port=10911/tcp --permanet
# 开放slave默认端口(当前集群模式可不开启)
firewall-cmd --remove-port=11011/tcp --permanet
# 重启防火墙
firewall-cmd --reload
```

#### 环境变量配置

```sh
vim /etc/profile
```

在profile文件的末尾加入如下命令：

```sh
#set rocketmq
ROCKETMQ_HOME=/opt/rocketmq-all-4.7.0-bin-release
PATH=$PATH:$ROCKETMQ_HOME/bin
export ROCKETMQ_HOME PATH
```

输入:wq保存并退出，并使得配置立即生效

```sh
source /etc/profile
```

#### 创建消息存储路径

```sh
mkdir /opt/rocketmq/store
mkdir /opt/rocketmq/store/commitlog
mkdir /opt/rocketmq/store/consumequeue
mkdir /opt/rocketmq/store/index
```

#### broker配置文件

**master1**

服务器：192.168.1.6

```sh
vim /opt/rocketmq-all-4.7.0-bin-release/conf/2m-2s-sync/broker-a.properties
```

修改配置如下：

```sh
# 所属集群名字
brokerClusterName=rocketmq-cluster
# broker名字，注意此处不同的配置文件填写的不一样
brokerName=broker-a
#0 表示 Master， >0 表示 slave
brokerId=0
#nameServer地址，分号分割
namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876
#在发送消息时，自动创建服务器不存在的topic，默认创建的队列数
defaultTopicQueueNums=4
#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭
autoCreateTopicEnable=true
#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭
autoCreateSubscriptionGroup=true
#Broker 对外服务的监听端口
listenPort=10911
#删除文件时间点，默认凌晨4点
deletewhen=04
#文件保留时间，默认 48 小时
fileReserveredTime=120
#commitLog每个文件的大小默认1G
mapedFileSizeCommitLog=1073741824
#ConsumeQueue每个文件默认存30w条，根据业务情况调整
mapedFileSizeConsumeQueue=300000
#destroyMapedFileIntervalForcibly=120000
#redeleteHangedFileInterval=120000
diskMaxUsedSpaceRatio=88
#存储路径
storePathRootDir=/opt/rocketmq/store
#commitLog 存储路径
storePathCommitLog=/opt/rocketmq/store/commitlog
#消费队列存储路径
storePathConsumeQueue=/opt/rocketmq/store/consumequeue
#消费索引存储路径
storePathIndex=/opt/rocketmq/store/index
# checkpoint 文件存储路径
storeCheckpoint=/opt/rocketmq/store/checkpoint
#abort 文件存储地址
abortFile=/opt/rocketmq/store/abort
#限制消息的大小
maxMessageSize=65536
# Broker的角色
# - ASYNC_MASTER 异步复制Master
# - SYNC_MASTER 同步双写MASTER
# - SLAVE
brokerRole=SYNC_MASTER
# 刷盘方式
# - ASYNC_FLUSH 异步刷盘
# - SYNC_FLUSH 同步刷盘
flushDiskType=SYNC_FLUSH

```

**Slave2**

服务器：192.168.1.6

```sh
vim /opt/rocketmq-all-4.7.0-bin-release/conf/2m-2s-sync/broker-b-s.properties
```

修改配置如下：

```sh
# 所属集群名字
brokerClusterName=rocketmq-cluster
# broker名字，注意此处不同的配置文件填写的不一样
brokerName=broker-b
#0 表示 Master， >0 表示 slave
brokerId=1
#nameServer地址，分号分割
namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876
#在发送消息时，自动创建服务器不存在的topic，默认创建的队列数
defaultTopicQueueNums=4
#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭
autoCreateTopicEnable=true
#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭
autoCreateSubscriptionGroup=true
#Broker 对外服务的监听端口
listenPort=11011
#删除文件时间点，默认凌晨4点
deletewhen=04
#文件保留时间，默认 48 小时
fileReserveredTime=120
#commitLog每个文件的大小默认1G
mapedFileSizeCommitLog=1073741824
#ConsumeQueue每个文件默认存30w条，根据业务情况调整
mapedFileSizeConsumeQueue=300000
#destroyMapedFileIntervalForcibly=120000
#redeleteHangedFileInterval=120000
diskMaxUsedSpaceRatio=88
#存储路径
storePathRootDir=/opt/rocketmq/store1
#commitLog 存储路径
storePathCommitLog=/opt/rocketmq/store1/commitlog
#消费队列存储路径
storePathConsumeQueue=/opt/rocketmq/store1/consumequeue
#消费索引存储路径
storePathIndex=/opt/rocketmq/store1/index
# checkpoint 文件存储路径
storeCheckpoint=/opt/rocketmq/store1/checkpoint
#abort 文件存储地址
abortFile=/opt/rocketmq/store/abort
#限制消息的大小
maxMessageSize=65536
# Broker的角色
# - ASYNC_MASTER 异步复制Master
# - SYNC_MASTER 同步双写MASTER
# - SLAVE
brokerRole=SLAVE
# 刷盘方式
# - ASYNC_FLUSH 异步刷盘
# - SYNC_FLUSH 同步刷盘
flushDiskType=ASYNC_FLUSH

```

**Master2**

服务器：192.168.1.7

```sh
vim /opt/rocketmq-all-4.7.0-bin-release/conf/2m-2s-sync/broker-b.properties
```

修改配置如下：

```sh
# 所属集群名字
brokerClusterName=rocketmq-cluster
# broker名字，注意此处不同的配置文件填写的不一样
brokerName=broker-b
#0 表示 Master， >0 表示 slave
brokerId=0
#nameServer地址，分号分割
namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876
#在发送消息时，自动创建服务器不存在的topic，默认创建的队列数
defaultTopicQueueNums=4
#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭
autoCreateTopicEnable=true
#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭
autoCreateSubscriptionGroup=true
#Broker 对外服务的监听端口
listenPort=10911
#删除文件时间点，默认凌晨4点
deletewhen=04
#文件保留时间，默认 48 小时
fileReserveredTime=120
#commitLog每个文件的大小默认1G
mapedFileSizeCommitLog=1073741824
#ConsumeQueue每个文件默认存30w条，根据业务情况调整
mapedFileSizeConsumeQueue=300000
#destroyMapedFileIntervalForcibly=120000
#redeleteHangedFileInterval=120000
diskMaxUsedSpaceRatio=88
#存储路径
storePathRootDir=/opt/rocketmq/store
#commitLog 存储路径
storePathCommitLog=/opt/rocketmq/store/commitlog
#消费队列存储路径
storePathConsumeQueue=/opt/rocketmq/store/consumequeue
#消费索引存储路径
storePathIndex=/opt/rocketmq/store/index
# checkpoint 文件存储路径
storeCheckpoint=/opt/rocketmq/store/checkpoint
#abort 文件存储地址
abortFile=/opt/rocketmq/store/abort
#限制消息的大小
maxMessageSize=65536
# Broker的角色
# - ASYNC_MASTER 异步复制Master
# - SYNC_MASTER 同步双写MASTER
# - SLAVE
brokerRole=SYNC_MASTER
# 刷盘方式
# - ASYNC_FLUSH 异步刷盘
# - SYNC_FLUSH 同步刷盘
flushDiskType=SYNC_FLUSH
```

**Slave2**

服务器：192.168.1.7

```sh
vim /opt/rocketmq-all-4.7.0-bin-release/conf/2m-2s-sync/broker-a-s.properties
```

修改配置如下：

```sh
# 所属集群名字
brokerClusterName=rocketmq-cluster
# broker名字，注意此处不同的配置文件填写的不一样
brokerName=broker-a
#0 表示 Master， >0 表示 slave
brokerId=1
#nameServer地址，分号分割
namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876
#在发送消息时，自动创建服务器不存在的topic，默认创建的队列数
defaultTopicQueueNums=4
#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭
autoCreateTopicEnable=true
#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭
autoCreateSubscriptionGroup=true
#Broker 对外服务的监听端口
listenPort=11011
#删除文件时间点，默认凌晨4点
deletewhen=04
#文件保留时间，默认 48 小时
fileReserveredTime=120
#commitLog每个文件的大小默认1G
mapedFileSizeCommitLog=1073741824
#ConsumeQueue每个文件默认存30w条，根据业务情况调整
mapedFileSizeConsumeQueue=300000
#destroyMapedFileIntervalForcibly=120000
#redeleteHangedFileInterval=120000
diskMaxUsedSpaceRatio=88
#存储路径
storePathRootDir=/opt/rocketmq/store1
#commitLog 存储路径
storePathCommitLog=/opt/rocketmq/store1/commitlog
#消费队列存储路径
storePathConsumeQueue=/opt/rocketmq/store1/consumequeue
#消费索引存储路径
storePathIndex=/opt/rocketmq/store1/index
# checkpoint 文件存储路径
storeCheckpoint=/opt/rocketmq/store1/checkpoint
#abort 文件存储地址
abortFile=/opt/rocketmq/store1/abort
#限制消息的大小
maxMessageSize=65536
# Broker的角色
# - ASYNC_MASTER 异步复制Master
# - SYNC_MASTER 同步双写MASTER
# - SLAVE
brokerRole=SLAVE
# 刷盘方式
# - ASYNC_FLUSH 异步刷盘
# - SYNC_FLUSH 同步刷盘
flushDiskType=ASYNC_FLUSH

```

#### 修改启动脚本文件

**runbroker.sh**

```sh
vim /bin/runbroker.sh
```

需要根据内存大小进行适当的对JVM参数进行调整：

```sh
# 开发环境配置 JVM Configuration
JAVA_OPT="${JAVA_OPT} -server -Xms256m -Xmx256m -Xmn128m"
```

**runserver.sh**

```sh
vim /bin/runserver.sh
```

需要根据内存大小进行适当的对JVM参数进行调整：

```sh
JAVA_OPT="${JAVA_OPT} -server -Xms256m -Xmx256m -Xmn128m -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m"
```

#### 服务启动

**启动NameServer集群**

分别在两台集群启动NameServer

```sh
cd /bin
nohup sh mqnamesrv &
```

**启动Broker集群**

master1：

```sh
cd /bin
nohup sh mqbroker -c /opt/rocketmq-all-4.7.0-bin-release/conf/2m-2s-sync/broker-a.properties &
```

slave2：

```sh
cd /bin
nohup sh mqbroker -c /opt/rocketmq-all-4.7.0-bin-release/conf/2m-2s-sync/broker-b-s.properties &
```

master2：

```sh
cd /bin
nohup sh mqbroker -c /opt/rocketmq-all-4.7.0-bin-release/conf/2m-2s-sync/broker-b.properties &
```

slave1：

```sh
cd /bin
nohup sh mqbroker -c /opt/rocketmq-all-4.7.0-bin-release/conf/2m-2s-sync/broker-a-s.properties &
```

#### 查看日志

```sh
# nameSrv日志
tail -f ~/logs/rocketmqlogs/namesrv.log
# broker日志
tail -f ~/logs/rocketmqlogs/broker.log
```

**易错点**

中间遇到无法启动broker的情况，是因为使用了同一个storePath，改一下就好。

### 3.4 mqadmin 管理工具

#### 使用方式

进入mq安装位置，在bin目录下执行 ./mqadmin {command} {args}

#### 命令介绍

略

### 3.5 集群监控平台搭建

#### 概述

我们需要自己对rocketmq-console进行编译打包运行。

https://github.com/apache/rocketmq-externals

#### 3.5.2 下载并编译打包

```
git clone https://github.com/apache/rocketmq-externals
cd rocketmq-console
mvn clean package -Dmaven.test.skip=true
```

注意：打包前在rocket-console中配置namesrv集群地址：

```
rocketmq.config.namesrvAddr=192.168.1.6:9876;192.168.1.7:9876;
```

上传到linux服务器上，启动rocketmq-console：

```
java -jar rocketmq-console-ng-1.0.0.jar
```

启动成功，我们就可以通过浏览器http://localhost:8080进入控制台界面了。

## 消息发送样例

* 创建maven项目并导入客户端依赖

  ```xml
  <!-- https://mvnrepository.com/artifact/org.apache.rocketmq/rocketmq-client -->
  <dependency>
      <groupId>org.apache.rocketmq</groupId>
      <artifactId>rocketmq-client</artifactId>
      <version>4.7.0</version>
  </dependency>
  ```

* 消息发送者步骤分析

```tex
1. 创建消息生产者producer,并指定生产者组名。
2. 指定NameServer地址
3. 启动producer
4. 创建消息对象，指定主题Topic、Tag和消息体
5. 发送消息
6. 关闭生产者Producer
```

* 消息消费者步骤分析

```tex
1. 创建消费者Consumer，指定消费者组名
2. 指定NameServer地址
3. 订阅主题Topic和Tag
4. 设置回调函数，处理消息
5. 启动消费者Consumer
```

### 4.1 基本样例

#### 4.1.1 消息发送

##### 1）发送同步消息

这种可靠性同步地发送方式使用比较广泛，比如：重要的消息通知，短信通知

```java
package cn.sorie.mq.rocketmq.base;

import org.apache.rocketmq.client.producer.DefaultMQProducer;
import org.apache.rocketmq.client.producer.SendResult;
import org.apache.rocketmq.client.producer.SendStatus;
import org.apache.rocketmq.common.message.Message;
import org.apache.rocketmq.remoting.common.RemotingHelper;

import java.util.concurrent.TimeUnit;

public class SyncProducer {
    public static void main(String[] args) throws Exception {
        //实例化生产者producer
        DefaultMQProducer producer = new DefaultMQProducer("group1");
        // 设置NameServer的地址
        producer.setNamesrvAddr("192.168.1.6:9876;192.168.1.7:9876");
        //启动producer
        producer.start();
        try {
            for (int i = 0; i < 10; i++) {
                //创建消息 指定topic,Tag和消息体
                Message msg = new Message("base",
                        "Tag1", ("Hello MQ" + i).getBytes(RemotingHelper.DEFAULT_CHARSET));
                //发送消息到一个broker
                SendResult sendResult = producer.send(msg);
                SendStatus status = sendResult.getSendStatus();
                String msgId = sendResult.getMsgId();
                int queueId = sendResult.getMessageQueue().getQueueId();

                //发送消息是否成功送达
                System.out.printf("发送状态：%s 消息id：%s 队列id:%d\n", status, msgId, queueId);
                TimeUnit.SECONDS.sleep(1);
            }
        } catch (Exception e) {
            e.printStackTrace();
        } finally {
            producer.shutdown();
        }
    }
}
```

#### 2)  发送异步消息

异步消息通常用在响应时间敏感的业务场景，即发送端不能容忍长时间等待Broker的响应。

```java
package cn.sorie.mq.rocketmq.base;

import org.apache.rocketmq.client.producer.DefaultMQProducer;
import org.apache.rocketmq.client.producer.SendCallback;
import org.apache.rocketmq.client.producer.SendResult;
import org.apache.rocketmq.client.producer.SendStatus;
import org.apache.rocketmq.common.message.Message;
import org.apache.rocketmq.remoting.common.RemotingHelper;

import java.util.concurrent.TimeUnit;

public class AsyncProducer {
    public static void main(String[] args) throws Exception {
        //实例化生产者producer
        DefaultMQProducer producer = new DefaultMQProducer("group1");
        // 设置NameServer的地址
        producer.setNamesrvAddr("192.168.1.6:9876;192.168.1.7:9876");
        //启动producer
        producer.start();
        producer.setRetryTimesWhenSendAsyncFailed(0);
        try {
            for (int i = 0; i < 10; i++) {
                //创建消息 指定topic,Tag和消息体
                final int index = i;
                Message msg = new Message("base",
                        "Tag1", ("Async Hello MQ" + i).getBytes(RemotingHelper.DEFAULT_CHARSET));
                //发送消息到一个broker
                producer.send(msg, new SendCallback() {
                    public void onSuccess(SendResult sendResult) {
                        SendStatus status = sendResult.getSendStatus();
                        String msgId = sendResult.getMsgId();
                        int queueId = sendResult.getMessageQueue().getQueueId();
                        System.out.printf("发送状态：%s 消息id：%s 队列id:%d\n", status, msgId, queueId);
                    }

                    public void onException(Throwable e) {
                        System.out.printf("%-10d Exception %s %n", index, e);
                    }
                });

                //发送消息是否成功送达
                TimeUnit.SECONDS.sleep(1);
            }
        } catch (Exception e) {
            e.printStackTrace();
        } finally {
            producer.shutdown();
        }
    }
}
```

#### 3) 单向消息

不在乎消息的发送状态，比如日志等消息。

```java
package cn.sorie.mq.rocketmq.base;

import org.apache.rocketmq.client.producer.DefaultMQProducer;
import org.apache.rocketmq.client.producer.SendCallback;
import org.apache.rocketmq.client.producer.SendResult;
import org.apache.rocketmq.client.producer.SendStatus;
import org.apache.rocketmq.common.message.Message;
import org.apache.rocketmq.remoting.common.RemotingHelper;

import java.util.concurrent.TimeUnit;

public class OneWayProducer {
    public static void main(String[] args) throws Exception {
        //实例化生产者producer
        DefaultMQProducer producer = new DefaultMQProducer("group1");
        // 设置NameServer的地址
        producer.setNamesrvAddr("192.168.1.6:9876;192.168.1.7:9876");
        //启动producer
        producer.start();
        try {
            for (int i = 0; i < 10; i++) {
                //创建消息 指定topic,Tag和消息体
                Message msg = new Message("base",
                        "Tag1", ("Hello MQ 单向消息" + i).getBytes(RemotingHelper.DEFAULT_CHARSET));
                //发送消息到一个broker
                producer.sendOneway(msg);


                //发送消息是否成功送达
                System.out.printf("发送状态：%n\n", i);
                TimeUnit.SECONDS.sleep(5);
            }
        } catch (Exception e) {
            e.printStackTrace();
        } finally {
            producer.shutdown();
        }
    }
}
```

### 4.1.2 消费消息

```java
public class Consumer {
    public static void main(String[] args) throws MQClientException {
        //1. 创建消费者Consumer，指定消费者组名
        DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("group1");
        //2. 指定NameServer地址
        consumer.setNamesrvAddr("192.168.1.6:9876;192.168.1.7:9876");
        //3. 订阅主题Topic和Tag
        consumer.subscribe("base", "Tag2");
        //4. 设置回调函数，处理消息
        consumer.registerMessageListener(new MessageListenerConcurrently() {
            //接收消息内容
            public ConsumeConcurrentlyStatus consumeMessage(List<MessageExt> list, ConsumeConcurrentlyContext consumeConcurrentlyContext) {
                for (MessageExt per : list) {
                    System.out.println(new String(per.getBody()));
                }
                return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;
            }
        });
        //5. 启动消费者Consumer
        consumer.start();
    }
}

```

#### 1） 负载均衡模式

消费者采用负载均衡模式方法消费消息，多个消费者共同消费队列消息，每个消费者处理的消息不同。 默认为负载均衡。

```java
package cn.sorie.mq.rocketmq.base.consumer;

import org.apache.rocketmq.client.consumer.DefaultMQPushConsumer;
import org.apache.rocketmq.client.consumer.listener.ConsumeConcurrentlyContext;
import org.apache.rocketmq.client.consumer.listener.ConsumeConcurrentlyStatus;
import org.apache.rocketmq.client.consumer.listener.MessageListenerConcurrently;
import org.apache.rocketmq.client.exception.MQClientException;
import org.apache.rocketmq.common.message.MessageExt;
import org.apache.rocketmq.common.protocol.heartbeat.MessageModel;

import java.util.List;

public class ClusterConsumer {
    public static void main(String[] args) throws MQClientException {
        //1. 创建消费者Consumer，指定消费者组名
        DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("group1");
        //2. 指定NameServer地址
        consumer.setNamesrvAddr("192.168.1.6:9876;192.168.1.7:9876");
        //3. 订阅主题Topic和Tag
        consumer.subscribe("base", "Tag1");
        //设置负载均衡
        consumer.setMessageModel(MessageModel.CLUSTERING);
        //4. 设置回调函数，处理消息
        consumer.registerMessageListener(new MessageListenerConcurrently() {
            //接收消息内容
            public ConsumeConcurrentlyStatus consumeMessage(List<MessageExt> list, ConsumeConcurrentlyContext consumeConcurrentlyContext) {
                for (MessageExt per : list) {
                    System.out.println(" 负载均衡" + new String(per.getBody()));
                }
                return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;
            }
        });
        //5. 启动消费者Consumer
        consumer.start();
    }
}
```



#### 2）广播模式

消费者采用广播的方式消费消息，每个消费者的消息是相同的。

```java
package cn.sorie.mq.rocketmq.base.consumer;

import org.apache.rocketmq.client.consumer.DefaultMQPushConsumer;
import org.apache.rocketmq.client.consumer.listener.ConsumeConcurrentlyContext;
import org.apache.rocketmq.client.consumer.listener.ConsumeConcurrentlyStatus;
import org.apache.rocketmq.client.consumer.listener.MessageListenerConcurrently;
import org.apache.rocketmq.client.exception.MQClientException;
import org.apache.rocketmq.common.message.MessageExt;
import org.apache.rocketmq.common.protocol.heartbeat.MessageModel;

import java.util.List;

public class ClusterConsumer {
    public static void main(String[] args) throws MQClientException {
        //1. 创建消费者Consumer，指定消费者组名
        DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("group1");
        //2. 指定NameServer地址
        consumer.setNamesrvAddr("192.168.1.6:9876;192.168.1.7:9876");
        //3. 订阅主题Topic和Tag
        consumer.subscribe("base", "Tag1");
        //设置负载均衡
        consumer.setMessageModel(MessageModel.BROADCASTING);
        //4. 设置回调函数，处理消息
        consumer.registerMessageListener(new MessageListenerConcurrently() {
            //接收消息内容
            public ConsumeConcurrentlyStatus consumeMessage(List<MessageExt> list, ConsumeConcurrentlyContext consumeConcurrentlyContext) {
                for (MessageExt per : list) {
                    System.out.println(" 负载均衡" + new String(per.getBody()));
                }
                return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;
            }
        });
        //5. 启动消费者Consumer
        consumer.start();
    }
}
```

### 4.2 顺序消息

​	消息有序指的是可以按照消息的发送顺序来消费消息(FIFO)。RocketMq可以严格的保证消息有序。可以分为分区有序或者全局有序。

​	顺序消费的原理解析，在默认情况下消息发送会采取Round Robin轮询方式把消息发送到不同的queue(分区队列)；而消费消息时从多个queue上拉取消息，这种情况发送和消费是不能保证顺序。但是如果控制发送的顺序只依次发送到同一个queue中，消费的时候只从这个queue依次拉取，则保证了顺序。当发送和消费参与的queue只有一个，则是全局有序；如果有多个queue参与，则是分区有序，即相对于每个queue，消息都是有序的。

#### 4.2.1 顺序消息生产

​	下面用订单进行分区有序的示例。一个订单的顺序：创建、付款、推送、完成。订单相同的消息会被先后发送到同一个队列中，消费时，同一个OrderId获取到的肯定是同一个队列。

```java
package cn.sorie.mq.rocketmq.order;

import org.apache.rocketmq.client.producer.*;
import org.apache.rocketmq.common.message.Message;
import org.apache.rocketmq.common.message.MessageQueue;
import org.apache.rocketmq.remoting.common.RemotingHelper;

import java.util.List;
import java.util.concurrent.TimeUnit;

public class Producer {
    public static void main(String[] args) throws Exception {
        //实例化生产者producer
        DefaultMQProducer producer = new DefaultMQProducer("group1");
        // 设置NameServer的地址
        producer.setNamesrvAddr("192.168.1.6:9876;192.168.1.7:9876");
        //启动producer
        producer.start();

        //构建消息集合
        List<OrderStep> orderStepList = OrderStep.buildOrders();
        try {
            //发送消息
            int i = 0;
            for (OrderStep per : orderStepList) {
                String body = per + "";
                Message msg = new Message("OrderTopic", "Order", "i" + i, body.getBytes());
                //参数一：消息对象
                //参数二：消息队列选择器
                //参数三：选择队列的业务标识（订单id）

                SendResult sendResult = producer.send(msg, new MessageQueueSelector() {
                    /**
                     * @param list    : 队列集合
                     * @param message : 消息对象
                     * @param o 业务标识参数
                     * @return
                     * @author soriee
                     * @date 2020/6/30 22:03
                     */
                    public MessageQueue select(List<MessageQueue> list, Message message, Object o) {
                        long orderId = (Long) o;
                        int index = (int) (orderId % list.size());
                        System.out.println("队列 " + index);
                        return list.get(index);
                    }
                }, per.getOrderId());
                System.out.println("发送成功: " + sendResult.toString());
                i++;
            }
        } catch (Exception e) {
            e.printStackTrace();
        } finally {
            producer.shutdown();
        }
    }
}
```

#### 4.2.2 顺序消息消费

```java
package cn.sorie.mq.rocketmq.order;

import org.apache.rocketmq.client.consumer.DefaultMQPushConsumer;
import org.apache.rocketmq.client.consumer.listener.*;
import org.apache.rocketmq.common.message.MessageExt;

import java.util.List;

public class Consumer {
    public static void main(String[] args) throws Exception{
        //1. 创建消费者Consumer，指定消费者组名
        DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("group1");
        //2. 指定NameServer地址
        consumer.setNamesrvAddr("192.168.1.6:9876;192.168.1.7:9876");
        //3. 订阅主题Topic和Tag
        consumer.subscribe("OrderTopic", "*");
        //4. 设置回调函数，处理消息
        consumer.registerMessageListener(new MessageListenerOrderly() {
            public ConsumeOrderlyStatus consumeMessage(List<MessageExt> list, ConsumeOrderlyContext consumeOrderlyContext) {
                for (MessageExt per : list) {
                    System.out.println("线程名称【" + Thread.currentThread().getName() + "】 消费消息：" + new String(per.getBody()));
                }
                return ConsumeOrderlyStatus.SUCCESS;
            }
        });
        //5. 启动消费者Consumer
        consumer.start();
    }
}
```

### 4.3 延时消息

​	比如电商里，提交了一个订单就可以发送一个延时消息，1h后去检查这个订单的状态，如果还是未付款就取消订单释放库存。

#### 4.3.1 启动消息消费者

```java
package cn.sorie.mq.rocketmq.delay;

import org.apache.rocketmq.client.consumer.DefaultMQPushConsumer;
import org.apache.rocketmq.client.consumer.listener.ConsumeOrderlyContext;
import org.apache.rocketmq.client.consumer.listener.ConsumeOrderlyStatus;
import org.apache.rocketmq.client.consumer.listener.MessageListenerOrderly;
import org.apache.rocketmq.common.message.MessageExt;

import java.util.List;

public class Consumer {
    public static void main(String[] args) throws Exception{
        //1. 创建消费者Consumer，指定消费者组名
        DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("group1");
        //2. 指定NameServer地址
        consumer.setNamesrvAddr("192.168.1.6:9876;192.168.1.7:9876");
        //3. 订阅主题Topic和Tag
        consumer.subscribe("topicDelay", "*");
        //4. 设置回调函数，处理消息
        consumer.registerMessageListener(new MessageListenerOrderly() {
            public ConsumeOrderlyStatus consumeMessage(List<MessageExt> list, ConsumeOrderlyContext consumeOrderlyContext) {
                for (MessageExt per : list) {
                    System.out.println("线程名称【" + Thread.currentThread().getName() + "】 消费消息：" + new String(per.getBody()) + System.currentTimeMillis());
                }
                return ConsumeOrderlyStatus.SUCCESS;
            }
        });
        //5. 启动消费者Consumer
        consumer.start();
    }
}
```

#### 4.3.2 发送延时消息

```java
package cn.sorie.mq.rocketmq.delay;

import org.apache.rocketmq.client.producer.DefaultMQProducer;
import org.apache.rocketmq.client.producer.SendResult;
import org.apache.rocketmq.client.producer.SendStatus;
import org.apache.rocketmq.common.message.Message;
import org.apache.rocketmq.remoting.common.RemotingHelper;

import java.util.concurrent.TimeUnit;

public class Producer {
    public static void main(String[] args) throws Exception {
        //实例化生产者producer
        DefaultMQProducer producer = new DefaultMQProducer("group1");
        // 设置NameServer的地址
        producer.setNamesrvAddr("192.168.1.6:9876;192.168.1.7:9876");
        //启动producer
        producer.start();
        try {
            for (int i = 0; i < 10; i++) {
                //创建消息 指定topic,Tag和消息体
                Message msg = new Message("topicDelay",
                        "Tag1", ("Hello MQ 延时消息" + i).getBytes(RemotingHelper.DEFAULT_CHARSET));
                //发送消息到一个broker
                msg.setDelayTimeLevel(2);
                SendResult sendResult = producer.send(msg);
                SendStatus status = sendResult.getSendStatus();
                String msgId = sendResult.getMsgId();
                int queueId = sendResult.getMessageQueue().getQueueId();

                //发送消息是否成功送达
                System.out.printf("发送状态：%s 消息id：%s 队列id:%d\n", status, msgId, queueId);
                TimeUnit.SECONDS.sleep(1);
            }
        } catch (Exception e) {
            e.printStackTrace();
        } finally {
            producer.shutdown();
        }
    }
}
```

#### 4.3.3 验证

您将会看到消息的消费比存储时间晚10秒

#### 4.3.4 使用限制

```java
// org.apache.rocketmq/store/config/MessageStoreConfig.java
private String messageDelayLevel = "1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h"
```

现在RocketMq并不支持任意时间的延时，需要设置几个固定延时等级，从1s到2h对应等级1到18

### 4.4 批量消息

​	批量消息能够显著提高传递小消息的性能，限制是这些批量消息应该有相同的topic，相同的waitStoreMsgOK，而且不能是延时消息。此外，这一批的消息总大小应不超过4MB。

#### 4.4.1 发送批量消息

​	如果您每次只发送4MB不到的消息，则很容易批处理，样例如下：

```java
String topic = "BatchTest";
List<Message> msgs = new ArrayList<>();
msgs.add(new Message(topic, "TagA", "OrderID001", "Hello World 0".getBytes));
msgs.add(new Message(topic, "TagA", "OrderID001", "Hello World 0".getBytes));
msgs.add(new Message(topic, "TagA", "OrderID001", "Hello World 0".getBytes));
try {
    producer.send(msgs);
} catch (Exception e) {
    e.printTrace();
}
```

​	如果消费的总长度可能大于4MB时，这时候最好把消费进行分割。

创建一个分割器

一个消息的长度=topic长度和body长度以及消息额外的属性以及日志开销20字节。

```java
public class ListSplitter implements Iterator<List<Message>> {
    private final int SIZE_LIMIT = 1000 * 1000;
    private final List<Message> messages;
    private int currIndex;
    public ListSplitter(List<Message> messages) {
            this.messages = messages;
    }
    @Override public boolean hasNext() {
        return currIndex < messages.size();
    }
    @Override public List<Message> next() {
        int nextIndex = currIndex;
        int totalSize = 0;
        for (; nextIndex < messages.size(); nextIndex++) {
            Message message = messages.get(nextIndex);
            int tmpSize = message.getTopic().length() + message.getBody().length;
            Map<String, String> properties = message.getProperties();
            for (Map.Entry<String, String> entry : properties.entrySet()) {
                tmpSize += entry.getKey().length() + entry.getValue().length();
            }
            tmpSize = tmpSize + 20; //for log overhead
            if (tmpSize > SIZE_LIMIT) {
                //it is unexpected that single message exceeds the SIZE_LIMIT
                //here just let it go, otherwise it will block the splitting process
                if (nextIndex - currIndex == 0) {
                   //if the next sublist has no element, add this one and then break, otherwise just break
                   nextIndex++;  
                }
                break;
            }
            if (tmpSize + totalSize > SIZE_LIMIT) {
                break;
            } else {
                totalSize += tmpSize;
            }
    
        }
        List<Message> subList = messages.subList(currIndex, nextIndex);
        currIndex = nextIndex;
        return subList;
    }
}
//then you could split the large list into small ones:
ListSplitter splitter = new ListSplitter(messages);
while (splitter.hasNext()) {
   try {
       List<Message>  listItem = splitter.next();
       producer.send(listItem);
   } catch (Exception e) {
       e.printStackTrace();
       //handle the error
   }
}
```



### 4.5 过滤消息

​	在大多数情况下,TAG是一个简单而有用的，其可以选择您想要的消息。例如：

```java
DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("CID_EXAMPLE");
consumer.subscribe("TOPIC", "TAGA || TAGB || TAGC");
```

​	消费者将接收TAGA或TAGB或TAGC的消息。但是限制是一个消息只能有一个标签，这对复杂的场景可能不起作用。在这种情况下，可以使用SQL表达式筛选我消息。SQL特性可以通过发送消息时的属性来进行计算。在RocketMQ定义的语法下，实现一些简单的逻辑。下面是一个例子：

​	

```
----------
|message   |
|----------|  a > 5 AND b = 'abc'  
|a=10      |   --->Gotten
|b='abc'   |
|c=true    |
------------

----------
|message   |  a > 5 AND b = 'abc'  
|----------|  ---> Missing
|a=1      |
|b='abc'   |
|c=true    |
------------
```

#### 4.5.1 SQL基本语法

RocketMQ只定义了一些基本语法来支持这个特性，你也可以很容易地扩展它。

* 数值比较, >, >= , <, <=, BETWEEN, =;
* 字符比较,比如：=，<>, IN;
* IS NULL或者 IS NOT NULL
* 逻辑符号 AND OR NOT；

支持的类型有：

* 数值，整数和小数
* 字符，必须用单引号括起来
* NULL, 特殊的常量
* 布尔值，TRUE或者FALSE

只有使用push模式的消费者才能够使用SQL92标准的sql语句

```java
public void subcribe(final String topic, final MessageSelector messageSelector)
```


#### 4.5.2 消息生产者

发送消息时，可以通过putUserProperty来设置消息的属性。

```java
msg.setUserProperty("i", String.valueOf(i));
```



#### 4.5.3 消息消费者

```java
consumer.subcribe("FilterSqlTopic", MessageSelector.bySql("i>5"));
```

### 4.6 事务消息

#### 4.6.1  流程分析

![](https://pic.imgdb.cn/item/5eff3c6414195aa59424388d.jpg)



上图大致说明了事务消息的大致方案，分为两个流程：正常事务消息的发送及提交、事务消息的补偿流程

##### 1）事务消息发送及提交

1. 发送消息（half消息）。
2. 服务端响应消息写入结果。
3. 根据发送结果执行本地事务（如果写入失败，此时half消息对业务不可见，本地逻辑不执行）。
4. 根据本地事务状态Commit或者RollBack(Commit操作生成消息索引，消息对消费者可见)。

##### 2）消息补偿

1. 对没有Commit和Rollback的事务消息(pending状态消息)，从服务端发起一次回查
2. Producer收到回查消息，检查回查消息的本地事务的状态
3. 根据本地事务状态，重新Commit或者RollBack

其中，补偿阶段用户解决消息Commit或者RollBack发生超时或者失败的情况

##### 3）事务消息状态

事务消息共有三种状态：提交状态、回滚状态、中间状态：

* TransactionStatus.CommitTransaction:提交事务，它允许消费者消费此消息
* TransactionStatus.RollBackTransaction: 回滚事务，它代表该消息将被删除，不允许被消费
* TransactionStatus.UnKnown：中间状态，它代表需要检查消息队列来确定状态

#### 4.6.2 发送事务消息

##### 1) 创建事务性生产者

​	使用TransactionMQProducer类创建生产者，并指定唯一的ProducerGroup，就可以设置自定义线程池来处理这些检查要求。执行本地事务后，需要根据执行结果对消息队列进行回复。回传的事务状态在请参考前一节。

​	//好像事务消息发送者关闭太快回查无法触发

```java
package cn.sorie.mq.rocketmq.transaction;

import org.apache.commons.lang3.StringUtils;
import org.apache.rocketmq.client.producer.*;
import org.apache.rocketmq.common.message.Message;
import org.apache.rocketmq.common.message.MessageExt;
import org.apache.rocketmq.remoting.common.RemotingHelper;

import java.util.concurrent.TimeUnit;

public class Producer {
    public static void main(String[] args) throws Exception {
        //实例化生产者producer
        TransactionMQProducer producer = new TransactionMQProducer("group5");
        // 设置NameServer的地址
        producer.setNamesrvAddr("192.168.1.6:9876;192.168.1.7:9876");

        //设置消息事务的消息监听器
        producer.setTransactionListener(new TransactionListener() {

            //该方法中去执行本地事务
            public LocalTransactionState executeLocalTransaction(Message message, Object o) {
                if (StringUtils.equals("TAGA", message.getTags())) {
                    return LocalTransactionState.COMMIT_MESSAGE;
                } else if (StringUtils.equals("TAGB", message.getTags())) {
                    return LocalTransactionState.ROLLBACK_MESSAGE;
                } else {
                    return LocalTransactionState.UNKNOW;
                }
            }
            //MQ进行消息事务状态的回查
            public LocalTransactionState checkLocalTransaction(MessageExt messageExt) {
                System.out.println(messageExt.getTags());
                return LocalTransactionState.COMMIT_MESSAGE;
            }
        });
        //启动producer
        String[] tags = {"TAGA", "TAGB", "TAGC"};
        producer.start();
        try {
            for (int i = 0; i < 3; i++) {
                //创建消息 指定topic,Tag和消息体
                Message msg = new Message("TransactionTopic",
                        tags[i], ("Hello MQ 事务消息" + i).getBytes(RemotingHelper.DEFAULT_CHARSET));
                //发送消息到一个broker
                SendResult sendResult = producer.sendMessageInTransaction(msg, null);
                SendStatus status = sendResult.getSendStatus();
                String msgId = sendResult.getMsgId();
                int queueId = sendResult.getMessageQueue().getQueueId();

                //发送消息是否成功送达
                System.out.printf("发送状态：%s 消息id：%s 队列id:%d, %s\n", status, msgId, queueId, tags[i]);
                TimeUnit.SECONDS.sleep(2);
            }
        } catch (Exception e) {
            e.printStackTrace();
        } finally {
//            int n = 100000;
//            while (true) {
//                
//            }
            producer.shutdown();
        }
    }
}
```

#### 4.6.3 使用限制

1. 事务消息不支持延时消息和批量消息
2. 为了避免单个消息检查太多次而导致半队列累计，我们默认将单个消息的检查次数限制为15次，但是用户可以通过Broker配置文件的trancationCheckMax参数修改此限制。如果已经检查某条消息超过N次的话，则丢弃此消息，并在默认情况下同时打印错误日志。用户可以通过重写AbstractTransactionCheckListener类来修改这个行为。
3. 事务消息将在Broker配置文件中的参数transactionMsgTimeout这样的特定时间长度之后被检查。当发送事务消息时，用户还可以通过设置用户属性CHECK_IMMUNITY_TIME_IN_SECONDS来改变这个限制，该参数优先于transactionMsgTimeout参数。
4. 事务消息可能不止一次被检查或消费。
5. 提交给用户的目标主题消息可能会失败，目前这一日志的记录而定。它的高可用性RocketMQ本身的高可用机制来保证，如果希望确保事务消息不丢失、并且事务完整性得到保证，建议使用同步双重写入机制。
6. 事务消息的生产者ID不能与其他类型消息的生产者ID共享。与其他类型的消息不同，事务消息允许反向查询、MQ服务器通过它们的生产者ID查询到消费者。

## 案例介绍

### 5.1 业务分析

​	模拟电商网站购物场景下的下单和支付业务。

#### 1）下单

![](https://pic.imgdb.cn/item/5f00846714195aa594a01a01.jpg)



1. 用户请求订单系统下单
2. 订单系统通过RPC调用订单服务下单
3. 订单服务调用优惠券服务，扣减优惠券
4. 订单服务调用库存服务，校验并扣减库存
5. 订单服务调用用户服务，扣减用户余额
6. 订单完成确认订单。

#### 2）支付

![](https://pic.imgdb.cn/item/5f00853414195aa594a077df.jpg)



1. 用户请求支付系统
2. 支付系统调用第三方支付平台API发起支付流程
3. 用户通过第三方支付平台支付成功后，第三方支付平台回调支付系统
4. 支付系统调用订单服务修改订单状态
5. 支付系统调用积分服务添加积分
6. 支付系统调用日志服务记录日志

### 5.2 问题分析

#### 问题1

​	用户提交订单后，扣减库存成功、扣减优惠券成功、使用余额成功，但是在确认订单操作失败，需要对库存、优惠券、余额进行回退。如何保证数据的完整性。

![](https://pic.imgdb.cn/item/5f00872014195aa594a15f0f.jpg)

#### 问题2

​	用户通过第三方支付平台支付成功后，第三方支付平台要回调API异步通知商家支付系统用户支付结果，支付系统根据支付结果修改订单状态、记录支付日志和给用户增加积分。

​	支付系统如何保证在收到第三方支付平台的异步通知时，如果快速给第三方支付凭条做出回应。

​	![](https://pic.imgdb.cn/item/5f00883114195aa594a1e2af.jpg)

通过MQ进行数据分发，提高系统处理性能。

![](https://pic.imgdb.cn/item/5f00885414195aa594a1f3f9.jpg)

## 技术分析

### 6.1 技术选型

* Spring Boot
* Dubbo
* Zookeeper
* RocketMQ
* MySql

![](https://pic.imgdb.cn/item/5f00893914195aa594a27427.jpg)



### 6.2 SpringBoot整合RocketMQ

​	下载rocketmq-spring 项目

​	https://github.com/apache/rocketmq-spring

​	将rocketmq-spring安装到本地仓库

```bash
mvn install -Dmaven.skip.test = true
```

### 6.2.1 消息生产者

#### 1) 添加依赖

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-parent</artifactId>
        <version>2.3.1.RELEASE</version>
        <relativePath/> <!-- lookup parent from repository -->
    </parent>
    <groupId>cn.sorie</groupId>
    <artifactId>rocketmq-producer</artifactId>
    <version>0.0.1-SNAPSHOT</version>
    <name>rocketmq-producer</name>
    <description>rocketmq</description>

    <properties>
        <java.version>1.8</java.version>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter</artifactId>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.rocketmq/rocketmq-spring-boot-starter -->
        <dependency>
            <groupId>org.apache.rocketmq</groupId>
            <artifactId>rocketmq-spring-boot-starter</artifactId>
            <version>2.1.0</version>
        </dependency>

        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <optional>true</optional>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
            <exclusions>
                <exclusion>
                    <groupId>org.junit.vintage</groupId>
                    <artifactId>junit-vintage-engine</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-maven-plugin</artifactId>
            </plugin>
        </plugins>
    </build>

</project>

```

#### 2) 发送消息测试者

```java
package cn.sorie.rocketmqproducer;

import lombok.extern.slf4j.Slf4j;
import org.apache.rocketmq.spring.core.RocketMQTemplate;
import org.junit.jupiter.api.Test;
import org.junit.runner.RunWith;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.test.context.junit4.SpringRunner;

@RunWith(SpringRunner.class)
@SpringBootTest(classes = {RocketmqProducerApplication.class})
@Slf4j
class RocketmqProducerApplicationTests {

    @Autowired
    private RocketMQTemplate rocketMQTemplate;

    @Test
    public void testSendMessage() {
        rocketMQTemplate.convertAndSend("springboot-rocketmq", "Hello Springboot RocketMq");
        log.info("发送消息");
    }
}
```

#### 3）application.properties

```properties
rocketmq.name-server=192.168.1.6:9876;192.168.1.7:9876;
rocketmq.producer.group=my-grp
```

### 6.2.2 消息消费者

#### 1) 添加依赖

同上

#### 2) 消费者

```java
package cn.sorie.rocketmqconsumer.listener;

import lombok.extern.slf4j.Slf4j;
import org.apache.rocketmq.spring.annotation.RocketMQMessageListener;
import org.apache.rocketmq.spring.core.RocketMQListener;
import org.springframework.stereotype.Component;

@RocketMQMessageListener(topic = "springboot-rocketmq",
        consumerGroup = "${rocketmq.producer.group}")
@Slf4j
@Component
public class Consumer implements RocketMQListener<String> {
    @Override
    public void onMessage(String s) {
        System.out.println("接收消息" + s);
    }
}
```

#### 3）application.properties

```properties
rocketmq.name-server=192.168.1.6:9876;192.168.1.7:9876;
rocketmq.producer.group=my-grp
```

### 6.3 Spring Boot整合Dubbo

rpc远程调用

下载dubbo-spring-boot-starter依赖包

安装到本地仓库

```shell
mvn install -Dmaven.skip.test = true
```

![](https://pic.imgdb.cn/item/5f01d3af14195aa59457b14c.jpg)

### 6.3.1 搭建Zookeeper集群

#### 1）准备工作

1. 安装JDK

2. 将Zookeeper上传服务器

3. 解压Zookeeper，并创建data目录，将conf下的zoo_sample.cfg文件名改为zoo.cfg

4. 建立/user/local/zookeeper-cluster, 将解压后的Zookeeper复制到以下三个目录

```
/user/local/zookeeper-cluster/zookeeper-1
/user/local/zookeeper-cluster/zookeeper-2
/user/local/zookeeper-cluster/zookeeper-3
```

5. 配置每一个Zookeeper的dataDir(zoo.cfg)clientPort 分别为 2181, 2182, 2183

修改/user/local/zookeeper-cluster/zookeeper-1/conf/zoo.cfg

```
clientPort=2181
dataDir=/zookeeper-cluster/zookeeper-1/data
```

​	其他也做如上改动

#### 2) 配置集群

1. 在每个zookeeper的data目录下创建一个myid文件，内容分别为1、2、3。这个文件就是记录每个服务器的id
2. 在每个zookeeper的zoo.cfg配置客户端访问端口(clientPort)和集群服务器IP列表

```
server.1=192.168.1.6:2881:3881
server.1=192.168.1.6:2882:3882
server.1=192.168.1.6:2883:3883
```

解释：server.服务器 ID = 服务器IP地址:服务器之间通信端口:服务器之间投票选举端口

#### 3) 启动集群

启动集群就是分别启动每个实例

```shell
zkServer.sh start
```

### 6.3.2 RPC服务接口

项目结构

<img src="https://pic.imgdb.cn/item/5f03265414195aa5940d489b.jpg" style="zoom:200%;" />

在interface下添加接口

```java
package cn.sorie.springbootdubboprovider.service;

public interface UserService {
    public String sayHello();
}
```

### 6.2.3 服务提供者

#### 1）依赖

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-parent</artifactId>
        <version>2.3.1.RELEASE</version>
        <relativePath/> <!-- lookup parent from repository -->
    </parent>
    <groupId>cn.sorie</groupId>
    <artifactId>springboot-dubbo-provider</artifactId>
    <version>0.0.1-SNAPSHOT</version>
    <name>springboot-dubbo-provider</name>
    <description>Demo project for Spring Boot</description>

    <properties>
        <java.version>1.8</java.version>
        <dubbo.version>2.7.7</dubbo.version>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter</artifactId>
            <exclusions>
                <exclusion>
                    <groupId>org.apache.logging.log4j</groupId>
                    <artifactId>log4j-to-slf4j</artifactId>
                </exclusion>
            </exclusions>
        </dependency>

        <dependency>
            <groupId>cn.sorie</groupId>
            <artifactId>springboot-dubbo-interface</artifactId>
            <version>0.0.1-SNAPSHOT</version>
        </dependency>

        <dependency>
            <groupId>org.apache.dubbo</groupId>
            <artifactId>dubbo-spring-boot-starter</artifactId>
            <version>${dubbo.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.dubbo</groupId>
            <artifactId>dubbo</artifactId>
            <version>${dubbo.version}</version>
            <type>pom</type>
            <scope>import</scope>
            <exclusions>
                <exclusion>
                    <groupId>org.springframework</groupId>
                    <artifactId>spring</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>javax.servlet</groupId>
                    <artifactId>servlet-api</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>log4j</groupId>
                    <artifactId>log4j</artifactId>
                </exclusion>
            </exclusions>
        </dependency>




        <!-- https://mvnrepository.com/artifact/com.101tec/zkclient -->
        <dependency>
            <groupId>com.101tec</groupId>
            <artifactId>zkclient</artifactId>
            <version>0.11</version>
            <exclusions>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                </exclusion>
            </exclusions>
        </dependency>

        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <optional>true</optional>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
            <exclusions>
                <exclusion>
                    <groupId>org.junit.vintage</groupId>
                    <artifactId>junit-vintage-engine</artifactId>
                </exclusion>
            </exclusions>
        </dependency>

        <dependency>
            <groupId>org.apache.zookeeper</groupId>
            <artifactId>zookeeper</artifactId>
            <version>3.4.9</version>
            <exclusions>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>log4j</groupId>
                    <artifactId>log4j</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.curator</groupId>
            <artifactId>curator-framework</artifactId>
            <version>4.3.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.curator</groupId>
            <artifactId>curator-recipes</artifactId>
            <version>4.3.0</version>
        </dependency>

    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-maven-plugin</artifactId>
            </plugin>
        </plugins>
    </build>

</project>

```

#### 2）application.properties

```properties
spring.application.name=dubbo-demo-provider
dubbo.application.id=dubbo-demo-provider
dubbo.application.name=dubbo-demo-provider
dubbo.registry.address=zookeeper://192.168.1.6:2182;zookeeper://192.168.1.6:2181;zookeeper://192.168.1.6:2183;
dubbo.registry.client=curator
dubbo.protocol.name=dubbo
dubbo.protocol.port=20880
dubbo.config-center.timeout=10000
dubbo.scan.base-packages=cn.sorie.springbootdubboprovider.service

```

#### 3)启动类

```java
@EnableDubboConfig
@SpringBootApplication
public class SpringbootDubboProviderApplication {

    public static void main(String[] args) {
        SpringApplication.run(SpringbootDubboProviderApplication.class, args);
    }

}
```

#### 4)服务实现

```java
package cn.sorie.springbootdubboprovider.service;


import org.springframework.stereotype.Service;

@Service
public class UserServiceImpl implements UserService{
    @Override
    public String sayHello() {
        return "hello";
    }
}
```

### 6.2.4 dubbo-admin搭建

下载tomcat和dubbo-admin.war

将war包放入tomcat的webapps目录下，启动tomcat即可

然后访问即可

http://192.168.1.6:8080/dubbo-admin

服务名和密码都是root

中间遇到8080端口被占用了，改为了8081端口

### 6.2.4 服务消费者

#### 1） 添加依赖

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-parent</artifactId>
        <version>2.3.1.RELEASE</version>
        <relativePath/> <!-- lookup parent from repository -->
    </parent>
    <groupId>cn.sorie</groupId>
    <artifactId>springboot-dubbo-consumer</artifactId>
    <version>0.0.1-SNAPSHOT</version>
    <name>springboot-dubbo-consumer</name>
    <description>Demo project for Spring Boot</description>

    <properties>
        <java.version>1.8</java.version>
        <dubbo.version>2.7.7</dubbo.version>

    </properties>

    <dependencies>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>
        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <optional>true</optional>
        </dependency>
        <dependency>
            <groupId>org.apache.dubbo</groupId>
            <artifactId>dubbo-spring-boot-starter</artifactId>
            <version>${dubbo.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.dubbo</groupId>
            <artifactId>dubbo</artifactId>
            <version>${dubbo.version}</version>
            <type>pom</type>
            <scope>import</scope>
            <exclusions>
                <exclusion>
                    <groupId>org.springframework</groupId>
                    <artifactId>spring</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>javax.servlet</groupId>
                    <artifactId>servlet-api</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>log4j</groupId>
                    <artifactId>log4j</artifactId>
                </exclusion>
            </exclusions>
        </dependency>

        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
            <exclusions>
                <exclusion>
                    <groupId>org.junit.vintage</groupId>
                    <artifactId>junit-vintage-engine</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.zookeeper</groupId>
            <artifactId>zookeeper</artifactId>
            <version>3.4.9</version>
            <exclusions>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>log4j</groupId>
                    <artifactId>log4j</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.curator</groupId>
            <artifactId>curator-framework</artifactId>
            <version>4.3.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.curator</groupId>
            <artifactId>curator-recipes</artifactId>
            <version>4.3.0</version>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-maven-plugin</artifactId>
            </plugin>
        </plugins>
    </build>

</project>
```

#### 2）配置文件

```yml
server:
  port: 8082
spring:
  application:
    name: dubbo-demo-consumer
dubbo:
  application:
    name: dubbo-demo-consumer
  registry:
    address: zookeeper://192.168.1.6:2182,192.168.1.6:2181,192.168.1.6:2183
```

#### 3）启动类

```java
package cn.sorie.springbootdubboconsumer;


import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;


//@EnableAutoConfiguration
@SpringBootApplication
public class SpringbootDubboConsumerApplication {

    public static void main(String[] args) {
        SpringApplication.run(SpringbootDubboConsumerApplication.class, args);
    }

}
```

#### 4）Controller

```java
package cn.sorie.springbootdubboconsumer.controller;

import cn.sorie.springbootdubboprovider.service.UserService;
import org.apache.dubbo.config.annotation.DubboReference;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;

@RestController
@RequestMapping("/user")
public class UserController {
    @DubboReference
    private UserService userService;

    @RequestMapping("/sayHello")
    public String sayHello() {
        return userService.sayHello();
    }
}
@RestController
@RequestMapping("/user")
public class UserController {
    @DubboReference
    private UserService userService;

    @RequestMapping("/sayHello")
    public String sayHello() {
        return userService.sayHello();
    }
}
```

## 环境搭建

### 7.1 数据库

#### 1）优惠券表

| Field        | Type                | Comment                  |
| ------------ | ------------------- | ------------------------ |
| coupon_id    | bigint(50) NOT NULL | 优惠券ID                 |
| coupon_price | decimal(10,2) NULL  | 优惠券金额               |
| user_id      | bigint(50) NULL     | 用户ID                   |
| order_id     | bigint(32) NULL     | 订单ID                   |
| is_used      | int(1) NULL         | 是否使用 0未使用 1已使用 |
| used_time    | timestamp NULL      | 使用时间                 |

#### 2）商品表

| Field        | Type                | Comment  |
| ------------ | ------------------- | -------- |
| goods_id     | bigint(50) NOT NULL | 主键     |
| goods_name   | varchar(255) NULL   | 商品名称 |
| goods_number | int(11) NULL        | 商品库存 |
| goods_price  | decimal(10,2) NULL  | 商品价格 |
| goods_desc   | varchar(255) NULL   | 商品描述 |
| add_time     | timestamp NULL      | 添加时间 |

#### 3）订单表

| Field           | Type                | Comment                                      |
| --------------- | ------------------- | -------------------------------------------- |
| order_id        | bigint(50) NOT NULL | 订单ID                                       |
| user_id         | bigint(50) NULL     | 用户ID                                       |
| order_status    | int(1) NULL         | 订单状态 0未确认 1已确认 2已取消 3无效 4退款 |
| pay_status      | int(1) NULL         | 支付状态 0未支付 1支付中 2已支付             |
| shipping_status | int(1) NULL         | 发货状态 0未发货 1已发货 2已退货             |
| address         | varchar(255) NULL   | 收货地址                                     |
| consignee       | varchar(255) NULL   | 收货人                                       |
| goods_id        | bigint(50) NULL     | 商品ID                                       |
| goods_number    | int(11) NULL        | 商品数量                                     |
| goods_price     | decimal(10,2) NULL  | 商品价格                                     |
| goods_amount    | decimal(10,0) NULL  | 商品总价                                     |
| shipping_fee    | decimal(10,2) NULL  | 运费                                         |
| order_amount    | decimal(10,2) NULL  | 订单价格                                     |
| coupon_id       | bigint(50) NULL     | 优惠券ID                                     |
| coupon_paid     | decimal(10,2) NULL  | 优惠券                                       |
| money_paid      | decimal(10,2) NULL  | 已付金额                                     |
| pay_amount      | decimal(10,2) NULL  | 支付金额                                     |
| add_time        | timestamp NULL      | 创建时间                                     |
| confirm_time    | timestamp NULL      | 订单确认时间                                 |
| pay_time        | timestamp NULL      | 支付时间                                     |

#### 4）订单商品日志表

| Field        | Type                 | Comment  |
| ------------ | -------------------- | -------- |
| goods_id     | int(11) NOT NULL     | 商品ID   |
| order_id     | varchar(32) NOT NULL | 订单ID   |
| goods_number | int(11) NULL         | 库存数量 |
| log_time     | datetime NULL        | 记录时间 |

#### 5）用户表

| Field         | Type                | Comment  |
| ------------- | ------------------- | -------- |
| user_id       | bigint(50) NOT NULL | 用户ID   |
| user_name     | varchar(255) NULL   | 用户姓名 |
| user_password | varchar(255) NULL   | 用户密码 |
| user_mobile   | varchar(255) NULL   | 手机号   |
| user_score    | int(11) NULL        | 积分     |
| user_reg_time | timestamp NULL      | 注册时间 |
| user_money    | decimal(10,0) NULL  | 用户余额 |

#### 6）用户余额日志表

| Field          | Type                | Comment                       |
| -------------- | ------------------- | ----------------------------- |
| user_id        | bigint(50) NOT NULL | 用户ID                        |
| order_id       | bigint(50) NOT NULL | 订单ID                        |
| money_log_type | int(1) NOT NULL     | 日志类型 1订单付款 2 订单退款 |
| use_money      | decimal(10,2) NULL  | 操作金额                      |
| create_time    | timestamp NULL      | 日志时间                      |

#### 7）订单支付表

| Field      | Type                | Comment            |
| ---------- | ------------------- | ------------------ |
| pay_id     | bigint(50) NOT NULL | 支付编号           |
| order_id   | bigint(50) NULL     | 订单编号           |
| pay_amount | decimal(10,2) NULL  | 支付金额           |
| is_paid    | int(1) NULL         | 是否已支付 1否 2是 |

#### 8）MQ消息生产表

| Field       | Type                  | Comment             |
| ----------- | --------------------- | ------------------- |
| id          | varchar(100) NOT NULL | 主键                |
| group_name  | varchar(100) NULL     | 生产者组名          |
| msg_topic   | varchar(100) NULL     | 消息主题            |
| msg_tag     | varchar(100) NULL     | Tag                 |
| msg_key     | varchar(100) NULL     | Key                 |
| msg_body    | varchar(500) NULL     | 消息内容            |
| msg_status  | int(1) NULL           | 0:未处理;1:已经处理 |
| create_time | timestamp NOT NULL    | 记录时间            |

#### 9）MQ消息消费表

| Field              | Type                  | Comment                          |
| ------------------ | --------------------- | -------------------------------- |
| msg_id             | varchar(50) NULL      | 消息ID                           |
| group_name         | varchar(100) NOT NULL | 消费者组名                       |
| msg_tag            | varchar(100) NOT NULL | Tag                              |
| msg_key            | varchar(100) NOT NULL | Key                              |
| msg_body           | varchar(500) NULL     | 消息体                           |
| consumer_status    | int(1) NULL           | 0:正在处理;1:处理成功;2:处理失败 |
| consumer_times     | int(1) NULL           | 消费次数                         |
| consumer_timestamp | timestamp NULL        | 消费时间                         |
| remark             | varchar(500) NULL     | 备注                             |

sql

```sql
/*
SQLyog Ultimate v8.32 
MySQL - 5.5.49 : Database - trade
*********************************************************************
*/

/*!40101 SET NAMES utf8 */;

/*!40101 SET SQL_MODE=''*/;

/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;
CREATE DATABASE /*!32312 IF NOT EXISTS*/`trade` /*!40100 DEFAULT CHARACTER SET utf8 */;

USE `trade`;

/*Table structure for table `trade_coupon` */

DROP TABLE IF EXISTS `trade_coupon`;

CREATE TABLE `trade_coupon` (
  `coupon_id` bigint(50) NOT NULL COMMENT '优惠券ID',
  `coupon_price` decimal(10,2) DEFAULT NULL COMMENT '优惠券金额',
  `user_id` bigint(50) DEFAULT NULL COMMENT '用户ID',
  `order_id` bigint(32) DEFAULT NULL COMMENT '订单ID',
  `is_used` int(1) DEFAULT NULL COMMENT '是否使用 0未使用 1已使用',
  `used_time` timestamp NULL DEFAULT NULL COMMENT '使用时间',
  PRIMARY KEY (`coupon_id`),
  KEY `FK_trade_coupon` (`user_id`),
  KEY `FK_trade_coupon2` (`order_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

/*Data for the table `trade_coupon` */

insert  into `trade_coupon`(`coupon_id`,`coupon_price`,`user_id`,`order_id`,`is_used`,`used_time`) values (345988230098857984,'20.00',345963634385633280,352537369385242624,1,'2019-07-27 16:58:44');

/*Table structure for table `trade_goods` */

DROP TABLE IF EXISTS `trade_goods`;

CREATE TABLE `trade_goods` (
  `goods_id` bigint(50) NOT NULL AUTO_INCREMENT,
  `goods_name` varchar(255) DEFAULT NULL COMMENT '商品名称',
  `goods_number` int(11) DEFAULT NULL COMMENT '商品库存',
  `goods_price` decimal(10,2) DEFAULT NULL COMMENT '商品价格',
  `goods_desc` varchar(255) DEFAULT NULL COMMENT '商品描述',
  `add_time` timestamp NULL DEFAULT NULL COMMENT '添加时间',
  PRIMARY KEY (`goods_id`)
) ENGINE=InnoDB AUTO_INCREMENT=345959443973935105 DEFAULT CHARSET=utf8;

/*Data for the table `trade_goods` */

insert  into `trade_goods`(`goods_id`,`goods_name`,`goods_number`,`goods_price`,`goods_desc`,`add_time`) values (345959443973935104,'Javase课程',999,'1000.00','传智播客出品Java视频课程','2019-07-09 20:38:00');

/*Table structure for table `trade_goods_number_log` */

DROP TABLE IF EXISTS `trade_goods_number_log`;

CREATE TABLE `trade_goods_number_log` (
  `goods_id` bigint(50) NOT NULL COMMENT '商品ID',
  `order_id` bigint(50) NOT NULL COMMENT '订单ID',
  `goods_number` int(11) DEFAULT NULL COMMENT '库存数量',
  `log_time` timestamp NULL DEFAULT NULL,
  PRIMARY KEY (`goods_id`,`order_id`),
  KEY `FK_trade_goods_number_log2` (`order_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

/*Data for the table `trade_goods_number_log` */

insert  into `trade_goods_number_log`(`goods_id`,`order_id`,`goods_number`,`log_time`) values (345959443973935104,352537369385242624,-1,'2019-07-27 16:58:44');

/*Table structure for table `trade_mq_consumer_log` */

DROP TABLE IF EXISTS `trade_mq_consumer_log`;

CREATE TABLE `trade_mq_consumer_log` (
  `msg_id` varchar(50) DEFAULT NULL,
  `group_name` varchar(100) NOT NULL,
  `msg_tag` varchar(100) NOT NULL,
  `msg_key` varchar(100) NOT NULL,
  `msg_body` varchar(500) DEFAULT NULL,
  `consumer_status` int(1) DEFAULT NULL COMMENT '0:正在处理;1:处理成功;2:处理失败',
  `consumer_times` int(1) DEFAULT NULL,
  `consumer_timestamp` timestamp NULL DEFAULT NULL,
  `remark` varchar(500) DEFAULT NULL,
  PRIMARY KEY (`group_name`,`msg_tag`,`msg_key`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

/*Data for the table `trade_mq_consumer_log` */

/*Table structure for table `trade_mq_producer_temp` */

DROP TABLE IF EXISTS `trade_mq_producer_temp`;

CREATE TABLE `trade_mq_producer_temp` (
  `id` varchar(100) NOT NULL,
  `group_name` varchar(100) DEFAULT NULL,
  `msg_topic` varchar(100) DEFAULT NULL,
  `msg_tag` varchar(100) DEFAULT NULL,
  `msg_key` varchar(100) DEFAULT NULL,
  `msg_body` varchar(500) DEFAULT NULL,
  `msg_status` int(1) DEFAULT NULL COMMENT '0:未处理;1:已经处理',
  `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

/*Data for the table `trade_mq_producer_temp` */

/*Table structure for table `trade_order` */

DROP TABLE IF EXISTS `trade_order`;

CREATE TABLE `trade_order` (
  `order_id` bigint(50) NOT NULL COMMENT '订单ID',
  `user_id` bigint(50) DEFAULT NULL COMMENT '用户ID',
  `order_status` int(1) DEFAULT NULL COMMENT '订单状态 0未确认 1已确认 2已取消 3无效 4退款',
  `pay_status` int(1) DEFAULT NULL COMMENT '支付状态 0未支付 1支付中 2已支付',
  `shipping_status` int(1) DEFAULT NULL COMMENT '发货状态 0未发货 1已发货 2已收货',
  `address` varchar(255) DEFAULT NULL COMMENT '收货地址',
  `consignee` varchar(255) DEFAULT NULL COMMENT '收货人',
  `goods_id` bigint(50) DEFAULT NULL COMMENT '商品ID',
  `goods_number` int(11) DEFAULT NULL COMMENT '商品数量',
  `goods_price` decimal(10,2) DEFAULT NULL COMMENT '商品价格',
  `goods_amount` decimal(10,0) DEFAULT NULL COMMENT '商品总价',
  `shipping_fee` decimal(10,2) DEFAULT NULL COMMENT '运费',
  `order_amount` decimal(10,2) DEFAULT NULL COMMENT '订单价格',
  `coupon_id` bigint(50) DEFAULT NULL COMMENT '优惠券ID',
  `coupon_paid` decimal(10,2) DEFAULT NULL COMMENT '优惠券',
  `money_paid` decimal(10,2) DEFAULT NULL COMMENT '已付金额',
  `pay_amount` decimal(10,2) DEFAULT NULL COMMENT '支付金额',
  `add_time` timestamp NULL DEFAULT NULL COMMENT '创建时间',
  `confirm_time` timestamp NULL DEFAULT NULL COMMENT '订单确认时间',
  `pay_time` timestamp NULL DEFAULT NULL COMMENT '支付时间',
  PRIMARY KEY (`order_id`),
  KEY `FK_trade_order` (`user_id`),
  KEY `FK_trade_order2` (`goods_id`),
  KEY `FK_trade_order3` (`coupon_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

/*Data for the table `trade_order` */

insert  into `trade_order`(`order_id`,`user_id`,`order_status`,`pay_status`,`shipping_status`,`address`,`consignee`,`goods_id`,`goods_number`,`goods_price`,`goods_amount`,`shipping_fee`,`order_amount`,`coupon_id`,`coupon_paid`,`money_paid`,`pay_amount`,`add_time`,`confirm_time`,`pay_time`) values (352537369385242624,345963634385633280,1,2,NULL,'北京',NULL,345959443973935104,1,'1000.00',NULL,'0.00','1000.00',345988230098857984,'20.00','100.00','880.00','2019-07-27 16:58:44','2019-07-27 16:58:44',NULL);

/*Table structure for table `trade_pay` */

DROP TABLE IF EXISTS `trade_pay`;

CREATE TABLE `trade_pay` (
  `pay_id` bigint(50) NOT NULL COMMENT '支付编号',
  `order_id` bigint(50) DEFAULT NULL COMMENT '订单编号',
  `pay_amount` decimal(10,2) DEFAULT NULL COMMENT '支付金额',
  `is_paid` int(1) DEFAULT NULL COMMENT '是否已支付 0:未付款 1正在付款 2已经付款',
  PRIMARY KEY (`pay_id`),
  KEY `FK_trade_pay` (`order_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

/*Data for the table `trade_pay` */

insert  into `trade_pay`(`pay_id`,`order_id`,`pay_amount`,`is_paid`) values (352542415984402432,352537369385242624,'880.00',2);

/*Table structure for table `trade_user` */

DROP TABLE IF EXISTS `trade_user`;

CREATE TABLE `trade_user` (
  `user_id` bigint(50) NOT NULL AUTO_INCREMENT COMMENT '用户ID',
  `user_name` varchar(255) DEFAULT NULL COMMENT '用户姓名',
  `user_password` varchar(255) DEFAULT NULL COMMENT '用户密码',
  `user_mobile` varchar(255) DEFAULT NULL COMMENT '手机号',
  `user_score` int(11) DEFAULT NULL COMMENT '积分',
  `user_reg_time` timestamp NULL DEFAULT NULL COMMENT '注册时间',
  `user_money` decimal(10,0) DEFAULT NULL COMMENT '用户余额',
  PRIMARY KEY (`user_id`)
) ENGINE=InnoDB AUTO_INCREMENT=345963634385633281 DEFAULT CHARSET=utf8;

/*Data for the table `trade_user` */

insert  into `trade_user`(`user_id`,`user_name`,`user_password`,`user_mobile`,`user_score`,`user_reg_time`,`user_money`) values (345963634385633280,'zs','123456','18888888888',100,'2020-10-10 00:00:00','900');

/*Table structure for table `trade_user_money_log` */

DROP TABLE IF EXISTS `trade_user_money_log`;

CREATE TABLE `trade_user_money_log` (
  `user_id` bigint(50) NOT NULL COMMENT '用户ID',
  `order_id` bigint(50) NOT NULL COMMENT '订单ID',
  `money_log_type` int(1) NOT NULL COMMENT '日志类型 1订单付款 2 订单退款',
  `use_money` decimal(10,2) DEFAULT NULL,
  `create_time` timestamp NULL DEFAULT NULL COMMENT '日志时间',
  PRIMARY KEY (`user_id`,`order_id`,`money_log_type`),
  KEY `FK_trade_user_money_log2` (`order_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

/*Data for the table `trade_user_money_log` */

insert  into `trade_user_money_log`(`user_id`,`order_id`,`money_log_type`,`use_money`,`create_time`) values (345963634385633280,352537369385242624,1,'100.00','2019-07-27 16:58:44');

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;
```



### 7.2 项目初始化

#### 7.2.1 工程概览

![](https://pic.imgdb.cn/item/5f0b237314195aa59431dc47.jpg)

#### 7.2.2 工程关系

![](https://pic.imgdb.cn/item/5f0b23ba14195aa59431f4a4.jpg)

![](https://pic.imgdb.cn/item/5f0b23d114195aa59431fcbb.jpg)

### 7.3 Mybatis逆向工程使用

 略

### 7.4 公共类介绍

* ID生成器

  IDWorker:Twitter雪花算法

* 异常处理类

  CustomerException：自定义异常类

  CastException：异常抛出类

* 常量类

  ShopCode：系统状态类

* 响应实体类

  Result：封装响应状态

## 下单业务

###  8.1 下单基本流程

![](https://pic.imgdb.cn/item/5f0c69cd14195aa5948c60ab.jpg)

#### 1）接口定义

* IOrderService

```java
public interface IOrderService {
    /**
     * 确认订单
     * @param order
     * @return Result
     */
    Result confirmOrder(TradeOrder order);
}
```

#### 2）业务类实现

```java
@Slf4j
@Component
@Service(interfaceClass = IOrderService.class)
public class OrderServiceImpl implements IOrderService {

    @Override
    public Result confirmOrder(TradeOrder order) {
        //1.校验订单
       
        //2.生成预订单
       
        try {
            //3.扣减库存
            
            //4.扣减优惠券
           
            //5.使用余额
           
            //6.确认订单
            
            //7.返回成功状态
           
        } catch (Exception e) {
            //1.确认订单失败,发送消息
            
            //2.返回失败状态
        }

    }
}
```

#### 3）校验订单

![](https://pic.imgdb.cn/item/5f145ba414195aa594e34985.jpg)

![](https://pic.imgdb.cn/item/5f145e6714195aa594e42def.jpg)

```java
private void checkOrder(TradeOrder order) {
        //1.校验订单是否存在
        if(order==null){
            CastException.cast(ShopCode.SHOP_ORDER_INVALID);
        }
        //2.校验订单中的商品是否存在
        TradeGoods goods = goodsService.findOne(order.getGoodsId());
        if(goods==null){
            CastException.cast(ShopCode.SHOP_GOODS_NO_EXIST);
        }
        //3.校验下单用户是否存在
        TradeUser user = userService.findOne(order.getUserId());
        if(user==null){
            CastException.cast(ShopCode.SHOP_USER_NO_EXIST);
        }
        //4.校验商品单价是否合法
        if(order.getGoodsPrice().compareTo(goods.getGoodsPrice())!=0){
            CastException.cast(ShopCode.SHOP_GOODS_PRICE_INVALID);
        }
        //5.校验订单商品数量是否合法
        if(order.getGoodsNumber()>=goods.getGoodsNumber()){
            CastException.cast(ShopCode.SHOP_GOODS_NUM_NOT_ENOUGH);
        }

        log.info("校验订单通过");
}
```

#### 4）生成预订单

![](https://pic.imgdb.cn/item/5f145e9514195aa594e43d50.jpg)

![](https://pic.imgdb.cn/item/5f145eb714195aa594e44914.jpg)

![](https://pic.imgdb.cn/item/5f145eda14195aa594e45312.jpg)



```java
private Long savePreOrder(TradeOrder order) {
        //1.设置订单状态为不可见
        order.setOrderStatus(ShopCode.SHOP_ORDER_NO_CONFIRM.getCode());
        //2.订单ID
        order.setOrderId(idWorker.nextId());
        //核算运费是否正确
        BigDecimal shippingFee = calculateShippingFee(order.getOrderAmount());
        if (order.getShippingFee().compareTo(shippingFee) != 0) {
            CastException.cast(ShopCode.SHOP_ORDER_SHIPPINGFEE_INVALID);
        }
        //3.计算订单总价格是否正确
        BigDecimal orderAmount = order.getGoodsPrice().multiply(new BigDecimal(order.getGoodsNumber()));
        orderAmount.add(shippingFee);
        if (orderAmount.compareTo(order.getOrderAmount()) != 0) {
            CastException.cast(ShopCode.SHOP_ORDERAMOUNT_INVALID);
        }

        //4.判断优惠券信息是否合法
        Long couponId = order.getCouponId();
        if (couponId != null) {
            TradeCoupon coupon = couponService.findOne(couponId);
            //优惠券不存在
            if (coupon == null) {
                CastException.cast(ShopCode.SHOP_COUPON_NO_EXIST);
            }
            //优惠券已经使用
            if ((ShopCode.SHOP_COUPON_ISUSED.getCode().toString())
                .equals(coupon.getIsUsed().toString())) {
                CastException.cast(ShopCode.SHOP_COUPON_INVALIED);
            }
            order.setCouponPaid(coupon.getCouponPrice());
        } else {
            order.setCouponPaid(BigDecimal.ZERO);
        }

        //5.判断余额是否正确
        BigDecimal moneyPaid = order.getMoneyPaid();
        if (moneyPaid != null) {
            //比较余额是否大于0
            int r = order.getMoneyPaid().compareTo(BigDecimal.ZERO);
            //余额小于0
            if (r == -1) {
                CastException.cast(ShopCode.SHOP_MONEY_PAID_LESS_ZERO);
            }
            //余额大于0
            if (r == 1) {
                //查询用户信息
                TradeUser user = userService.findOne(order.getUserId());
                if (user == null) {
                    CastException.cast(ShopCode.SHOP_USER_NO_EXIST);
                }
            //比较余额是否大于用户账户余额
            if (user.getUserMoney().compareTo(order.getMoneyPaid().longValue()) == -1) {
                CastException.cast(ShopCode.SHOP_MONEY_PAID_INVALID);
            }
            order.setMoneyPaid(order.getMoneyPaid());
        }
    } else {
        order.setMoneyPaid(BigDecimal.ZERO);
    }
    //计算订单支付总价
    order.setPayAmount(orderAmount.subtract(order.getCouponPaid())
                       .subtract(order.getMoneyPaid()));
    //设置订单添加时间
    order.setAddTime(new Date());

    //保存预订单
    int r = orderMapper.insert(order);
    if (ShopCode.SHOP_SUCCESS.getCode() != r) {
        CastException.cast(ShopCode.SHOP_ORDER_SAVE_ERROR);
    }
    log.info("订单:["+order.getOrderId()+"]预订单生成成功");
    return order.getOrderId();
}
```

#### 5）扣减库存

* 通过dubbo调用商品服务完成扣减库存

```java
private void reduceGoodsNum(TradeOrder order) {
        TradeGoodsNumberLog goodsNumberLog = new TradeGoodsNumberLog();
        goodsNumberLog.setGoodsId(order.getGoodsId());
        goodsNumberLog.setOrderId(order.getOrderId());
        goodsNumberLog.setGoodsNumber(order.getGoodsNumber());
        Result result = goodsService.reduceGoodsNum(goodsNumberLog);
        if (result.getSuccess().equals(ShopCode.SHOP_FAIL.getSuccess())) {
            CastException.cast(ShopCode.SHOP_REDUCE_GOODS_NUM_FAIL);
        }
        log.info("订单:["+order.getOrderId()+"]扣减库存["+order.getGoodsNumber()+"个]成功");
    }
```

* 商品服务GoodsService扣减库存

```java
@Override
public Result reduceGoodsNum(TradeGoodsNumberLog goodsNumberLog) {
    if (goodsNumberLog == null ||
            goodsNumberLog.getGoodsNumber() == null ||
            goodsNumberLog.getOrderId() == null ||
            goodsNumberLog.getGoodsNumber() == null ||
            goodsNumberLog.getGoodsNumber().intValue() <= 0) {
        CastException.cast(ShopCode.SHOP_REQUEST_PARAMETER_VALID);
    }
    TradeGoods goods = goodsMapper.selectByPrimaryKey(goodsNumberLog.getGoodsId());
    if(goods.getGoodsNumber()<goodsNumberLog.getGoodsNumber()){
        //库存不足
        CastException.cast(ShopCode.SHOP_GOODS_NUM_NOT_ENOUGH);
    }
    //减库存
    goods.setGoodsNumber(goods.getGoodsNumber()-goodsNumberLog.getGoodsNumber());
    goodsMapper.updateByPrimaryKey(goods);


    //记录库存操作日志
    goodsNumberLog.setGoodsNumber(-(goodsNumberLog.getGoodsNumber()));
    goodsNumberLog.setLogTime(new Date());
    goodsNumberLogMapper.insert(goodsNumberLog);

    return new Result(ShopCode.SHOP_SUCCESS.getSuccess(),ShopCode.SHOP_SUCCESS.getMessage());
}
```

#### 6）扣减优惠券

* 通过dubbo完成扣减优惠券

```java
private void changeCoponStatus(TradeOrder order) {
    //判断用户是否使用优惠券
    if (!StringUtils.isEmpty(order.getCouponId())) {
        //封装优惠券对象
        TradeCoupon coupon = couponService.findOne(order.getCouponId());
        coupon.setIsUsed(ShopCode.SHOP_COUPON_ISUSED.getCode());
        coupon.setUsedTime(new Date());
        coupon.setOrderId(order.getOrderId());
        Result result = couponService.changeCouponStatus(coupon);
        //判断执行结果
        if (result.getSuccess().equals(ShopCode.SHOP_FAIL.getSuccess())) {
            //优惠券使用失败
            CastException.cast(ShopCode.SHOP_COUPON_USE_FAIL);
        }
        log.info("订单:["+order.getOrderId()+"]使用扣减优惠券["+coupon.getCouponPrice()+"元]成功");
    }

}
```

* 优惠券服务CouponService更改优惠券状态

```java
@Override
public Result changeCouponStatus(TradeCoupon coupon) {
    try {
        //判断请求参数是否合法
        if (coupon == null || StringUtils.isEmpty(coupon.getCouponId())) {
            CastException.cast(ShopCode.SHOP_REQUEST_PARAMETER_VALID);
        }
		//更新优惠券状态为已使用
        couponMapper.updateByPrimaryKey(coupon);
        return new Result(ShopCode.SHOP_SUCCESS.getSuccess(), ShopCode.SHOP_SUCCESS.getMessage());
    } catch (Exception e) {
        return new Result(ShopCode.SHOP_FAIL.getSuccess(), ShopCode.SHOP_FAIL.getMessage());
    }
}
```

#### 7）扣减用户余额

* 通过用户服务完成扣减余额

```java
private void reduceMoneyPaid(TradeOrder order) {
    //判断订单中使用的余额是否合法
    if (order.getMoneyPaid() != null && order.getMoneyPaid().compareTo(BigDecimal.ZERO) == 1) {
        TradeUserMoneyLog userMoneyLog = new TradeUserMoneyLog();
        userMoneyLog.setOrderId(order.getOrderId());
        userMoneyLog.setUserId(order.getUserId());
        userMoneyLog.setUseMoney(order.getMoneyPaid());
        userMoneyLog.setMoneyLogType(ShopCode.SHOP_USER_MONEY_PAID.getCode());
        //扣减余额
        Result result = userService.changeUserMoney(userMoneyLog);
        if (result.getSuccess().equals(ShopCode.SHOP_FAIL.getSuccess())) {
            CastException.cast(ShopCode.SHOP_USER_MONEY_REDUCE_FAIL);
        }
        log.info("订单:["+order.getOrderId()+"扣减余额["+order.getMoneyPaid()+"元]成功]");
    }
}
```

* 用户服务UserService,更新余额

![](https://pic.imgdb.cn/item/5f145cd614195aa594e3a838.jpg)

![](https://pic.imgdb.cn/item/5f145cb614195aa594e39d7b.jpg)

```java
@Override
public Result changeUserMoney(TradeUserMoneyLog userMoneyLog) {
    //判断请求参数是否合法
    if (userMoneyLog == null
            || userMoneyLog.getUserId() == null
            || userMoneyLog.getUseMoney() == null
            || userMoneyLog.getOrderId() == null
            || userMoneyLog.getUseMoney().compareTo(BigDecimal.ZERO) <= 0) {
        CastException.cast(ShopCode.SHOP_REQUEST_PARAMETER_VALID);
    }

    //查询该订单是否存在付款记录
    TradeUserMoneyLogExample userMoneyLogExample = new TradeUserMoneyLogExample();
    userMoneyLogExample.createCriteria()
            .andUserIdEqualTo(userMoneyLog.getUserId())
            .andOrderIdEqualTo(userMoneyLog.getOrderId());
   int count = userMoneyLogMapper.countByExample(userMoneyLogExample);
   TradeUser tradeUser = new TradeUser();
   tradeUser.setUserId(userMoneyLog.getUserId());
   tradeUser.setUserMoney(userMoneyLog.getUseMoney().longValue());
   //判断余额操作行为
   //【付款操作】
   if (userMoneyLog.getMoneyLogType().equals(ShopCode.SHOP_USER_MONEY_PAID.getCode())) {
           //订单已经付款，则抛异常
           if (count > 0) {
                CastException.cast(ShopCode.SHOP_ORDER_PAY_STATUS_IS_PAY);
            }
       	   //用户账户扣减余额
           userMapper.reduceUserMoney(tradeUser);
       }
    //【退款操作】
    if (userMoneyLog.getMoneyLogType().equals(ShopCode.SHOP_USER_MONEY_REFUND.getCode())) {
         //如果订单未付款,则不能退款,抛异常
         if (count == 0) {
         CastException.cast(ShopCode.SHOP_ORDER_PAY_STATUS_NO_PAY);
     }
     //防止多次退款
     userMoneyLogExample = new TradeUserMoneyLogExample();
     userMoneyLogExample.createCriteria()
             .andUserIdEqualTo(userMoneyLog.getUserId())
                .andOrderIdEqualTo(userMoneyLog.getOrderId())
                .andMoneyLogTypeEqualTo(ShopCode.SHOP_USER_MONEY_REFUND.getCode());
     count = userMoneyLogMapper.countByExample(userMoneyLogExample);
     if (count > 0) {
         CastException.cast(ShopCode.SHOP_USER_MONEY_REFUND_ALREADY);
     }
     	//用户账户添加余额
        userMapper.addUserMoney(tradeUser);
    }


    //记录用户使用余额日志
    userMoneyLog.setCreateTime(new Date());
    userMoneyLogMapper.insert(userMoneyLog);
    return new Result(ShopCode.SHOP_SUCCESS.getSuccess(),ShopCode.SHOP_SUCCESS.getMessage());
}
```

#### 8）确认订单 

```java
private void updateOrderStatus(TradeOrder order) {
    order.setOrderStatus(ShopCode.SHOP_ORDER_CONFIRM.getCode());
    order.setPayStatus(ShopCode.SHOP_ORDER_PAY_STATUS_NO_PAY.getCode());
    order.setConfirmTime(new Date());
    int r = orderMapper.updateByPrimaryKey(order);
    if (r <= 0) {
        CastException.cast(ShopCode.SHOP_ORDER_CONFIRM_FAIL);
    }
    log.info("订单:["+order.getOrderId()+"]状态修改成功");
}
```

#### 9）小结

```java
@Override
public Result confirmOrder(TradeOrder order) {
    //1.校验订单
    checkOrder(order);
    //2.生成预订单
    Long orderId = savePreOrder(order);
    order.setOrderId(orderId);
    try {
        //3.扣减库存
        reduceGoodsNum(order);
        //4.扣减优惠券
        changeCoponStatus(order);
        //5.使用余额
        reduceMoneyPaid(order);
        //6.确认订单
        updateOrderStatus(order);
        log.info("订单:["+orderId+"]确认成功");
        return new Result(ShopCode.SHOP_SUCCESS.getSuccess(), ShopCode.SHOP_SUCCESS.getMessage());
    } catch (Exception e) {
        //确认订单失败,发送消息
        ...
        return new Result(ShopCode.SHOP_FAIL.getSuccess(), ShopCode.SHOP_FAIL.getMessage());
    }
}
```

### 8.2 失败补偿机制

#### 8.2.1 消息发送方

* 配置RocketMQ属性值

```properties
rocketmq.name-server=192.168.25.135:9876;192.168.25.138:9876
rocketmq.producer.group=orderProducerGroup

mq.order.consumer.group.name=order_orderTopic_cancel_group
mq.order.topic=orderTopic
mq.order.tag.confirm=order_confirm
mq.order.tag.cancel=order_cancel
```

* 注入模板类和属性值信息

```java
 @Autowired
 private RocketMQTemplate rocketMQTemplate;

 @Value("${mq.order.topic}")
 private String topic;

 @Value("${mq.order.tag.cancel}")
 private String cancelTag;
```

* 发送下单失败消息

```java
@Override
public Result confirmOrder(TradeOrder order) {
    //1.校验订单
    //2.生成预订
    try {
        //3.扣减库存
        //4.扣减优惠券
        //5.使用余额
        //6.确认订单
    } catch (Exception e) {
        //确认订单失败,发送消息
        CancelOrderMQ cancelOrderMQ = new CancelOrderMQ();
        cancelOrderMQ.setOrderId(order.getOrderId());
        cancelOrderMQ.setCouponId(order.getCouponId());
        cancelOrderMQ.setGoodsId(order.getGoodsId());
        cancelOrderMQ.setGoodsNumber(order.getGoodsNumber());
        cancelOrderMQ.setUserId(order.getUserId());
        cancelOrderMQ.setUserMoney(order.getMoneyPaid());
        try {
            sendMessage(topic, 
                        cancelTag, 
                        cancelOrderMQ.getOrderId().toString(), 
                    JSON.toJSONString(cancelOrderMQ));
    } catch (Exception e1) {
        e1.printStackTrace();
            CastException.cast(ShopCode.SHOP_MQ_SEND_MESSAGE_FAIL);
        }
        return new Result(ShopCode.SHOP_FAIL.getSuccess(), ShopCode.SHOP_FAIL.getMessage());
    }
}
```

```java
private void sendMessage(String topic, String tags, String keys, String body) throws Exception {
    //判断Topic是否为空
    if (StringUtils.isEmpty(topic)) {
        CastException.cast(ShopCode.SHOP_MQ_TOPIC_IS_EMPTY);
    }
    //判断消息内容是否为空
    if (StringUtils.isEmpty(body)) {
        CastException.cast(ShopCode.SHOP_MQ_MESSAGE_BODY_IS_EMPTY);
    }
    //消息体
    Message message = new Message(topic, tags, keys, body.getBytes());
    //发送消息
    rocketMQTemplate.getProducer().send(message);
}
```

### 8.2.2 消费接收方

* 配置RocketMQ属性值

```properties
rocketmq.name-server=192.168.25.135:9876;192.168.25.138:9876
mq.order.consumer.group.name=order_orderTopic_cancel_group
mq.order.topic=orderTopic
```

* 创建监听类，消费消息

```java
@Slf4j
@Component
@RocketMQMessageListener(topic = "${mq.order.topic}", 
                         consumerGroup = "${mq.order.consumer.group.name}",
                         messageModel = MessageModel.BROADCASTING)
public class CancelOrderConsumer implements RocketMQListener<MessageExt>{

    @Override
    public void onMessage(MessageExt messageExt) {
        ...
    }
}
```

#### 1）回退库存

* 流程分析

![](https://pic.imgdb.cn/item/5f145d0b14195aa594e3b848.jpg)

![](https://pic.imgdb.cn/item/5f145d5f14195aa594e3d3b8.jpg)

![](https://pic.imgdb.cn/item/5f145d4c14195aa594e3ce57.jpg)

* 消息消费者

```java
@Slf4j
@Component
@RocketMQMessageListener(topic = "${mq.order.topic}",consumerGroup = "${mq.order.consumer.group.name}",messageModel = MessageModel.BROADCASTING )
public class CancelMQListener implements RocketMQListener<MessageExt>{


    @Value("${mq.order.consumer.group.name}")
    private String groupName;

    @Autowired
    private TradeGoodsMapper goodsMapper;

    @Autowired
    private TradeMqConsumerLogMapper mqConsumerLogMapper;

    @Autowired
    private TradeGoodsNumberLogMapper goodsNumberLogMapper;

    @Override
    public void onMessage(MessageExt messageExt) {
        String msgId=null;
        String tags=null;
        String keys=null;
        String body=null;
        try {
            //1. 解析消息内容
            msgId = messageExt.getMsgId();
            tags= messageExt.getTags();
            keys= messageExt.getKeys();
            body= new String(messageExt.getBody(),"UTF-8");

            log.info("接受消息成功");

            //2. 查询消息消费记录
            TradeMqConsumerLogKey primaryKey = new TradeMqConsumerLogKey();
            primaryKey.setMsgTag(tags);
            primaryKey.setMsgKey(keys);
            primaryKey.setGroupName(groupName);
            TradeMqConsumerLog mqConsumerLog = mqConsumerLogMapper.selectByPrimaryKey(primaryKey);

            if(mqConsumerLog!=null){
                //3. 判断如果消费过...
                //3.1 获得消息处理状态
                Integer status = mqConsumerLog.getConsumerStatus();
                //处理过...返回
                if(ShopCode.SHOP_MQ_MESSAGE_STATUS_SUCCESS.getCode().intValue()==status.intValue()){
                    log.info("消息:"+msgId+",已经处理过");
                    return;
                }

                //正在处理...返回
                if(ShopCode.SHOP_MQ_MESSAGE_STATUS_PROCESSING.getCode().intValue()==status.intValue()){
                    log.info("消息:"+msgId+",正在处理");
                    return;
                }

                //处理失败
                if(ShopCode.SHOP_MQ_MESSAGE_STATUS_FAIL.getCode().intValue()==status.intValue()){
                    //获得消息处理次数
                    Integer times = mqConsumerLog.getConsumerTimes();
                    if(times>3){
                        log.info("消息:"+msgId+",消息处理超过3次,不能再进行处理了");
                        return;
                    }
                    mqConsumerLog.setConsumerStatus(ShopCode.SHOP_MQ_MESSAGE_STATUS_PROCESSING.getCode());

                    //使用数据库乐观锁更新
                    TradeMqConsumerLogExample example = new TradeMqConsumerLogExample();
                    TradeMqConsumerLogExample.Criteria criteria = example.createCriteria();
                    criteria.andMsgTagEqualTo(mqConsumerLog.getMsgTag());
                    criteria.andMsgKeyEqualTo(mqConsumerLog.getMsgKey());
                    criteria.andGroupNameEqualTo(groupName);
                    criteria.andConsumerTimesEqualTo(mqConsumerLog.getConsumerTimes());
                    int r = mqConsumerLogMapper.updateByExampleSelective(mqConsumerLog, example);
                    if(r<=0){
                        //未修改成功,其他线程并发修改
                        log.info("并发修改,稍后处理");
                    }
                }

            }else{
                //4. 判断如果没有消费过...
                mqConsumerLog = new TradeMqConsumerLog();
                mqConsumerLog.setMsgTag(tags);
                mqConsumerLog.setMsgKey(keys);
                mqConsumerLog.setConsumerStatus(ShopCode.SHOP_MQ_MESSAGE_STATUS_PROCESSING.getCode());
                mqConsumerLog.setMsgBody(body);
                mqConsumerLog.setMsgId(msgId);
                mqConsumerLog.setConsumerTimes(0);

                //将消息处理信息添加到数据库
                mqConsumerLogMapper.insert(mqConsumerLog);
            }
            //5. 回退库存
            MQEntity mqEntity = JSON.parseObject(body, MQEntity.class);
            Long goodsId = mqEntity.getGoodsId();
            TradeGoods goods = goodsMapper.selectByPrimaryKey(goodsId);
            goods.setGoodsNumber(goods.getGoodsNumber()+mqEntity.getGoodsNum());
            goodsMapper.updateByPrimaryKey(goods);

            //记录库存操作日志
            TradeGoodsNumberLog goodsNumberLog = new TradeGoodsNumberLog();
            goodsNumberLog.setOrderId(mqEntity.getOrderId());
            goodsNumberLog.setGoodsId(goodsId);
            goodsNumberLog.setGoodsNumber(mqEntity.getGoodsNum());
            goodsNumberLog.setLogTime(new Date());
            goodsNumberLogMapper.insert(goodsNumberLog);

            //6. 将消息的处理状态改为成功
            mqConsumerLog.setConsumerStatus(ShopCode.SHOP_MQ_MESSAGE_STATUS_SUCCESS.getCode());
            mqConsumerLog.setConsumerTimestamp(new Date());
            mqConsumerLogMapper.updateByPrimaryKey(mqConsumerLog);
            log.info("回退库存成功");
        } catch (Exception e) {
            e.printStackTrace();
            TradeMqConsumerLogKey primaryKey = new TradeMqConsumerLogKey();
            primaryKey.setMsgTag(tags);
            primaryKey.setMsgKey(keys);
            primaryKey.setGroupName(groupName);
            TradeMqConsumerLog mqConsumerLog = mqConsumerLogMapper.selectByPrimaryKey(primaryKey);
            if(mqConsumerLog==null){
                //数据库未有记录
                mqConsumerLog = new TradeMqConsumerLog();
                mqConsumerLog.setMsgTag(tags);
                mqConsumerLog.setMsgKey(keys);
                mqConsumerLog.setConsumerStatus(ShopCode.SHOP_MQ_MESSAGE_STATUS_FAIL.getCode());
                mqConsumerLog.setMsgBody(body);
                mqConsumerLog.setMsgId(msgId);
                mqConsumerLog.setConsumerTimes(1);
                mqConsumerLogMapper.insert(mqConsumerLog);
            }else{
                mqConsumerLog.setConsumerTimes(mqConsumerLog.getConsumerTimes()+1);
                mqConsumerLogMapper.updateByPrimaryKeySelective(mqConsumerLog);
            }
        }

    }
}
```

#### 2）回退优惠券

```java
@Slf4j
@Component
@RocketMQMessageListener(topic = "${mq.order.topic}",consumerGroup = "${mq.order.consumer.group.name}",messageModel = MessageModel.BROADCASTING )
public class CancelMQListener implements RocketMQListener<MessageExt>{


    @Autowired
    private TradeCouponMapper couponMapper;

    @Override
    public void onMessage(MessageExt message) {

        try {
            //1. 解析消息内容
            String body = new String(message.getBody(), "UTF-8");
            MQEntity mqEntity = JSON.parseObject(body, MQEntity.class);
            log.info("接收到消息");
            //2. 查询优惠券信息
            TradeCoupon coupon = couponMapper.selectByPrimaryKey(mqEntity.getCouponId());
            //3.更改优惠券状态
            coupon.setUsedTime(null);
            coupon.setIsUsed(ShopCode.SHOP_COUPON_UNUSED.getCode());
            coupon.setOrderId(null);
            couponMapper.updateByPrimaryKey(coupon);
            log.info("回退优惠券成功");
        } catch (UnsupportedEncodingException e) {
            e.printStackTrace();
            log.error("回退优惠券失败");
        }

    }
}
```

#### 3）回退余额

```java
@Slf4j
@Component
@RocketMQMessageListener(topic = "${mq.order.topic}",consumerGroup = "${mq.order.consumer.group.name}",messageModel = MessageModel.BROADCASTING )
public class CancelMQListener implements RocketMQListener<MessageExt>{


    @Autowired
    private IUserService userService;

    @Override
    public void onMessage(MessageExt messageExt) {

        try {
            //1.解析消息
            String body = new String(messageExt.getBody(), "UTF-8");
            MQEntity mqEntity = JSON.parseObject(body, MQEntity.class);
            log.info("接收到消息");
            if(mqEntity.getUserMoney()!=null && mqEntity.getUserMoney().compareTo(BigDecimal.ZERO)>0){
                //2.调用业务层,进行余额修改
                TradeUserMoneyLog userMoneyLog = new TradeUserMoneyLog();
                userMoneyLog.setUseMoney(mqEntity.getUserMoney());
                userMoneyLog.setMoneyLogType(ShopCode.SHOP_USER_MONEY_REFUND.getCode());
                userMoneyLog.setUserId(mqEntity.getUserId());
                userMoneyLog.setOrderId(mqEntity.getOrderId());
                userService.updateMoneyPaid(userMoneyLog);
                log.info("余额回退成功");
            }
        } catch (UnsupportedEncodingException e) {
            e.printStackTrace();
            log.error("余额回退失败");
        }

    }
}
```

#### 4）取消订单

```java
@Override
    public void onMessage(MessageExt messageExt) {
        String body = new String(messageExt.getBody(), "UTF-8");
        String msgId = messageExt.getMsgId();
        String tags = messageExt.getTags();
        String keys = messageExt.getKeys();
        log.info("CancelOrderProcessor receive message:"+messageExt);
        CancelOrderMQ cancelOrderMQ = JSON.parseObject(body, CancelOrderMQ.class);
        TradeOrder order = orderService.findOne(cancelOrderMQ.getOrderId());
		order.setOrderStatus(ShopCode.SHOP_ORDER_CANCEL.getCode());
        orderService.changeOrderStatus(order);
        log.info("订单:["+order.getOrderId()+"]状态设置为取消");
        return order;
    }
```

## 8.3 测试

### 1）准备测试环境

```java
@RunWith(SpringRunner.class)
@SpringBootTest(classes = ShopOrderServiceApplication.class)
public class OrderTest {

    @Autowired
    private IOrderService orderService;
}
```

### 2）准备测试数据

* 用户数据
* 商品数据
* 优惠券数据

### 3）测试下单成功流程

```java
@Test    
public void add(){
    Long goodsId=XXXL;
    Long userId=XXXL;
    Long couponId=XXXL;

    TradeOrder order = new TradeOrder();
    order.setGoodsId(goodsId);
    order.setUserId(userId);
    order.setGoodsNumber(1);
    order.setAddress("北京");
    order.setGoodsPrice(new BigDecimal("5000"));
    order.setOrderAmount(new BigDecimal("5000"));
    order.setMoneyPaid(new BigDecimal("100"));
    order.setCouponId(couponId);
    order.setShippingFee(new BigDecimal(0));
    orderService.confirmOrder(order);
}
```

执行完毕后,查看数据库中用户的余额、优惠券数据，及订单的状态数据

### 4）测试下单失败流程

代码同上。

执行完毕后，查看用户的余额、优惠券数据是否发生更改，订单的状态是否为取消。

## 支付业务

### 9.1 创建支付订单

![](https://pic.imgdb.cn/item/5f145da214195aa594e3e91e.jpg)

```java
public Result createPayment(TradePay tradePay) {
    //查询订单支付状态
    try {
        TradePayExample payExample = new TradePayExample();
        TradePayExample.Criteria criteria = payExample.createCriteria();
        criteria.andOrderIdEqualTo(tradePay.getOrderId());
        criteria.andIsPaidEqualTo(ShopCode.SHOP_ORDER_PAY_STATUS_IS_PAY.getCode());
        int count = tradePayMapper.countByExample(payExample);
        if (count > 0) {
            CastException.cast(ShopCode.SHOP_ORDER_PAY_STATUS_IS_PAY);
        }

        long payId = idWorker.nextId();
        tradePay.setPayId(payId);
        tradePay.setIsPaid(ShopCode.SHOP_ORDER_PAY_STATUS_NO_PAY.getCode());
        tradePayMapper.insert(tradePay);
        log.info("创建支付订单成功:" + payId);
    } catch (Exception e) {
        return new Result(ShopCode.SHOP_FAIL.getSuccess(), ShopCode.SHOP_FAIL.getMessage());
    }
    return new Result(ShopCode.SHOP_SUCCESS.getSuccess(), ShopCode.SHOP_SUCCESS.getMessage());
}
```

### 9.2 支付回调 

####  9.2.1 流程分析

![](https://pic.imgdb.cn/item/5f145df714195aa594e4085c.jpg)

![](https://pic.imgdb.cn/item/5f145e3214195aa594e419e7.jpg)

#### 9.2.2 代码实现

```java
public Result callbackPayment(TradePay tradePay) {

    if (tradePay.getIsPaid().equals(ShopCode.SHOP_ORDER_PAY_STATUS_IS_PAY.getCode())) {
        tradePay = tradePayMapper.selectByPrimaryKey(tradePay.getPayId());
        if (tradePay == null) {
            CastException.cast(ShopCode.SHOP_PAYMENT_NOT_FOUND);
        }
        tradePay.setIsPaid(ShopCode.SHOP_ORDER_PAY_STATUS_IS_PAY.getCode());
        int i = tradePayMapper.updateByPrimaryKeySelective(tradePay);
        //更新成功代表支付成功
        if (i == 1) {
            TradeMqProducerTemp mqProducerTemp = new TradeMqProducerTemp();
            mqProducerTemp.setId(String.valueOf(idWorker.nextId()));
            mqProducerTemp.setGroupName("payProducerGroup");
            mqProducerTemp.setMsgKey(String.valueOf(tradePay.getPayId()));
            mqProducerTemp.setMsgTag(topic);
            mqProducerTemp.setMsgBody(JSON.toJSONString(tradePay));
            mqProducerTemp.setCreateTime(new Date());
            mqProducerTempMapper.insert(mqProducerTemp);
            TradePay finalTradePay = tradePay;
            executorService.submit(new Runnable() {
                @Override
                public void run() {
                    try {
                        SendResult sendResult = sendMessage(topic, 
                                                            tag, 
                                                            finalTradePay.getPayId(), 
                                                            JSON.toJSONString(finalTradePay));
                        log.info(JSON.toJSONString(sendResult));
                        if (SendStatus.SEND_OK.equals(sendResult.getSendStatus())) {
                            mqProducerTempMapper.deleteByPrimaryKey(mqProducerTemp.getId());
                            System.out.println("删除消息表成功");
                        }
                    } catch (Exception e) {
                        e.printStackTrace();
                    }
                }
            });
        } else {
            CastException.cast(ShopCode.SHOP_PAYMENT_IS_PAID);
        }
    }
    return new Result(ShopCode.SHOP_SUCCESS.getSuccess(), ShopCode.SHOP_SUCCESS.getMessage());
}
```

##### 线程池优化消息发送逻辑

* 创建线程池对象

```java
@Bean
public ThreadPoolTaskExecutor getThreadPool() {

    ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();

    executor.setCorePoolSize(4);

    executor.setMaxPoolSize(8);

    executor.setQueueCapacity(100);

    executor.setKeepAliveSeconds(60);

    executor.setThreadNamePrefix("Pool-A");

    executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy());

    executor.initialize();

    return executor;

}
```

* 使用线程池

```java
@Autowired
private ThreadPoolTaskExecutor executorService;

executorService.submit(new Runnable() {
    @Override
    public void run() {
        try {
            SendResult sendResult = sendMessage(topic, tag, finalTradePay.getPayId(), JSON.toJSONString(finalTradePay));
            log.info(JSON.toJSONString(sendResult));
            if (SendStatus.SEND_OK.equals(sendResult.getSendStatus())) {
                mqProducerTempMapper.deleteByPrimaryKey(mqProducerTemp.getId());
                System.out.println("删除消息表成功");
            }
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
});
```



#### 9.2.3 处理消息

支付成功后，支付服务payService发送MQ消息，订单服务、用户服务、日志服务需要订阅消息进行处理

1. 订单服务修改订单状态为已支付
2. 日志服务记录支付日志
3. 用户服务负责给用户增加积分

以下用订单服务为例说明消息的处理情况

##### 1）配置RocketMQ属性值

```properties
mq.pay.topic=payTopic
mq.pay.consumer.group.name=pay_payTopic_group
```

##### 2）消费消息

* 在订单服务中，配置公共的消息处理类

```java
public class BaseConsumer {

    public TradeOrder handleMessage(IOrderService 
                                    orderService, 
                                    MessageExt messageExt,Integer code) throws Exception {
        //解析消息内容
        String body = new String(messageExt.getBody(), "UTF-8");
        String msgId = messageExt.getMsgId();
        String tags = messageExt.getTags();
        String keys = messageExt.getKeys();
        OrderMQ orderMq = JSON.parseObject(body, OrderMQ.class);
        
        //查询
        TradeOrder order = orderService.findOne(orderMq.getOrderId());

        if(ShopCode.SHOP_ORDER_MESSAGE_STATUS_CANCEL.getCode().equals(code)){
            order.setOrderStatus(ShopCode.SHOP_ORDER_CANCEL.getCode());
        }

        if(ShopCode.SHOP_ORDER_MESSAGE_STATUS_ISPAID.getCode().equals(code)){
            order.setPayStatus(ShopCode.SHOP_ORDER_PAY_STATUS_IS_PAY.getCode());
        }
        orderService.changeOrderStatus(order);
        return order;
    }

}
```

* 接受订单支付成功消息

```java
@Slf4j
@Component
@RocketMQMessageListener(topic = "${mq.pay.topic}", 
                         consumerGroup = "${mq.pay.consumer.group.name}")
public class PayConsumer extends BaseConsumer implements RocketMQListener<MessageExt> {

    @Autowired
    private IOrderService orderService;

    @Override
    public void onMessage(MessageExt messageExt) {
        try {
            log.info("CancelOrderProcessor receive message:"+messageExt);
            TradeOrder order = handleMessage(orderService, 
                                             messageExt, 
                                             ShopCode.SHOP_ORDER_MESSAGE_STATUS_ISPAID.getCode());
            log.info("订单:["+order.getOrderId()+"]支付成功");
        } catch (Exception e) {
            e.printStackTrace();
            log.error("订单支付失败");
        }
    }
}
```

## 整体联调

通过Rest客户端请求shop-order-web和shop-pay-web完成下单和支付操作

### 10.1 准备工作

#### 1）配置RestTemplate类

```java
@Configuration
public class RestTemplateConfig {

    @Bean
    @ConditionalOnMissingBean({ RestOperations.class, RestTemplate.class })
    public RestTemplate restTemplate(ClientHttpRequestFactory factory) {

        RestTemplate restTemplate = new RestTemplate(factory);

        // 使用 utf-8 编码集的 conver 替换默认的 conver（默认的 string conver 的编码集为"ISO-8859-1"）
        List<HttpMessageConverter<?>> messageConverters = restTemplate.getMessageConverters();
        Iterator<HttpMessageConverter<?>> iterator = messageConverters.iterator();
        while (iterator.hasNext()) {
            HttpMessageConverter<?> converter = iterator.next();
            if (converter instanceof StringHttpMessageConverter) {
                iterator.remove();
            }
        }
        messageConverters.add(new StringHttpMessageConverter(Charset.forName("UTF-8")));

        return restTemplate;
    }

    @Bean
    @ConditionalOnMissingBean({ClientHttpRequestFactory.class})
    public ClientHttpRequestFactory simpleClientHttpRequestFactory() {
        SimpleClientHttpRequestFactory factory = new SimpleClientHttpRequestFactory();
        // ms
        factory.setReadTimeout(15000);
        // ms
        factory.setConnectTimeout(15000);
        return factory;
    }
}
```

#### 2）配置请求地址

* 订单系统

```properties
server.host=http://localhost
server.servlet.path=/order-web
server.port=8080
shop.order.baseURI=${server.host}:${server.port}${server.servlet.path}
shop.order.confirm=/order/confirm
```

* 支付系统

```properties
server.host=http://localhost
server.servlet.path=/pay-web
server.port=9090
shop.pay.baseURI=${server.host}:${server.port}${server.servlet.path}
shop.pay.createPayment=/pay/createPayment
shop.pay.callbackPayment=/pay/callbackPayment
```

### 10.2 下单测试

 ```java
@RunWith(SpringRunner.class)
@ContextConfiguration(classes = ShopOrderWebApplication.class)
@TestPropertySource("classpath:application.properties")
public class OrderTest {

    @Autowired
    private RestTemplate restTemplate;

    @Value("${shop.order.baseURI}")
    private String baseURI;

    @Value("${shop.order.confirm}")
    private String confirmOrderPath;

    @Autowired
    private IDWorker idWorker;
   
   /**
     * 下单
     */
    @Test
    public void confirmOrder(){
        Long goodsId=XXXL;
        Long userId=XXXL;
        Long couponId=XXXL;

        TradeOrder order = new TradeOrder();
        order.setGoodsId(goodsId);
        order.setUserId(userId);
        order.setGoodsNumber(1);
        order.setAddress("北京");
        order.setGoodsPrice(new BigDecimal("5000"));
        order.setOrderAmount(new BigDecimal("5000"));
        order.setMoneyPaid(new BigDecimal("100"));
        order.setCouponId(couponId);
        order.setShippingFee(new BigDecimal(0));

        Result result = restTemplate.postForEntity(baseURI + confirmOrderPath, order, Result.class).getBody();
        System.out.println(result);
    }

}
 ```

### 10.3 支付测试

```java
@RunWith(SpringRunner.class)
@ContextConfiguration(classes = ShopPayWebApplication.class)
@TestPropertySource("classpath:application.properties")
public class PayTest {

    @Autowired
    private RestTemplate restTemplate;

    @Value("${shop.pay.baseURI}")
    private String baseURI;

    @Value("${shop.pay.createPayment}")
    private String createPaymentPath;

    @Value("${shop.pay.callbackPayment}")
    private String callbackPaymentPath;

    @Autowired
    private IDWorker idWorker;

   /**
     * 创建支付订单
     */
    @Test
    public void createPayment(){

        Long orderId = 346321587315814400L;
        TradePay pay = new TradePay();
        pay.setOrderId(orderId);
        pay.setPayAmount(new BigDecimal(4800));

        Result result = restTemplate.postForEntity(baseURI + createPaymentPath, pay, Result.class).getBody();
        System.out.println(result);
    }
   
    /**
     * 支付回调
     */
    @Test
    public void callbackPayment(){
        Long payId = 346321891507720192L;
        TradePay pay = new TradePay();
        pay.setPayId(payId);
        pay.setIsPaid(ShopCode.SHOP_ORDER_PAY_STATUS_IS_PAY.getCode());
        Result result = restTemplate.postForEntity(baseURI + callbackPaymentPath, pay, Result.class).getBody();
        System.out.println(result);

    }

}
```

## 高级功能

### 11.1消息存储

分布式队列因为有高可靠性，所以数据要进行持久化存储

![](https://pic.imgdb.cn/item/5f10556f14195aa594b1f6fa.jpg)

1. 消息生产者发送消息

2. MQ收到消息，将消息进行持久化，在存储中新增一条记录

3. 返回ACK给生产者

4. MQ push消息给对应的消费者，然后等待消费者返回ACK
5. 如果消息消费者在指定时间内成功返回ack，那么MQ认为消息消费成功，在存储中删除消息，即执行第6步；如果MQ在指定时间内没有收到ACK，则认为消息消费失败，会尝试重新push消息,重复执行4、5、6步骤
6. MQ删除消息

#### 11.1.1 存储介质

* 关系型数据库DB

Apache下开源的另外一款MQ—ActiveMQ（默认采用的KahaDB做消息存储）可选用JDBC的方式来做消息持久化，通过简单的xml配置信息即可实现JDBC消息存储。由于，普通关系型数据库（如Mysql）在单表数据量达到千万级别的情况下，其IO读写性能往往会出现瓶颈。在可靠性方面，该种方案非常依赖DB，如果一旦DB出现故障，则MQ的消息就无法落盘存储会导致线上故障。

![](https://pic.imgdb.cn/item/5f10575214195aa594b28e20.jpg)

- 文件系统

  目前业界较为常用的几款产品（RocketMQ/Kafka/RabbitMQ）均采用的是消息刷盘至所部署虚拟机/物理机的文件系统来做持久化（刷盘一般可以分为异步刷盘和同步刷盘两种模式）。消息刷盘为消息存储提供了一种高效率、高可靠性和高性能的数据持久化方式。除非部署MQ机器本身或是本地磁盘挂了，否则一般是不会出现无法持久化的故障问题。

![](https://pic.imgdb.cn/item/5f10578414195aa594b29cc1.jpg)

#### 11.1.2 性能对比

文件系统>关系型数据库DB

#### 11.1.3 消息的存储和发送

##### 1）消息存储

磁盘如果使用得当，磁盘的速度完全可以匹配上网络 的数据传输速度。目前的高性能磁盘，顺序写速度可以达到600MB/s， 超过了一般网卡的传输速度。但是磁盘随机写的速度只有大概100KB/s，和顺序写的性能相差6000倍！因为有如此巨大的速度差别，好的消息队列系统会比普通的消息队列系统速度快多个数量级。RocketMQ的消息用顺序写,保证了消息存储的速度。

##### 2）消息发送

Linux操作系统分为【用户态】和【内核态】，文件操作、网络操作需要涉及这两种形态的切换，免不了进行数据复制。

一台服务器 把本机磁盘文件的内容发送到客户端，一般分为两个步骤：

1）read；读取本地文件内容； 

2）write；将读取的内容通过网络发送出去。

这两个看似简单的操作，实际进行了4 次数据复制，分别是：

1. 从磁盘复制数据到内核态内存；
2. 从内核态内存复 制到用户态内存；(省去的是这一步)
3. 然后从用户态 内存复制到网络驱动的内核态内存；
4. 最后是从网络驱动的内核态内存复 制到网卡中进行传输。

![](https://pic.imgdb.cn/item/5f1058a114195aa594b2eeb5.jpg)

RocketMQ充分利用了上述特性，也就是所谓的“零拷贝”技术，提高消息存盘和网络发送的速度。

> 这里需要注意的是，采用MappedByteBuffer这种内存映射的方式有几个限制，其中之一是一次只能映射1.5~2G 的文件至用户态的虚拟内存，这也是为何RocketMQ默认设置单个CommitLog日志数据文件为1G的原因了

#### 11.1.4 消息存储结构

RocketMQ消息的存储是由ConsumeQueue和CommitLog配合完成 的，消息真正的物理存储文件是CommitLog，ConsumeQueue是消息的逻辑队列，类似数据库的索引文件，存储的是指向物理存储的地址。每 个Topic下的每个Message Queue都有一个对应的ConsumeQueue文件。

![](https://pic.imgdb.cn/item/5f10596f14195aa594b32a01.jpg)

* CommitLog：存储消息的元数据
* ConsumerQueue：存储消息在CommitLog的索引
* IndexFile：为了消息查询提供了一种通过key或时间区间来查询消息的方法，这种通过IndexFile来查找消息的方法不影响发送与消费消息的主流程

#### 11.1.5 刷盘机制

RocketMQ的消息是存储到磁盘上的，这样既能保证断电后恢复， 又可以让存储的消息量超出内存的限制。RocketMQ为了提高性能，会尽可能地保证磁盘的顺序写。消息在通过Producer写入RocketMQ的时 候，有两种写磁盘方式，分布式同步刷盘和异步刷盘。

![](https://pic.imgdb.cn/item/5f145ef914195aa594e45c89.jpg)

##### 1）同步刷盘

在返回写成功状态时，消息已经被写入磁盘。具体流程是，消息写入内存的PAGECACHE后，立刻通知刷盘线程刷盘， 然后等待刷盘完成，刷盘线程执行完成后唤醒等待的线程，返回消息写 成功的状态。

##### 2）异步刷盘

在返回写成功状态时，消息可能只是被写入了内存的PAGECACHE，写操作的返回快，吞吐量大；当内存里的消息量积累到一定程度时，统一触发写磁盘动作，快速写入。

##### 3）配置

**同步刷盘还是异步刷盘，都是通过Broker配置文件里的flushDiskType 参数设置的，这个参数被配置成SYNC_FLUSH、ASYNC_FLUSH中的 一个。**

### 11.2 高可用性机制

![](https://pic.imgdb.cn/item/5f105b5a14195aa594b3bca4.jpg)

RocketMQ分布式集群是通过Master和Slave的配合达到高可用性的。

Master和Slave的区别：在Broker的配置文件中，参数 brokerId的值为0表明这个Broker是Master，大于0表明这个Broker是 Slave，同时brokerRole参数也会说明这个Broker是Master还是Slave。

Master角色的Broker支持读和写，Slave角色的Broker仅支持读，也就是 Producer只能和Master角色的Broker连接写入消息；Consumer可以连接 Master角色的Broker，也可以连接Slave角色的Broker来读取消息。

#### 11.2.1 消息消费高可用

在Consumer的配置文件中，并不需要设置是从Master读还是从Slave 读，当Master不可用或者繁忙的时候，Consumer会被自动切换到从Slave 读。有了自动切换Consumer这种机制，当一个Master角色的机器出现故障后，Consumer仍然可以从Slave读取消息，不影响Consumer程序。这就达到了消费端的高可用性。

#### 11.2.2 消息发送高可用

在创建Topic的时候，把Topic的多个Message Queue创建在多个Broker组上（相同Broker名称，不同 brokerId的机器组成一个Broker组），这样当一个Broker组的Master不可 用后，其他组的Master仍然可用，Producer仍然可以发送消息。 RocketMQ目前还不支持把Slave自动转成Master，如果机器资源不足， 需要把Slave转成Master，则要手动停止Slave角色的Broker，更改配置文 件，用新的配置文件启动Broker。



#### 11.2.3 消息主从复制

如果一个Broker组有Master和Slave，消息需要从Master复制到Slave 上，有同步和异步两种复制方式。

##### 1）同步复制

同步复制方式是等Master和Slave均写 成功后才反馈给客户端写成功状态；

在同步复制方式下，如果Master出故障， Slave上有全部的备份数据，容易恢复，但是同步复制会增大数据写入 延迟，降低系统吞吐量。

##### 2）异步复制 

异步复制方式是只要Master写成功 即可反馈给客户端写成功状态。

在异步复制方式下，系统拥有较低的延迟和较高的吞吐量，但是如果Master出了故障，有些数据因为没有被写 入Slave，有可能会丢失；

##### 3）配置

同步复制和异步复制是通过Broker配置文件里的brokerRole参数进行设置的，这个参数可以被设置成ASYNC_MASTER、 SYNC_MASTER、SLAVE三个值中的一个。

##### 4）总结

![](https://pic.imgdb.cn/item/5f105de114195aa594b477c7.jpg)



实际应用中要结合业务场景，合理设置刷盘方式和主从复制方式， 尤其是SYNC_FLUSH方式，由于频繁地触发磁盘写动作，会明显降低 性能。通常情况下，应该把Master和Save配置成ASYNC_FLUSH的刷盘 方式，主从之间配置成SYNC_MASTER的复制方式，这样即使有一台 机器出故障，仍然能保证数据不丢，是个不错的选择。

### 11.3 负载均衡

#### 11.3.1 Producer负载均衡

Producer端，每个实例在发消息的时候，默认会轮询所有的message queue发送，以达到让消息平均落在不同的queue上。而由于queue可以散落在不同的broker，所以消息就发送到不同的broker下，如下图：

![](https://pic.imgdb.cn/item/5f105e2214195aa594b48cca.jpg)

图中箭头线条上的标号代表顺序，发布方会把第一条消息发送至 Queue 0，然后第二条消息发送至 Queue 1，以此类推。

#### 11.3.2 Consumer负载均衡

##### 1）集群模式

在集群消费模式下，每条消息只需要投递到订阅这个topic的Consumer Group下的一个实例即可。RocketMQ采用主动拉取的方式拉取并消费消息，在拉取的时候需要明确指定拉取哪一条message queue。

而每当实例的数量有变更，都会触发一次所有实例的负载均衡，这时候会按照queue的数量和实例的数量平均分配queue给每个实例。

默认的分配算法是AllocateMessageQueueAveragely，如下图：

![](https://pic.imgdb.cn/item/5f105e7d14195aa594b4aa8b.jpg)

还有另外一种平均的算法是AllocateMessageQueueAveragelyByCircle，也是平均分摊每一条queue，只是以环状轮流分queue的形式，如下图：

![](https://pic.imgdb.cn/item/5f105ec514195aa594b4bfa4.jpg)

需要注意的是，集群模式下，queue都是只允许分配只一个实例，这是由于如果多个实例同时消费一个queue的消息，由于拉取哪些消息是consumer主动控制的，那样会导致同一个消息在不同的实例下被消费多次，所以算法上都是一个queue只分给一个consumer实例，一个consumer实例可以允许同时分到不同的queue。

通过增加consumer实例去分摊queue的消费，可以起到水平扩展的消费能力的作用。而有实例下线的时候，会重新触发负载均衡，这时候原来分配到的queue将分配到其他实例上继续消费。

但是如果consumer实例的数量比message queue的总数量还多的话，多出来的consumer实例将无法分到queue，也就无法消费到消息，也就无法起到分摊负载的作用了。所以需要控制让queue的总数量大于等于consumer的数量。

##### 2）广播模式

由于广播模式下要求一条消息需要投递到一个消费组下面所有的消费者实例，所以也就没有消息被分摊消费的说法。

在实现上，其中一个不同就是在consumer分配queue的时候，所有consumer都分到所有的queue。

![](https://pic.imgdb.cn/item/5f145f0f14195aa594e461fe.jpg)



### 11.4 消息重试

#### 11.4.1 顺序消息的重试

对于顺序消息，当消费者消费消息失败后，消息队列 RocketMQ 会自动不断进行消息重试（每次间隔时间为 1 秒），这时，应用会出现消息消费被阻塞的情况。因此，在使用顺序消息时，务必保证应用能够及时监控并处理消费失败的情况，避免阻塞现象的发生。

#### 11.4.2 无序消息的重试

对于无序消息（普通、定时、延时、事务消息），当消费者消费消息失败时，您可以通过设置返回状态达到消息重试的结果。

无序消息的重试只针对集群消费方式生效；广播方式不提供失败重试特性，即消费失败后，失败消息不再重试，继续消费新的消息。

##### 1）重试次数

消息队列 RocketMQ 默认允许每条消息最多重试 16 次，每次重试的间隔时间如下：

| 第几次重试 | 与上次重试的间隔时间 | 第几次重试 | 与上次重试的间隔时间 |
| :--------: | :------------------: | :--------: | :------------------: |
|     1      |        10 秒         |     9      |        7 分钟        |
|     2      |        30 秒         |     10     |        8 分钟        |
|     3      |        1 分钟        |     11     |        9 分钟        |
|     4      |        2 分钟        |     12     |       10 分钟        |
|     5      |        3 分钟        |     13     |       20 分钟        |
|     6      |        4 分钟        |     14     |       30 分钟        |
|     7      |        5 分钟        |     15     |        1 小时        |
|     8      |        6 分钟        |     16     |        2 小时        |

如果消息重试 16 次后仍然失败，消息将不再投递。如果严格按照上述重试时间间隔计算，某条消息在一直消费失败的前提下，将会在接下来的 4 小时 46 分钟之内进行 16 次重试，超过这个时间范围消息将不再重试投递。

**注意：** 一条消息无论重试多少次，这些重试消息的 Message ID 不会改变。

##### 2）配置方式

**消费失败后，重试配置方式**

集群消费方式下，消息消费失败后期望消息重试，需要在消息监听器接口的实现中明确进行配置（三种方式任选一种）：

- 返回 Action.ReconsumeLater （推荐）
- 返回 Null
- 抛出异常

```java
public class MessageListenerImpl implements MessageListener {
    @Override
    public Action consume(Message message, ConsumeContext context) {
        //处理消息
        doConsumeMessage(message);
        //方式1：返回 Action.ReconsumeLater，消息将重试
        return Action.ReconsumeLater;
        //方式2：返回 null，消息将重试
        return null;
        //方式3：直接抛出异常， 消息将重试
        throw new RuntimeException("Consumer Message exceotion");
    }
}
```

**消费失败后，不重试配置方式**

集群消费方式下，消息失败后期望消息不重试，需要捕获消费逻辑中可能抛出的异常，最终返回 Action.CommitMessage，此后这条消息将不会再重试。

```java
public class MessageListenerImpl implements MessageListener {
    @Override
    public Action consume(Message message, ConsumeContext context) {
        try {
            doConsumeMessage(message);
        } catch (Throwable e) {
            //捕获消费逻辑中的所有异常，并返回 Action.CommitMessage;
            return Action.CommitMessage;
        }
        //消息处理正常，直接返回 Action.CommitMessage;
        return Action.CommitMessage;
    }
}
```

**自定义消息最大重试次数**

消息队列 RocketMQ 允许 Consumer 启动的时候设置最大重试次数，重试时间间隔将按照如下策略：

- 最大重试次数小于等于 16 次，则重试时间间隔同上表描述。
- 最大重试次数大于 16 次，超过 16 次的重试时间间隔均为每次 2 小时。

```java
Properties properties = new Properties();
//配置对应 Group ID 的最大消息重试次数为 20 次
properties.put(PropertyKeyConst.MaxReconsumeTimes,"20");
Consumer consumer =ONSFactory.createConsumer(properties);
```

> 注意：
>
> - 消息最大重试次数的设置对相同 Group ID 下的所有 Consumer 实例有效。
> - 如果只对相同 Group ID 下两个 Consumer 实例中的其中一个设置了 MaxReconsumeTimes，那么该配置对两个 Consumer 实例均生效。
> - 配置采用覆盖的方式生效，即最后启动的 Consumer 实例会覆盖之前的启动实例的配置

**获取消息重试次数**

消费者收到消息后，可按照如下方式获取消息的重试次数：

```java
public class MessageListenerImpl implements MessageListener {
    @Override
    public Action consume(Message message, ConsumeContext context) {
        //获取消息的重试次数
        System.out.println(message.getReconsumeTimes());
        return Action.CommitMessage;
    }
}
```

### 11.5 死信队列

当一条消息初次消费失败，消息队列 RocketMQ 会自动进行消息重试；达到最大重试次数后，若消费依然失败，则表明消费者在正常情况下无法正确地消费该消息，此时，消息队列 RocketMQ 不会立刻将消息丢弃，而是将其发送到该消费者对应的特殊队列中。

在消息队列 RocketMQ 中，这种正常情况下无法被消费的消息称为死信消息（Dead-Letter Message），存储死信消息的特殊队列称为死信队列（Dead-Letter Queue）。

#### 11.5.1 死信特性

死信消息具有以下特性

- 不会再被消费者正常消费。
- 有效期与正常消息相同，均为 3 天，3 天后会被自动删除。因此，请在死信消息产生后的 3 天内及时处理。

死信队列具有以下特性：

- 一个死信队列对应一个 Group ID， 而不是对应单个消费者实例。
- 如果一个 Group ID 未产生死信消息，消息队列 RocketMQ 不会为其创建相应的死信队列。
- 一个死信队列包含了对应 Group ID 产生的所有死信消息，不论该消息属于哪个 Topic。

#### 11.5.2 查看死信信息

1. 在控制台查询出现死信队列的主题信息

![](https://pic.imgdb.cn/item/5f10608f14195aa594b54889.jpg)

2. 在消息界面根据主题查询死信消息

![](https://pic.imgdb.cn/item/5f145f2c14195aa594e4694c.jpg)

3. 选择重新发送消息

一条消息进入死信队列，意味着某些因素导致消费者无法正常消费该消息，因此，通常需要您对其进行特殊处理。排查可疑因素并解决问题后，可以在消息队列 RocketMQ 控制台重新发送该消息，让消费者重新消费一次。

### 11.6 消费幂等

消息队列 RocketMQ 消费者在接收到消息以后，有必要根据业务上的唯一 Key 对消息做幂等处理的必要性。

#### 11.6.1 消费幂等的必要性

在互联网应用中，尤其在网络不稳定的情况下，消息队列 RocketMQ 的消息有可能会出现重复，这个重复简单可以概括为以下情况：

- 发送时消息重复

  当一条消息已被成功发送到服务端并完成持久化，此时出现了网络闪断或者客户端宕机，导致服务端对客户端应答失败。 如果此时生产者意识到消息发送失败并尝试再次发送消息，消费者后续会收到两条内容相同并且 Message ID 也相同的消息。

- 投递时消息重复

  消息消费的场景下，消息已投递到消费者并完成业务处理，当客户端给服务端反馈应答的时候网络闪断。 为了保证消息至少被消费一次，消息队列 RocketMQ 的服务端将在网络恢复后再次尝试投递之前已被处理过的消息，消费者后续会收到两条内容相同并且 Message ID 也相同的消息。

- 负载均衡时消息重复（包括但不限于网络抖动、Broker 重启以及订阅方应用重启）

  当消息队列 RocketMQ 的 Broker 或客户端重启、扩容或缩容时，会触发 Rebalance，此时消费者可能会收到重复消息。

#### 11.6.2 处理方式

因为 Message ID 有可能出现冲突（重复）的情况，所以真正安全的幂等处理，不建议以 Message ID 作为处理依据。 最好的方式是以业务唯一标识作为幂等处理的关键依据，而业务的唯一标识可以通过消息 Key 进行设置：

```java
Message message = new Message();
message.setKey("ORDERID_100");
SendResult sendResult = producer.send(message);
```

订阅方收到消息时可以根据消息的 Key 进行幂等处理：

```java
consumer.subscribe("ons_test", "*", new MessageListener() {
    public Action consume(Message message, ConsumeContext context) {
        String key = message.getKey()
        // 根据业务唯一标识的 key 做幂等处理
    }
});
```

# RabbitMQ

# Kafka



# TCP/IP

## 3次握手4次关闭

![](https://pic.imgdb.cn/item/60a21d196ae4f77d35f626c0.jpg)

第一次握手
当客户端向服务器发起连接请求时，客户端会发送同步序列标号SYN到服务器，在这里我们设SYN为x，等待服务器确认，这时客户端的状态为SYN_SENT。

第二次握手
当服务器收到客户端发送的SYN后，服务器要做的是确认客户端发送过来的SYN，在这里服务器发送确认包ACK，这里的ACK为x+1，意思是说“我收到了你发送的SYN了”，同时，服务器也会向客户端发送一个SYN包，这里我们设SYN为y。这时服务器的状态为SYN_RECV。

一句话，服务器端发送SYN和ACK两个包。

第三次握手
客户端收到服务器发送的SYN和ACK包后，需向服务器发送确认包ACK，“我也收到你发送的SYN了，我这就给你发个确认过去，然后我们即能合体了”，这里的ACK为y+1，发送完毕后，客户端和服务器的状态为ESTABLISH，即TCP连接成功。

在三次握手中，客户端和服务器端都发送两个包SYN和ACK，只不过服务器端的两个包是一次性发过来的，客户端的两个包是分两次发送的。

换个易于理解的视角来看为什么要三次握手。
客户端和服务端通信前要进行连接，“三次握手”的作用就是双方都能明确自己和对方的收、发能力是正常的。

第一次握手：客户端发送网络包，服务端收到了。这样服务端就能得出结论：客户端的发送能力、服务端的接收能力是正常的。

第二次握手：服务端发包，客户端收到了。这样客户端就能得出结论：服务端的接收、发送能力，客户端的接收、发送能力是正常的。

从客户端的视角来看，我接到了服务端发送过来的响应数据包，说明服务端接收到了我在第一次握手时发送的网络包，并且成功发送了响应数据包，这就说明，服务端的接收、发送能力正常。

而另一方面，我收到了服务端的响应数据包，说明我第一次发送的网络包成功到达服务端，这样，我自己的发送和接收能力也是正常的。

第三次握手：客户端发包，服务端收到了。这样服务端就能得出结论：客户端的接收、发送能力，服务端的发送、接收能力是正常的。

第一、二次握手后，服务端并不知道客户端的接收能力以及自己的发送能力是否正常。

而在第三次握手时，服务端收到了客户端对第二次握手作的回应。从服务端的角度，我在第二次握手时的响应数据发送出去了，客户端接收到了。所以，我的发送能力是正常的。而客户端的接收能力也是正常的。

经历了上面的三次握手过程，客户端和服务端都确认了自己的接收、发送能力是正常的。之后就可以正常通信了。

每次都是接收到数据包的一方可以得到一些结论，发送的一方其实没有任何头绪。

我虽然有发包的动作，但是我怎么知道我有没有发出去，而对方有没有接收到呢？

而从上面的过程可以看到，最少是需要三次握手过程的。两次达不到让双方都得出自己、对方的接收、发送能力都正常的结论。

## 四次挥手关闭连接

当客户端端和B端要断开连接时，需要四次握手，这里称为四次挥手。
断开连接请求可以由客户端发出，也可以由服务器端发出，在这里我们客户端向B端请求断开连接。

第一次挥手
客户端向服务器端请求断开连接时会向B端发送一个带有FIN标记的报文段，这里的FIN是FINish的意思。

第二次挥手
服务器端收到客户端发送的FIN后，服务器端现在可能现在还有数据没有传完，所以服务器端并不会马上向客户端发送FIN，而是先发送一个确认序号ACK，意思是说“你发的断开连接请求我收到了，但是我现在还有数据没有发完，请稍等一下呗”。

第三次挥手
当服务器端的事情忙完了，那么此时服务器端就可以断开连接了，此时服务器端向客户端发送FIN序号，意思是这次可以断开连接了。

第四次挥手
客户端收到服务器端发送的FIN后，会向服务器端发送确认ACK，然后经过两个MSL时长后断开连接。

MSL是Maximum Segment Lifetime，最大报文段生存时间，2个MSL是报文段发送和接收的最长时间。在RFC 793中定义MSL通常为2分钟，即超过两分钟即认为这个报文已经在网络中被丢弃了。
在 Linux 中查看默认的MSL值（60s）：

```
[root@DanCentOS65var]# cat /proc/sys/net/ipv4/tcp_fin_timeout
60
```

====

TCP 连接是双向传输的对等的模式，就是说双方都可以同时向对方发送或接收数据。

当有一方要关闭连接时，会发送指令告知对方，我要关闭连接了。这时对方会回一个 ACK，此时一个方向的连接关闭。

但是另一个方向仍然可以继续传输数据，等到发送完了所有的数据后，会发送一个 FIN 段来关闭此方向上的连接。接收方发送 ACK 确认关闭连接。

注意，接收到 FIN 报文的一方只能回复一个 ACK, 它是无法马上返回对方一个 FIN 报文段的，因为结束数据传输的“指令”是上层应用层给出的，我只是一个“搬运工”，我无法了解“上层的意志”。

## 为什么在第四次挥手后会有2个MSL的延时？

假定网络不可靠，那么第四次发送的ACK可能丢失，即服务器端无法收到这个ACK，如果服务器端收不到这个确认ACK，服务器端会定时向客户端重复发送FIN，直到服务器端收到客户端的确认ACK。所以这个2MSL就是用来处理这个可能丢失的ACK的

## ISN

三次握手的一个重要功能是客户端和服务端交换 SYN 段里面指明 ISN(Initial Sequence Number), 以便让对方知道接下来接收数据的时候如何按序列号组装数据。
如果 ISN 是固定的，攻击者很容易猜出后续的确认号：

```
ISN = M + F(localhost, localport, remotehost, remoteport)
```

M 是一个计时器，每隔 4 毫秒加 1。F 是一个 Hash 算法，根据源 IP、目的 IP、源端口、目的端口生成一个随机数值。要保证 Hash 算法不能被外部轻易推算得出。



# 图解TCP/IP

## 2. TCP/IP基础知识

### 2.4 协议分层模型

![](https://pic.imgdb.cn/item/60a1f5216ae4f77d3559ba9e.jpg)

### 2.5 分层模型与通信示例

![](https://pic.imgdb.cn/item/60a1f5516ae4f77d355b76e3.jpg)

## 4. IP协议

### 4.2 IP基础知识

​	MAC地址用来标识同一个链路不同计算机的识别码。IP用于"连接到网络中的所有主机中识别出进行通信的目标地址"。

* **一跳(hop)**: 指利用数据链路层以下分层的功能传输数据帧的一个区间。
* **分片处理**: 将较大的IP包分为多个较小的IP包。
  * 只能由目标主机进行。
  * 路由器出出力负荷会加重
* 路由MTU发现
  * 指发送端主机到接口端主机至今不需要分配时最大MTU的大小。
* 面向无连接。
  * 简化
  * 提速。
* ip地址: 32位正整数
  * 网络标识(前三)和主机标识。
  * 相同段内相连的主机必须有相同的网络地址。
* 分类
  * A类，首尾0开头的, 0.0.0.0~127.0.0.0
  * B类，前两位是10。128.0.0.1~191.255.0.0
  * C类，前三位是110， 192.168.0.0~239.255.255.0。
  * D类前四位1110，224.0.0.0~239.255.255.255
* 广播地址
  * 主机标识部分全部设置为1
  * 本地广播
    * 在本网络内的广播。
      * 192.168.0.255
  * 直接广播
    * 不同网络之间。
* 多播
  * 将包发给特定组内所有主机。
  * 使用D类地址，前4位是1110
* 子网掩码
  * 网络标识和主机标识不受限于地址类别，通过子网掩码细分更小粒度的网络。
  * 实际上就是将原来类别中的主机地址部分用作子网地址。
  * 32位数字
  * 子网掩码必须是首尾开始连续的1。
* 全局地址、私有地址
* 路由控制
  * 路由控制表
    * 管理员手动设置。(静态路由控制)
    * 路由器和其他路由器相互交换信息时自动刷新。(动态路由控制)
  * 默认路由
    * 指路由表中任何一个地址都能与之匹配的记录。
      * 0.0.0.0
  * 主机路由。
  * 环回地址。

### 4.6 IPv6

![](https://pic.imgdb.cn/item/60a1fada6ae4f77d35924317.jpg)

## 6. TCP与UDP 

* TCP
  * 面向连接、可靠的协议。
  * 可以保证发送顺序。
* UDP
  * 不可靠的数据报协议。



**套接字**

​	程序利用套接字，可以设置对端的IP地址、端口号，并实现数据的发送与接收。

![](https://pic.imgdb.cn/item/60a208866ae4f77d35239a1b.jpg)

**端口号**

​	用来识别本机中正在进行的通信应用程序。

​	TCP/IP或UDP/IP通常采用5个信息识别一个通信。

* 原ip地址
* 目标ip地址。
* 协议号。
* 源端口号。
* 目标端口号。



**如何确定端口号**

* 标准既定端口号。
* 时序分配法
  * 服务端有必要确定监听端口号，客户端没必要。



**TCP**

* 连接

  * 指各种设备、线路，或网络中进行通信的两个应用程序为了相互传递消息而转悠、虚拟的通信线路，也叫作虚拟电路

* 通过序列号与确认应答提高可靠性。

* 如何确认重发超时时间。

  * 理想: 最小时间，保证应答能返回。

  * 随网络环境不同有所变化。

  * Unix和Windows，超时以0.5秒进行控制，因此重发超时都是0.5整数倍。最初不知道往返时间，都设置为6秒。

  * 如果还是收不到应答，会以2,4倍的指数函数进行增长。

  * 重发超过一定次数，如果还是没有任何应答， 就会强制关闭连接。

  * TCP以段位段位发送数据。

  * 利用窗口提高速度

    * 多个段进行应答。
    * 当某一段报文丢失后，发送端会一直受到需要为1001的确认应答。发送端主机如果连续3次受到同一个确认应答，就会重发。

  * 流控制

    * 一种可以让发送端根据接收端的实际接收能力控制发送的数据量的机制。

  * 拥塞控制

    * 慢启动。

    * 最初设置为1个数据段大小。

    * 超时重设为1。

    * 重复确认应答会设置为当前窗口的一半。

    * 每次收到一个确认，拥塞窗口的值就+1。

      * 将拥塞窗口大小和接收端主机通知的窗口大小取最小值，发送比起还要小的数据列。
      * 随着包每次往返，拥塞窗口会以1、2、4等指数函数增长，激增可能会导致网络阻塞的发生。通过阈值控制，只要超出某个阈值，在每次收到一次确认应答时，只允许以下面这种比例方法拥塞窗口。

      

![](https://pic.imgdb.cn/item/60a217db6ae4f77d35c22282.jpg)

![](https://pic.imgdb.cn/item/60a2183a6ae4f77d35c5e053.jpg)

* Nagle算法
  * 指发送端即使还有应该发送的数据，但如果这部分数据量还很小，进行延迟发送的一种处理机制。
  * 仅在任意一个条件下才能发送数据。
    * 已发送的数据都已经收到确认应发时。
    * 可以发送最大段长度(MSS)的数据时。
* 延迟确认应发
  * 如果每次都立刻回复确认，可能会返回一个较小的窗口。
  * 为了提高网络利用率，引入了一个方法，收到数据以后不立即返回应答，而是延迟一段时间的机制。
    * 在没有2*最大段长度的数据为止，不做确认应发。
    * 其他情况下，最大延迟0.5秒发送确认应发(很多系统设置0.2秒左右)。
* 捎带应答
  * 根据应用层协议，发送出去的消息到达对端，对端进行处理后，会返回一个回执。
  * 此类通信中，确认应答和回执数据可以通过一个包发送，叫捎带应答。

# Redis

## 搭建主从

注意从端的masterauth。



## 搭建cluster



## 遇到的问题

**由于目标计算机积极拒绝，无法连接。**	

​	配置文件注销bind 127.0.0.1

**DENIED Redis is running in protected mode because protected mode is enabled**

以下几个方案都可

* 配置文件禁用protected mode
* 命令CONFIG SET protected-mode no
* 启动时添加参数--protected-mode no
* 设置密码或指定bind地址



**因为RDB目录不存在启动失败**

* 创建对应目录

## 服务启动

window环境下

**启动服务端**

在redis文件下执行以下命令

```
redis-server.exe redis.windows.conf
```

**启动客户端**

```
redis-cli 
```

## 数据类型

* string
* hash
* list
* set
* sorted_set



### **stirng**

```sh
# 添加修改数据
set key value

# 获取数据
get key

# 删除数据
del key

# 添加修改多个数据
mset key1 value1 key2 value2

# 获取多个数据
mget key1 key2

# 获取数据字符个数(字符长度)
strlen key

// 追加信息到原始信息后(如果存在追加，不存在则新建)
append key value
```

* 多数据操作可以节省数数据传输时间
* 需要考虑数据量，比如50条可能mset好，如果是1亿条，就要重新考虑，可能需要分组执行



* 设置数值数据增加/减少指定范围的值

```
incr key
incrby key increment
incrbyfloat key increment
decr key
decrby key increment
```

**string作为数值操作**

* string在redis内部存储默认就是一个字符串，当遇到增减类操作incr, decr时会转成数值型进行计算

* redis所有操作都是原子性的，采用单线程处理所有业务，命令是一个一个执行的，因此无需考虑并发带来的数据影响

* 注意：按数字进行操作时，如果原始数据不能转换为数值或者超宇了redis上限范围，将报错9223372036854775807（java Long最大值,Long.MAX_VALUE)


Tips 1:

* redis用于控制数据库主键id，为数据库表主键提供生成策略，保证数据库表的主键唯一性

* 此方案适用于所有数据库，且支持数据库集群

**声明周期**

```sh
setex key seconds vlaue
# 毫秒
psetex key milliseonds value
```

**注意事项**

* 数据操作不成功的反馈与数据正常操作之前的差异
  1. 表示运行结果是否成功
     * (integer)0 -> false 失败
     * (interger)1 -> true 成功
  2. 表示运行结果值
     * (integer) 3 -> 3 3个
     * (integer)1 -> 1 1个
* 数据未获取到
  (nil) 等同于null
* 数据量最大存储量
  512MB
* 数据计算最大范围(java中long的最大值)
  9223372036854775807

### hash

**存储的困惑**

对象类数据的存储如果具有较频繁的更新需求会显得笨重

* 新的存储需求：对一系列存储的数据进行编组，方便管理，典型应用存储对象信息
* 需要的存储结构：一个存储空间保存多个键值对数据
* has类型：底层使用哈希表结构实现数据存储

key -> field -> value

**hash存储结构优化**

* 如果field数量较少，存储结构优化为类结构
* 如果field数量较多，存储结构使用HashMap结构

#### hash类型数据的基本操作

```
// 添加修改数据
hset key field value

//获取数据
hget key field
hgetall key

//删除数据
hdel key field1 field2

// 添加修改多个
hmset key field1 value1 field2 value2

//获取多个数据
hmget key field1 field2

//获取hash表中字段的数量
hlen key

//获取hash表中是否存在指定字段
hexists key field
```

#### hash类型数据的扩展操作

```
//获取hash表中的字段名或者字段值
hkeys key
hvals key
//设置制定字段数值数据增加制定范围的值
hincrby key field increment
hincrbyfloat key field increment
//存在不设置 不存在就设置
hsetnx key field value
```

#### hash 类型数据操作的注意事项

* hash类型下的value只能存储字符串，不允许存储其他数据类型，不存在嵌套现象，如果数据未获取到，对应值为(nil)
* 每个hash可以存储2^32 -1 
* hash类型是非贴近对象的数据存储格式，并且可以灵活添加删除对象属性。但hash设计初衷不是为了存储大量对象而设计的，切记不可滥用，更不可以将hash对象列表使用
* hgetall操作可以获取全部属性，但是内部field过多，遍历整体数据效率就会很低，有可能成为数据访问的瓶颈

#### string存对象和hash存对象

* string整存整取
* hash比较分离

### list

```sh
# 添加修改数据
lpush key value1 [value2].....
rpush key value1 [value2].....

# 获取数据
lrange key start stop
lindex key index
llen key

# 获取并移除数据
lpop key
rpop key

# 规定时间内获取并移除数据 Redis Blpop 命令移出并获取列表的第一个元素， 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。
# 单位秒
blpop key1 [key2] timeout
brpop key1 [key2] timeout

# 移除指定数据
# 根据参数 count 的值，移除列表中与参数 value 相等的元素。

#count 的值可以是以下几种：
#count > 0 : 从表头开始向表尾搜索，移除与 value 相等的元素，数量为 count 。
# count < 0 : 从表尾开始向表头搜索，移除与 value 相等的元素，数量为 count 的绝对值。
# count = 0 : 移除表中所有与 value 相等的值。
lrem key count value
```

### set

* 新的存储需求：存储大量数据，在查询方面提供更高的效率。
* 需要的存储结构：能够保存大量数据，高效的内部存储机制，便于查询

```sh
# 添加数据
sadd key member1 [member2]
# 获取全部数据
smembers key
# 删除数据
srem key member1 [member2]
# 获取数据总量
scard key
# 判断集合中是否包含指定数据
sismember key member

# 求2两个集合的交、并、差集
sinter key1 [key2]
sunion key1 [key2]
sdiff key1 [key2]

# 求两个集合的交、并、差集并存储到指定集合中
sinterstore destination key1 [key2]
sunionstore destination key1 [key2]
sdiffstore destionation key1 [key2]

# 将指定数据从原始集合异动到目标集合
smove source destination member
```

#### sorted_set

* 新的存储需求：数据排序有利于数据的有效展示，需要提供一种可以根据自身特征进行排序的方式
* 需要的结存结构：新的存储模型，可以保存可排序的数据
* sorted_set类型：在set的存储结构基础上添加可排序的字段

```sh
# 添加数据
zadd key score1 member1 [score2 member2]
# 获取全部数据
zrange key start stop [withscores]
# 倒序
zrevrange key start stop [withscores]
# 删除数据
zrem key member [member...]
# 按条件获取数据
zrangebyscore key min max [withscores] [limit offset count]
zrevrangebyscore key min max [withscores] [limit offset count]
# 条件删除数据 从rank 0开始
zremrangebyrank key start stop
zremrangebyscore key min max

# 获取集合数据量
zcard key
# 过滤score
zcount key min max
# 集合交并
zinterstore destination numkeys key [key ..]
zunionstore destination numkeys key [key ..]

# 获取数据对应的索引（排名）
zrank key member
zrevrank key member

# score值的获取与修改
zscore key member
zincrby key increment member
```

#### 注意事项

* score保存的数据存储空间是64位，如果是整数范围是-9007199254740992~9007199254740992
* score保存的数据也可以是一个双精度double值，基于双精度浮点数的特征，可能会丢失经度，使用时要慎重
* sorted_set底层存储还是基于set结构的，因此数据不能重复，如果重复添加相同数据，score将被覆盖，保存最后一次修改的结果

```sh
# 当前系统时间
time
```

## 通用命令

```sh
# 删除key
del key
# 获取key是否存在
exists key
# 获取key类型
# 为指定key设置有效期
expire key seconds
pexpire key milliseconds
expireat key timestamp
pexpireat key milliseconds-timestamp
# 获取key的有效时间
ttl key
pttl key
# 切换key从时效性转换为永久性
persist key
# 查询key
keys parttern
```

查询模式规则

\* 匹配任意数量的字符      ？ 匹配任意一个字符    []匹配一个自定字符

keys * 查询所有

keys it* 查询所有以it开头

keys *heima 查询所有以heima结尾

keys ??heima 查询所有前面两个字符任意，后面以黑马结尾

keys user:? 查询任何以user:开头，最后一个字符结尾

keys u[st]er:1 查询所有以u开头，以er:1结尾，中间包含一个字符（s或t)

```sh
# 改名
rename key newkey  #会覆盖
renamenx key newkey #不会覆盖
# 对所有key排序
SORT key [BY pattern] [LIMIT offset count] [GET pattern [GET pattern ...]] [ASC|DESC] [ALPHA] [STORE destination]
  #summary: Sort the elements in a list, set or sorted set
  #since: 1.0.0
  #group: generic
# 其他key通用操作
help @generic
# 切换数据库
select index
# 其他操作
quit
ping
echo message
# 数据异动
move key db
# 数据清除
dbsize
flushdb
flushall
```

## Linux安装

```sh
wget http://download.redis.io/releases/redis-5.0.8.tar.gz
tar zxvf redis-5.0.8.tar.gz
make
make install
redis-server redis-6379.conf
# 查看进程
ps -ef | grep redis-
# 杀死进程
kill -s 9 pid
```

## 持久化

### RDB

```sh
save
```

**作用**

​	手动执行一次保存，会在设定的数据文件目录保存一个rdb数据

#### 配置

**dbfilename dump.rdb**

​	说明：设置本地数据库文件名，默认值为dump.rdb

​	经验：通常设置为dump-端口号.rdb

**dir**

​	说明：设置存储.rdb的路径

​	经验：通常设置成存储空间较大的目录中，目录名称data

**rdbcompression yes**

​	说明：设置存储至本地数据库时是否压缩数据，默认为yes，采用LZF压缩

​	经验：通常默认为开启状态，如果设置为no，可以节省CPU运行时间，但会使存储的文件变大（巨大）

**rdbchecksum yes**

​	说明：设置是否进行RDB文件格式校验，该校验过程在写文件和读文件过程中进行。

​	经验：通常默认为开启状态，如果设置为no，可以节约读写性过程约10%时间消耗，但存储一定的数据损坏风险。

#### 恢复

​	启动时恢复

#### save指令工作原理

​	save执行时间很长怎么办？

​	注意：save指令的执行会阻塞当前redis服务器，直到当前RDB过程完成为止，有可能会造成长时间阻塞，线上环境不建议使用。

#### 后台保存-bgsave

```
bgsave
```

作用

​	手动启动后台保存操作，但不是立即执行

**bgsave工作原理**

![](https://pic.imgdb.cn/item/5ea2fd18c2a9a83be5da6152.jpg)

注意：bgsave命令是针对save阻塞问题做的优化。Redis内部所有涉及到RDB操作都采用bgsave的方式。save命令可以放弃使用。

**bgsave指令相关配置**

**stop-writes-on-bgsave-error yes**

​	说明：后台存储过程中如果出现错误现象，是否停止保存操作

​	经验：通常默认为开启状态。

#### 自动执行save

redis服务器基于条件执行，满足条件就保存。

**配置**

```
save second changes
```

**作用**

​	满足限定时间范围key的变化数量达到制定数量即进行持久化

**参数**

​	seond：监控时间范围

​	changes: 监控key的变化量

**位置**

​	在conf文件中进行配置

**范例**

​	save 900 1

​	save 300 10

​	save 60 10000

注意：

​	save配置要根据实际业务情况进行配置，频度过高或者过低都会出现性能问题，结果可能是灾难性的。

​	save配置中对second和changes设置通常具有互补对应关系，尽量不要设置包含性关系

​	save配置启动后执行的是bgsave



#### 区别

![](https://pic.imgdb.cn/item/5ea30232c2a9a83be5de7ba8.jpg)

#### rdb特殊启动形式

* 全量复制
* 服务器运行过程中重启

```
debug reload
```

* 关闭服务器时制定保存数据

```
shutdown save
```

**rdb优点**

* RDB是一个紧凑压缩的二进制文件，存储效率高
* RDB内部存储的是redis在某个时间点的快照，非常适合于数据备份，全量复制等场景。
* RDB恢复数据的速度要比AOF快很多
* 应用：服务器中每X小时执行bgsave备份，并将RDB文件拷贝到远程服务机器中，用于灾难恢复。

**rdb缺点**

* RDB方式无论是执行指令还是利用配置，无法做到实时持久化，具有较大的可能性丢失数据。
* bgsave指令每次运行要执行fork操作创建子进程，要牺牲掉一些性能。
* Redis的众多版本中未进行RDB文件格式的版本统一，有可能出现各版本服务之间数据格式无法兼容现象。

### AOF

* 不写全数据，仅记录部分数据

* 改记录数据为记录操作过程
* 对所有操作均进行记录，排除丢失数据的风险

#### 概念

AOF(append only file)持久化：以独立日志的方式记录每次写命令，重启时再重新执行AOF文件中命令，达到恢复数据的目的。与RDB相比可以简单描述为改记录数据为记录数据产生的过程。：

AOF的主要作用是解决了数据持久化的实时性，目前已经是Redis持久化的主流方式。

**写数据过程**

![](https://pic.imgdb.cn/item/5ec62803c2a9a83be57d5c0a.jpg)



**写数据策略**

* always（每次）

  ​	每次写入操作均同步到AOF文件中，数据零误差，性能较低

* everysec（每秒）

  ​	每秒将缓冲区中的指令同步到AOF文件中，数据准确性较高，性能较高

  ​	在系统突然宕机的情况下丢失1秒的数据

* no（系统控制）

  ​	由操作系统控制每次同步到AOF文件的周期，整体过程不可控

#### AOF功能开启

* 配置

```
appendonly yes|no
```

* 作用

  是否开启AOF持久化功能，默认为不开启

* 配置

```
appendfsync always|everysec|no
```

* 作用

  AOF写数据策略

* 配置

```
appendfilename filename
```

* 作用

  AOF之持久化文件名，默认文件名为appendonly.aof, 建议配置为appendonly-端口号.aof

* 配置

```
appendfsync always|everysec|no
```

* 作用

  AOF写数据策略

#### AOF重写

随着命令不断写入AOF，文件会越来越大，为了解决这个问题，Redis引入了AOF重写机制压缩文件体积。AOF文件重写是将Redis进程的数据转化为写命令同步到新AOF文件的过程。简单说就是将同一个数据的若干个命令执行结果转化成最终结果数据对应的指令进行记录。

#### AOF重写规则

* 进程内已超时的数据不再写入文件

* 忽略无效指令，重写时使用进程数据直接生成，这样新的AOF文件只保留最终数据写入命令

* 对同一条数据的多条写命令合并为一条命令

  为防止数据过大造成客户端缓冲区溢出，对list、set、hash、zset等数据，每条指令最多写入64个元素。

#### AOF重写方式

* 手动重写

  ```
  bgrewriteaof
  ```



![](https://pic.imgdb.cn/item/5ec634c0c2a9a83be58fd552.jpg)

* 自动重写

```
auto-aof-rewrite-min-size size
auto-aof-rewrite-percentage percentage
```

* 自动重写触发比对参数(运行指令info Persistence获取具体信息)

```
aof_current_size
aof_base_size
```

* 自动重写触发条件

```
aof_current_size > auto-aof-rewrite-min-size
(aof_current_size - aof_base_size) / aof_base_size >= auto-aof-rewrite-percentage
```

**自动重写工作原理自动重写**

![](https://pic.imgdb.cn/item/5ec636f6c2a9a83be592e697.jpg)

#### AOF VS RDB

![](https://pic.imgdb.cn/item/5ec6372ac2a9a83be59339bd.jpg)



![](https://pic.imgdb.cn/item/5ec6375fc2a9a83be5939011.jpg)

## 删除策略

数据特征

Redis是一种内存级数据库，所有数据均存放在内存中，内存中的数据可以通过TTL指令获取其状态

* XX： 具有时效性的数据
* -1：永久有效的数据
* -2：已经过期的数据 被删除的数据 或未定义的数据

已经过期的数据真的删除了吗？

### 数据删除策略

* 定时删除
* 惰性删除
* 定期删除

**时效性数据存储格式**

![](https://pic.imgdb.cn/item/5ec7b8a8c2a9a83be5e5766f.jpg)



**数据删除策略的目标**

在内存占用和CPU占用之间寻找一种平衡，顾此失彼都会造成redis性能的下降，甚至引发服务宕机或内存泄露。

### 定时删除

* 创建一个定时器，当key设置有过期时间，且过期时间到达时，由定时任务立即执行对键的删除。

* 优点：节约内存，到时就删除，快速释放不必要的内存占用。
* 缺点：CPU压力很大，无论CPU此时负载量多高，均占用CPU，会影响redis服务器响应时间和指令吞吐量。



* 总结：用处理器性能换存储空间。

### 惰性删除

* 、数据到达过期时间，不做处理。等下次访问该数据时删除。
* 优点：节约CPU性能，发现必须删除的时候才删除
* 缺点：内存压力很大，出现长期占用内存的数据。

### 定期删除

* Redis启动服务器初始化时，读取配置server.hz的值，默认为10
* 每秒钟执行server.hz次serverCron()
  - databasesCron()
    - activeExpireCycle()
* activeExpireCycle()对每个expires[*]逐一进行检测，每次执行250ms/server.hz
* 对某个expires[*]检测时，随机挑选W个key检测
  * key超时删除
  * 如果一轮中删除的key数量> w * 25%,循环该过程
  * 否则检查下一个expires[*],0-15循环。
  * W取值=ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP属性值
* 参数current_db用于记录activeExpireCycle()进入哪个expires[*]执行



* 周期性轮询redis库中时效性数据，采用随机抽取的策略，利用过期数据占比的方式控制删除频度
* 特点1：CPU性能占用设置有峰值，检测频度可自定义设置。
* 特点2：内存压力不是很大，长期占用的冷数据会被持续处理。
* 总结：周期性抽查存储空间（随机抽查，重点抽查）

**dynamic-hz yes**

​	动态hz

### 逐出算法

新数据进入redis，内存不足怎么办？

* Redis使用内存存储数据，在执行每一个命令前，会调用freeMemoryIfNeeded()检测内存是否充足。如果内存不满足新加入数据最低存储要求，redis要求临时删除一些数据为当前指令清理存储空间。清理数据的策略称为逐出算法。
* 注意：逐出策略的过程不是100%能够清理出足够的可用空间，如果不成功则反复执行。当对所有数据尝试完毕后，如果不能达到内存清理要求，将出现错误信息。

* 配置最大可用内存

```
maxmemory
```

占用物理内存的比例，默认值为0，表示不限制。生产环境中根据需求设定，通常在50%。

* 配置每次选取待删除的数据个数

```
maxmeory-samples
```

选取数据时并不会全库扫描，导致严重的性能消耗，降低读写性能，因此采用随机获取数据的方式作为待检测删除数据。

* 删除策略

```
maxmemory-policy
```

达到最大内存后，对被挑选出来的数据进行删除的策略。

* 检测易失数据（可能会过期的数据集server.db[i].expires)
  1. volatile-lru: 挑选最长时间没有使用的数据淘汰
  2. volatile-lfu: 挑选最近使用次数最少的数据进行淘汰
  3. volatile-ttl: 挑选刚要过期的数据淘汰
  4. volatile-random: 任意选择数据淘汰
* 检测全库数据（所有数据集server.db[i].dict)
  5. allkeys-lru: 挑选最长时间没有使用过的数据淘汰
  6. allkeys-lfu: 挑选最近使用次数最少的数据进行淘汰
  7. allkeys-random: 任意选择数据淘汰
* 放弃数据驱逐
  8. no-enviction(驱逐): 禁止驱逐(redis4.0中默认策略)，会引发OOM



可以使用info命令输出监控信息，查询缓存hit和miss次数，根据业务需求调优redis配置。



### 服务器配置

```
# 设置服务器以守护进程的方式运行
daemonize yes

# 绑定主机地址
bind 127.0.0.1

# 设置端口号
port 6379

# 设置数据库数量
databases 16

# 设置服务器以指定日志记录级别
loglevel debug|verbose|notice|warning

# 日志记录文件名 注意日志级别开发库设置为verbose，生产库设置为notice，简化日志输出量，降低写日志的IO频度
logfile 端口号.log

# 设置同一时间最大客户端连接数，默认无限制。当客户端连接达到上限，Redis会关闭新的连接
maxclients 0

# 客户端闲置等待最大时长，达到最大值后关闭连接，如需关闭该功能，设置为0
timeout 300

#导入并加载指定配置文件信息，用于快速创建redis公共配置较多的redis实例配置文件，便于维护
include /path/server-端口号.conf
```

## 高级数据结构

### BitMaps

问题：年度浏览量最低，月度浏览量最低，周浏览量最低

存储需求：

非黑即白的存储

其实就是string的二进制操作api

```sh
#获取指定key对应偏移量上的bit值
getbit key offset

# 获取指定key对应偏移量上的bit值,value只能是1,或者0
setbit key offset value

# 对指定key按位进行交、并、非、异或操作，并将结果保存到destKey中
bittop op destKey key1 [key2 ....]
	and, or, not, xor
# 统计key中1的数量
bitcount key [start end]
```

### HyperLogLog

统计独立UV

* 原始方案：set

  ​	* 存储每个用户的id

* 改进方案：BitMaps

  * 存储每个用户的状态bit

* 全新方案HyperLogLog

**基数**

* 基数是数据集去重后元素个数
* HyperLogLog是用来做基数统计的，运用了LogLog算法

LogLog算法

```
# 添加数据
pfadd key element [element2...]
# 统计数据
pfcount key [key...]
# 合并数据
pfmerge destkety sourcekey [sourcekey]
```

相关说明

* 用于进行基数统计，不是集合，不保存数据，只记录数量而不是具体数据
* 核心是基数估算算法，最终数值存在一定误差
* 误差范围：基数估计的结果是一个带有0.81%标准错误的近似值
* 耗空间极小，每个hyperloglog key占用12k的内存用于标记基数
* pdadd命令不是一次性分配12k内存使用，会随着基数的增加内存逐渐增大
* pfmerge命令合并后占用的存储空间为12K，无论合并之前的数据量多少

### GEO

附近的人

就是位置关系

```
# 添加坐标点
geoadd key longitude latitude member [longitude latitude member...]
# 获取坐标
geopos key member [member ...]
# 计算坐标点距离
geodist key member1 member2 [unit]
# 根据坐标求范围内的数据
georadius key longitude latitude radius m|km|ft|mi [withcoord][withdist][withhash][count count]
# 根据点求范围内数据
georadiusbymember key member radius m|km|ft|mi [withcoord][withdist][withhash][count count]
# 获取指定点对应坐标的hash值
geohash key member [member...]
```

## 主从复制

### 主从复制

**互联网三高架构**

* 高并发
* 高性能
* 高可用（追求99.999%的高可用)



单机redis的风险与问题

* 问题1：机器故障

  * 现象：硬盘故障、系统崩溃
  * 本质：数据丢失，很可能对业务造成灾难性打击
  * 结论：基本上会放弃使用redis

* 问题2：容量瓶颈

  * 现象：内存不足，从16G升级到64G，从64G升级到128G，无限升级内存
  * 本质：穷，硬件条件跟不上
  * 结论：放弃使用redis   

* 结论

  ​	为了避免单点Redis服务器故障，准备多台服务器，相互连通。将数据复制多个副本保存在不同的服务器上，连接在一起，并保证数据是同步的。即使其中有一台服务器宕机，其他服务器依然可以继续提供服务，实现Redis高可用，同时实现数据冗余备份。



### 主从复制简介

![](https://pic.imgdb.cn/item/5ed3a554c2a9a83be5796588.jpg)

主从复制即将master中数据即时、有效地复制到slave中。

特征：一个master可以拥有多个slave，一个slave只对应一个master

职责：

* master
  * 写数据
  * 执行写操作时，将出现变化的数据自动同步到slave
  * 读数据（可忽略）
* slave
  * 读数据
  * 写数据（禁止）

### 主从复制作用

* 读写分离：master写，slave读，提高服务器的读写负载能力
* 负载均衡：基于主从结构，配合读写分离，由slave分担master负载，并根据需求的变化，改变slave的数量，通过多个从节点分担数据读取负载，大大提高Redis服务器并发量与数据吞吐量。
* 故障恢复：当master出现问题时，由slave提供服务，实现快速的故障恢复。
* 数据冗余：实现数据热备份，是持久化之外的一种数据冗余方式。
* 高可用基石：基于主从复制，构建哨兵模式与集群，实现Redis的高可用方案。



### 主从复制工作流程

3个阶段：

* 建立连接阶段
* 数据同步阶段
* 命令传播阶段

![](https://pic.imgdb.cn/item/5ed3a9d6c2a9a83be582e5f0.jpg)



### 阶段1：建立连接阶段

* 建立slave到master的连接，使master能够识别slave，并保存slave端口号

  1. slave给master发送指令：slaveof ip port

  2. master接收命令，响应对方

  3. slave保存master的IP与端口

     masterhost

     masterport

  4. slave根据保存的信息创建连接master的socket

  5. salve周期性发送命令：ping

  6. master响应pong

  7. slave发送指令：auth password

  8. master授权验证

  9. slave发送命令： replconf listening-port <port-number>

  10. master保存slave的端口号

  ![](https://pic.imgdb.cn/item/5ed3ab4cc2a9a83be585fd38.jpg)



### 搭建主从结构

* 方式1：客户端发送命令

  ```
  slaveof <masterip> <masterport>
  ```

* 方式2：启动服务器参数

  ```
  redis-server --slaveof <masterip> <masterport>
  ```

* 方式3： 服务器配置

  ```
  slaveof <masterip> <masterport>
  ```

  

* slave信息

  * master_link_down_since_seconds
  * masterhost
  * masterport

* master信息

  * slave_listening_port（多个）



断开连接

```
slaveof no one
```



#### 授权访问

```
#master设置密码
requirepass <password>

#master客户端发送命令设置密码
config set requirepass <password>
config get requirepass

#slave客户端发送命令设置密码
auth <password>

#客户端配置文件设置密码
masterauth <password>

#启动客户端设置密码
redis-cli -a <password>
```



### 阶段2： 数据同步阶段

* 在slave初连接master后，复制master中所有数据到slave
* 将slave的数据库状态更新成master当前的数据库状态

#### 工作流程

![](https://pic.imgdb.cn/item/5eda6329c2a9a83be5c76ef8.jpg)

#### 数据同步阶段master说明

1. 如果master数据量巨大，数据同步阶段应避免流量高峰期，避免造成master阻塞，影响业务正常执行

2. 复制缓冲区大小设置不合理，会导致数据溢出，如果全量复制周期太长，进行部分复制时发现数据已经存在丢失的情况，必须进行第二次全量复制，致使slave陷入死循环状态

```sh
repl-backlog-size 1mb
```

3. master单机内存占用主机内存的比例不应过大，建议使用50%-70%的内存，留下30%-50%内存用于执行bgsave命令和创建缓冲区。

#### 数据同步阶段slave说明

1. 为避免slave进行全量复制、部分复制时服务器响应阻塞或数据不同步，建议关闭此期间对外服务

```sh
slave-server-stale-data yes|no
```

2. 数据同步阶段，master发送slave信息可以理解master是slave的一个客户端，主动向slave发送命令
3. 多个slave同时对master请求数据同步，master发送的RDB文件增多，会对带宽造成巨大冲击，如果master带宽不足，那么数据同步需要根据业务需求，适量错峰。
4. slave过多时，建议调整为拓扑结构，从一主多从变为树状结构，中间节点既是master，也是slave，注意使用树状结构时，由于层级深度，导致深度越高的slave与最顶层master间数据同步延迟较大，数据一致性变差，应谨慎选择。

### 阶段3：命令传播阶段

* 当master数据库状态被修改后，导致主从服务器库状态不一致，此时需要让主从同步到一致的状态，同步的动作称为命令传播。
* master将接收到的数据变更命令发给slave，slave接收命令后执行命令。

#### 命令传播阶段的部分复制

* 命令传播阶段出现了断网现象

  * 网络闪断闪连  忽略
  * 长时间网络中断   全量复制
  * 短时间网络中断 部分复制
* 部分复制的三个核心要素
  * 服务器的运行id(runid)
  * 主服务器的复制积压缓冲区
  * 主从服务器的复制偏移量

#### 服务器运行ID(runid)

* 概念：服务器运行ID是每一台服务器每次运行的身份识别码，一台服务器多次运行可以生成多个运行id。

* 组成：运行id由40位字符组成，是一个随机的十六进制字符。

* 作用：运行id被用于在服务器间进行传输，识别身份。

  ​	如果想两次操作均对同一台服务器进行，必须每次操作携带对应的运行id，用于对方识别。

* 实现方式：运行id在每台服务器启动时自动生成的，master在首次连接slave时，会将自己的运行ID发送给slave，slave保存此ID，通过info server命令，可以查看节点runid。

### 复制缓冲区

* 概念：复制缓冲区，又名复制积压缓冲区，是一个先进先出的(FIFO)的队列，用于存储服务器执行过的命令，每次传播命令，master都会将传播的命令记录下来，并存储在复制缓冲区。
* 复制缓冲区默认数据存储空间大小是1M，由于存储空间大小是固定的，当入队的数量大于队列长度时，最先入队的元素会被弹出，而新元素会被放入队列。
* 由来：每台服务器启动时，如果开启AOF或被连接成为master节点，即创建复制缓冲区。
* 作用：用于保存master收到的所有指令（仅影响数据变更的指令，例如set，select)
* 数据来源：当master接收到主客户端指令时，除了将指令执行，会将该指令存储到缓冲区中。

![image-20200606101537760](C:\Users\81929\AppData\Roaming\Typora\typora-user-images\image-20200606101537760.png)



#### 复制缓冲区内部工作原理

* 组成
  * 偏移量
  * 字节值
* 工作原理
  * 通过offset区分不同slave当前数据传播的差异。
  * slave和master都记录offset

![](https://pic.imgdb.cn/item/5edafd63c2a9a83be5b335c5.jpg)



​	

#### 主从服务器复制偏移量（offset）

* 概念：一个数字，描述复制缓冲区中的指令字节位置
* 分类
  * master复制偏移量：记录发送给所有slave的指令字节对应的偏移量（多个）
  * slave复制偏移量：记录slave接收master发送过来的指令字节对应的位置（一个）
* 数据来源
  * master端：发送一次记录一次
  * slave端：接收节次记录一次
* 作用：同步信息，比对master与slave的差异，当slave断线后，恢复数据使用

### 数据同步+命令传播阶段工作流程

![image-20200606103802566](C:\Users\81929\AppData\Roaming\Typora\typora-user-images\image-20200606103802566.png)

### 心跳机制

* 进入命令传播阶段，master与slave间需要进行信息交换，使用心跳机制进行维护，实现双方连接保持在线。
* master心跳：
  * 指令：PING
  * 周期：由repl-ping-slave-period决定，默认10秒
  * 作用：判断slave是否在线
  * 查询：INFO replication             获取slave最后一次连接时间间隔，lag项维持在0或1为正常
* slave心跳任务：
  * 指令：REPLCONF ACK {offset}
  * 周期：1秒
  * 作用1：汇报slave自己的复制偏移量，获取最新的数据变更指令
  * 作用2：判断master是否在线

#### 心跳阶段的注意事项

* 当slave多数掉线或延迟过高时，master为保障数据稳定性，将拒绝所有信息同步操作

```sh
min-slaves-to-write 2
min-slaves-max-lag 8
```

slave数量少于2个，或者所有slave的延迟都大于等于8秒时，强制关闭master写功能，停止数据同步。

* slave数量由slave发送 REPLCONF ACK命令做确认
* slave延迟由slave发送REPLCONF ACK命令做确认

![](https://pic.imgdb.cn/item/5edb052fc2a9a83be5c34457.jpg)

### 主从复制常见问题

#### 频繁的全量复制（1）

伴随系统的进行，master的数据量会越来越大，一旦master重启，runid会发生变化，会导致全部slave的全量复制操作。



内部优化方案：

1. master内部创建master_replid变量，使用runid相同的策略生成，长度41位，并发送给所有slave

2. 在master关闭时指令指令，shutdown save，进行RDB持久化，将runid与offset保存到RDB文件中

* repl-id, repl-offset
* 通过redis-check_rdb命令可以查看该信息

3. master重启后加载RDB文件，恢复数据
4. 重启后，将RDB文件中保存repl-id与repl-offeset加载到内存中。
   * master_repl_id = repl      master_repl_offset = repl-offset
   * 通过info命令可以查看该信息

作用：

​	本机保存上次runid，重启后恢复该值，使所有slave认为还是之前的master



#### 频繁的全量复制（2）

* 问题现象
  * 网络环境不佳，出现网络中断，slave不提供服务
* 问题原因
  * 复制缓冲区过小，断网后slave的offset越界，触发全量复制
* 最终结果
  * slave反复进行全量复制
* 解决方案
  * 修改复制缓冲区大小

```sh
repl-backlog-size
```

* 建议设置如下：
  1. 测算从master到slave重连平均时长second
  2. 获取master每秒产生的写数据总量write_size_per_second
  3. 最优复制缓冲区空间 = 2 * second * write_size_per_second

#### 频繁的网络中断（1）

* 问题现象
  * master的CPU占用过高或 slave频繁断开连接
* 问题原因
  * slave每1秒发送REPLCONF ACK命令到master
  * 当slave接到慢查询(keys *, hgetall等)，会大量占用CPU性能
  * master每1秒调用复制定时函数replicationCron()，比对slave发现长时间没有进行响应。
* 最终结果
  * master各种资源（输出缓冲区、带宽、连接等）被严重占用
* 解决方案
  * 通过设置合理的超时时间，确认是否释放slave(默认60秒)

```
repl-timeout
```

该参数定义了超时时间的阈值（默认60）,超过该值，释放slave

#### 频繁的网络中断（2）

* 问题现象
  * slave与master连接断开
* 问题原因
  * master发送ping命令频度低
  * master设定超时时间较短
  * ping指令在网络中存在丢包
* 解决方案
  * 提高ping指令发送的频度

```
repl-ping-slave-period
```

超时时间repl-time的时间至少是ping指令频度的5-10倍，否则slave很容易被判定超时。

#### 数据不一致

* 问题现象

  * 多个slave获取相同数据不同步

* 问题原因

  * 网络信息不同步，数据发送有延迟

* 解决方案

  * 优化主从间网络环境，通常放置在同一个机房部署，如使用阿里云等云服务器要注意此现象

  * 监控主从节点延迟（通过offset）判断，如果slave延迟过大，暂时屏蔽程序对该slave的数据访问

    ```
    slave-serve-stale-data yes|no
    ```

    开启后仅响应info，slaveof等少数命令（慎用，除非对数据一致性要求很高。



## 哨兵

主机宕机

![](https://pic.imgdb.cn/item/5edb1cd6c2a9a83be5f46c3d.jpg)

* 将宕机的master下线
* 找一个slave作为master
* 通识所有的slave连接新的master
* 启动新的master和slave
* 全量复制xN+ 部分复制xN
* 谁来确认master宕机了？
* 找一个master？怎么找？
* 修改配置后，原始的master恢复了怎么办？

#### 哨兵简介

哨兵（sentinel）是一个分布式系统，用于对主从结构的没台服务器进行监控，当出现故障通过投票机制选择新的master并将所有slave连接到新的master。

![](https://pic.imgdb.cn/item/5edb1e1cc2a9a83be5f7dc43.jpg)

#### 哨兵的作用

* 监控

  ​	不断地检查master和slave是否正常进行

  ​	master存活检测、master与slave运行情况检测

* 通知（提醒）

  ​	当被监控的服务器出现问题时，向其他（哨兵间，客户端）发送通知

* 自动故障转移

  ​	断开master与slave连接，选取一个slave作为master，将其他slave连接到新的master，并告知客户端新的服务器地址。

注意：

​	哨兵也是一台redis服务器，只是不提供数据服务

​	通常哨兵配置数量为奇数（竞选活动可能打平）

#### 配置哨兵

* 配置一拖二的主从结构

* 配置三个哨兵（配置相同，接口不同）

  ​	参看sentinel.conf

* 启动哨兵

```
redis-sentinel setinel-端口号.conf
```



sentinel.conf

输入cat sentinel.conf | grep -v '#' | grep -v "^$"

```
daemonize no
pidfile /var/run/redis-sentinel.pid
logfile ""
dir /tmp
sentinel monitor mymaster 127.0.0.1 6379 2  # 最后一个参数是多少个哨兵认为这个master挂了，那么就挂了 一般设置为哨兵的一半+1
sentinel down-after-milliseconds mymaster 30000
sentinel parallel-syncs mymaster 1  # master挂了 一次有多少台数据复制
sentinel failover-timeout mymaster 180000  # 超时
sentinel deny-scripts-reconfig yes
```

复制一份

```sh
sed 's/26379/26381/g' sentinel-26379.conf > sentinel-26381.conf
```

启动后配置文件会发生变化

哨兵之间可以相互感知

关掉master后，哨兵投票推举一个slave出来当master

![](https://pic.imgdb.cn/item/5edb2610c2a9a83be50e7c48.jpg)



### 哨兵工作原理

#### 主从切换

* 哨兵在进行主从切换过程中经历了三个阶段
  * 监控
  * 通知
  * 故障转移

#### 阶段1：监控阶段

* 用于同步各个节点的状态信息

  * 获取各个sentinel的状态（是否在线）

  * 获取master的状态

    * master属性
      * runid
      * role：master
    * 各个slave的详细信息

  * 获取所有slave的状态（根据master中的slave信息）

    * slave属性
      * runid
      * role：slave
      * master_host，master_port
      * offset
      * .......

    ![](https://pic.imgdb.cn/item/5edb2763c2a9a83be5129274.jpg)



![](https://pic.imgdb.cn/item/5edb27ccc2a9a83be513c7f4.jpg)



#### 阶段2：通知阶段

一个哨兵获取到master的信息会传递给其他哨兵

![image-20200606132238835](C:\Users\81929\AppData\Roaming\Typora\typora-user-images\image-20200606132238835.png)



#### 阶段3：故障转移阶段

发现故障

一个哨兵发现故障，

![](https://pic.imgdb.cn/item/5edb28adc2a9a83be5165723.jpg)

选举哨兵

![image-20200606132715357](C:\Users\81929\AppData\Roaming\Typora\typora-user-images\image-20200606132715357.png)

* 服务器列表中挑选备选master

  * 在线的
  * 响应慢的pass掉
  * 与原来的master断开时间久的
  * 优先原则
    * 优先级
    * offset
    * runid（runid比较小的）
  * 发送指令（sentinel）
    * 向新的master发送 slaveof no one
    * 向其他slave发送salveof 新masterIP端口

  ![image-20200606132923564](C:\Users\81929\AppData\Roaming\Typora\typora-user-images\image-20200606132923564.png)



#### 总结

* 监控
  * 同步信息
* 通知
  * 保持连通
* 故障转移
  * 发现问题
  * 竞选负责人
  * 优选新master
  * 新master上任，其他slave切换master，原master作为slave故障回复后连接

![](https://pic.imgdb.cn/item/5edb2b42c2a9a83be51e15c4.jpg)



## 集群

**业务发展过程中遇到的峰值瓶颈**

* redis提供的服务OPS可以达到10万/秒，当前OPS已经达到20万/秒
* 内存单击容量达到256G,当前业务需求内存容量1T
* 使用集群的方式可以快速解决上述问题



#### 集群架构

* 集群就是使用网络将若干台计算机联通起来，并提供统一的管理方式，使其对外呈现单机的服务效果。

#### 集群作用

* 分散单台服务器的访问压力，实现负载均衡
* 分散单台服务器的存储压力，实现可扩展性
* 降级单台服务器宕机带来的业务灾难

![](https://pic.imgdb.cn/item/5edb2d6ec2a9a83be5246230.jpg)



#### 集群结构设计

**数据结构设计**

* 通过算法设计，计算出key应该保存的位置
* 将所有的存储空间计划切割成16384份，每台主机保存一部分
  * 每份代表一个存储空间，不是一个key的保存空间
* 按key按照计算出的结果放到对应的存储空间

![](https://pic.imgdb.cn/item/5edb2e7fc2a9a83be5275694.jpg)



* 增强可扩展性

![](https://pic.imgdb.cn/item/5edb2e5cc2a9a83be526f4b3.jpg)

#### 集群内部通讯设计

* 各个数据库相互通信，保存各个库中槽的编号数据
* 一次命中，直接返回
* 一次未命中，告知具体位置

![](https://pic.imgdb.cn/item/5edb2f06c2a9a83be528bb53.jpg)



#### cluster集群搭建

cluster配置redis-conf

```sh
cluster-enabled yes
cluster-config-file nodes-6379.conf
cluster-node-timeout  10000 #企业用的时候时间会比较长
```

然后复制出来很多个

启动三主三从

要基于ruby和rubygem

```sh
./redis-trib.rb create --replicas 1 127.0.0.1:6379 # replicas后一个master连n个slave的标记
```

![](https://pic.imgdb.cn/item/5edb3136c2a9a83be52ece6e.jpg)

![](https://pic.imgdb.cn/item/5edb3177c2a9a83be52f80d6.jpg)



#### cluster设置与获取数据

![](https://pic.imgdb.cn/item/5edb3217c2a9a83be53144c1.jpg)

![](https://pic.imgdb.cn/item/5edb3233c2a9a83be5319da5.jpg)



#### 主从下线与主从切换

从离线直接离线，

主离线标记离线，从slave推举一个主出来。

如果原主又上线，只能变成slave



#### cluster配置

```sh
#设置加入cluster，成为其中的节点
cluster-enabled yes
#cluster配置文件名，该文件属于自动生成，仅用于快速查找文件并查找文件内容
cluster-config-file nodes-6379.conf
# 节点服务响应超时时间，用于判定该节点是否下线或切换为从节点
cluster-node-timeout  10000 #企业用的时候时间会比较长
# master连接slave最小数量
cluster-migration-barrier <count>
```

#### cluster节点操作命令

```sh
# 查看集群节点信息
cluster nodes
# 进入一个从节点redis，切换其主节点
cluster replicate <master-id>
# 发现一个新节点，新增主节点
cluster meet ip:port
# 忽略一个没有slot的节点
cluster forget <id>
# 手动故障转移
cluster failover
```



## 缓存预热


"宕机"

1. 请求数据较高
2. 主从之间吞吐量较大，数据同步操作频度高



**解决方案**

前置准备工作：

1.日常例行统计数据访问记录，统计访问频度比较高的热点数据

2.利用LRU数据删除策略，构建数据留存队列

​		例如:storm和kafka配合

准备工作:

3.将统计结果中的数据分类，根据级别，redis优先加载级别较高的热点数据

4.利用分布式多服务器同时进行数据读取，提高数据加载过程

实施：

1. 使用脚本程序固定触发缓存预热过程
2. 如果条件允许，使用CDN，效果会更好

总结：

缓存预热就是系统启动前，提前将相关的缓存数据直接加载到缓存系统。避免在用户请求的时候，先查询数据库，然后在将数据缓存的问题。用户直接查询的就是缓存的数据。

## 缓存雪崩

数据库服务器崩溃(1)

1. 忽然数据库连接量激增
2. 应用服务器无法及时处理请求
3. 大量408，500错误页面出现
4. 客户反复刷新页面获取数据
5. 数据库崩溃
6. 应用服务器崩溃
7. 重启服务器无效
8. Redis服务器崩溃
9. Redis集群崩溃
10. 重启数据库后再次被瞬间流量放到

问题排查

1. 较短时间内，缓存中较多的key集中过期
2. 此周期请求访问过期的数据，redis未命中，redis向数据库获取数据
3. 数据库同时接收到大量的请求无法及时处理
4. Redis大量请求被积压，开始出现超时现象
5. 数据库流量激增，数据库崩溃
6. 重启后仍然面对缓存中无数据可用的问题
7. Redis服务器资源被严重占用，Redis数据库服务器崩溃
8. Redis集群呈现崩溃，集群瓦解
9. 应用服务器无法及时得到数据响应请求，来自客户端的请求数量越来越多，应用服务器崩溃
10. 应用服务器，redis，数据库全部重启，效果不理想。

解决方案（道）

1. 更多的页面静态化处理

2. 构建多级缓存架构

   Nginx缓存+redis缓存+ehcache缓存

3. 检测mysql严重耗时业务进行优化

   对数据库的瓶颈排查：例如超时查询、耗时较高的业务等

4. 灾难预警机制

   监控redis服务器指标性能

   * CPU占用、CPU使用率
   * 内存容量
   * 查询平均响应时间
   * 线程数

5. 限流、降级

   短时间内牺牲一些客户体验，限制一部分请求访问，降低应用服务器压力，待业务低速运转后，再逐步开放访问

解决方案（术）

1. LRU和LFU切换

2. 数据有效策略调整

   * 根据业务数据有效期进行分类错峰，A类90分钟，B类80分钟，C类70分钟
   * 过期时间使用固定时间+随机值的形式，稀释集中到期的key的数量

3. 超热数据使用永久key

4. 定期维护（自动+人工）

   ​	对即将过期的数据做访量分析，确认是否延时，配合访问量统计，做热点数据的延时

5. 加锁

   ​	慎用！

总结

缓存雪崩就是瞬间过期数据量太大，导致对数据库服务器造成压力。如能够有效避免，过期时间集中，可以有效解决雪崩现象出现(约40%)，配合其他策略一起使用，并监控数据库运行数据，根据运行记录进行快速调整。

## 缓存击穿

数据库崩溃2

1. 系统平稳运行中
2. 数据库连接量瞬间激增
3. Redis服务器无大量key过期
4. Redis内存平稳，无波动
5. Redis服务器CPU正常
6. 数据库崩溃

问题排查

1. Redis中某个key过期，该key访问量巨大
2. 多个数据请求从服务器直接到Redis中，均未命中
3. Redis在短时间内发起了大量对数据库中同一数据的访问

问题

* 单个key高热数据
* key过期

解决方案（术)

1. 预先设定

   以电商为例，每个商家根据店铺等级，指定若干款主打产品，在购物节期间，加大此类信息的key的过期时长。

   注意：购物节不仅仅指当天，以及后续若干天，访问峰值呈现逐渐降低的趋势

2. 现场调整

   监控访问量，对自然流量激增的数据延长过期时间或者设置为永久key

3. 后台刷新数据

   启动定时任务，高峰期来临之前，刷新数据有效期，确保不丢失

4. 二级缓存

   设置不同失效时间，保障不会被同时淘汰就行

5. 加锁

   分布式锁，防止被击穿，但是要注意也是性能瓶颈，慎重。

总结

缓存击穿就是单个高热数据过期的瞬间，数据访问量较大，未命中redis后，发起了大量对同一数据的数据访问，导致对数据库服务器造成压力。应对策略应该在业务数据分析与预防方面进行，配合运行监控测试与即时调整策略，毕竟单个key 的过期监控难度较高，配合雪崩处理策略即可。

## 缓存穿透

数据库崩溃3

1. 系统平稳运行中
2. 应用服务器流量随时间增量较大
3. Redis服务器命中率随时间逐步降低
4. Redis内存平稳，内存无压力
5. Redis服务器CPU占用激增
6. 数据库服务器压力激增
7. 数据库崩溃

问题排查

1.Redis大面积出现未命中

2.出现非正常URL的访问

问题分析

* 获取的数据在数据库中也不存在，数据库查询未得到对应数据
* Redis获取到null数据未进行持久化，直接返回
* 下次此类数据到达重复上述过程
* 出现黑客攻击服务器

解决方案（术）

1. 缓存null

   ​	对查询结果为null的数据进行缓存（长期使用，定期清理），设定短时限，例如30-60秒，最高5分钟

2. 白名单策略

   * 提前预热各种分类数据id对应的bitmaps，id作为bitmaps的offset，相当于设置了数据白名单。当加载正常数据时，放行，加载异常数据时直接拦截（效率偏低）
   * 使用布隆过滤器（有关布隆过滤器命中问题对当前状况可以忽略）

3. 实施监控

   实时监控redis命中率（业务正常范围时，通常会有一个波动值）与null数据的占比。

   * 非活动时段波动：通常检测3-5倍，超过5倍纳入重点排查对象
   * 活动时段波动：通常检测10-50倍，超过50倍纳入重点排查对象

   根据倍数不同，启动不同的排查流程，然后使用黑名单进行防控（运营）

4. key加密

   ​	问题出现后，临时启动防灾业务key，对key进行业务层传输加密服务，设定校验程序，过来的key校验

   ​	例如每天随机分配60个加密串，挑选2-3个，混淆在页面数据id中，发现key不满足规则，驳回数据访问。

总结：

缓存穿透访问了不存在的数据，跳过了合法数据的redis数据缓存阶段，每次访问数据库，导致对数据库服务器造成压力。通常此类数据出现量是一个较低的值，当出现此类情况以毒攻毒，并及时报警。应对策略应该在临时预案防范方面多做文章。



无论是黑名单还是白名单，都是对整体系统的压力，警报解除后尽快移除。

## 性能指标

* 性能指标

| Name                    | Description              |
| ----------------------- | ------------------------ |
| latency                 | Redis响应一个请求的时间  |
| instanceous_ops_per_sec | 平均每秒处理的请求总数   |
| hit rate(calculated)    | 缓存命中率（计算出来的） |

* 内存指标

| Name                    | Description                                    |
| ----------------------- | ---------------------------------------------- |
| used_memory             | 已使用内存                                     |
| mem_fragmentation_ratio | 内存碎片率                                     |
| evicted_keys            | 由于最大内存限制被移除的key的数量              |
| blocked_clients         | 由于BLPOP,BRPOP，or BRPOPLPUSH而被阻塞的客户端 |

* 基本活动指标：Basic activity

| Name                       | Description                |
| -------------------------- | -------------------------- |
| connected_clients          | 客户端连接数               |
| connected_slaves           | Slave数量                  |
| master_last_io_seconds_ago | 最近一次主从交互之后的秒数 |
| keyspace                   | 数据库中key值的总数        |

* 持久化指标

| Name                       | Description                        |
| -------------------------- | ---------------------------------- |
| rdb_last_save_time         | 最后一次持久化保存到磁盘的时间戳   |
| rdb_chages_since_last_save | 自最后一次持久化以来数据库的更改数 |

* 错误指标

| Name                           | Description                      |
| ------------------------------ | -------------------------------- |
| rejected_connections           | 由于达到maxClients限制而被拒绝   |
| keyspace_misses                | Key值查找失败的次数（没有命中）  |
| master_link_down_since_seconds | 主从断开的持续时间（以秒为单位） |

## 性能监控

建议开发一些

* Cloud Insight Redis
* Prometheus
* Redis-stat
* Redis-faina
* RedsiLive
* zabbix

* 命令
  * benchmark
  * redis cli
    * monitor
    * slowlog



* 命令

```
redis-benchmark [-h] [-p] [-c] [-n<requests>] [-k]
```

* 范例

```
redis-benchmark
```

​	说明：50个连接，10000次请求对应的性能

* 范例2

```
redis-benchmark -c 100 -n 5000
```

说明：100个连接，5000次请求对应的性能

### monitor

* 命令

```
monitor
```

打印服务器调试信息

### slowlog

* 命令

```
slowlog [operator]
```

* get:获取慢查询日志
* len:获取慢查询日志条目数
* reset:重置慢查询日志



* 相关配置

```
slowlog-log-slower-than 1000 # 设置慢查询日志下限，单位：微秒
slowlog-max-len 100 # 设置慢查询命令对应的日志显示长度，单位：命令数
```



## 待补充内容

* redis6新特性介绍
* redis lua脚本使用
* redission
* redis配置文件
* pipeline

https://redis.io/topics/config

# 《Java并发编程艺术》

## 第1章 并发编程的挑战

### 1.1 上下文切换

​	CPU会不断切换线程执行，每个时间片一般是几十毫秒。执行一个时间片后，会保存上一个任务的状态，然后切换到下一个任务。任务的保存到加载的过程就是一次上下文切换。

#### 1.1.1 多线程一定快么？

​	看例子ConcurrencyTest。

![](https://pic.imgdb.cn/item/5fa4c3c41cd1bbb86b92a423.jpg)

#### 1.1.2 测试上下文切换的次数和时长

* 使用Lmbench3来测量切换时长
* 用vmstat(linux)测量上下文切换的次数。

![](https://pic.imgdb.cn/item/5fa4c5891cd1bbb86b930150.jpg)

https://www.cnblogs.com/ftl1012/p/vmstat.html

Procs（进程）：

>  r: 运行队列中进程数量
>
>  b： 等待IO的进程数量

Memory（内存）：

>  swpd: 使用虚拟内存大小
>
>  free: 可用内存大小
>
>  buff: 用作缓冲的内存大小
>
>  cache: 用作缓存的内存大小

Swap：

 si: 每秒从交换区写到内存的大小

>  so: 每秒写入交换区的内存大小

IO：（现在的Linux版本块的大小为1024bytes）

>  bi: 每秒读取的块数
>
>  bo: 每秒写入的块数

系统：

> in: 每秒中断数，包括时钟中断。【interrupt】
>
> cs: 每秒上下文切换数。    【count/second】

CPU（以百分比表示）：

>  us: 用户进程执行时间(user time)
>
>  sy: 系统进程执行时间(system time)
>
>  id: 空闲时间(包括IO等待时间),中央处理器的空闲时间 。以百分比表示。
>
>  wa: 等待IO时间

### 1.1.3 如何减少上下文切换

* 无锁并发编程。
* CAS算法。
* 使用最少线程。
* 协程。

### 1.1.4 减少上下文切换实战

​	通过减少线上大量WAITTING的线程，来减少上下文切换次数。

​	第一步： 使用jstack命令dump线程信息。

```
sudo -u admin /opt/ ifeve/ java/ bin/ jstack 31177 > /home/ tengfei. fangtf/ dump17
```

​	第二步：统计所有线程处于什么状态，发现300多个线程处于WAITING状态。

```
grep java. lang. Thread. State dump17 | awk '{print $ 2$ 3$ 4$ 5}' | sort | uniq -c
```

​	第三步：打开dump文件查看处于WATING的线程都在做什么

​	第四步：减少对应的工作线程数。

​	第五步：重启应用，然后重新统计WAITING线程。

### 1.2 死锁

避免死锁的几个常见方法。

* 避免一个线程同时获取多个锁。
* 避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源。
* 尝试使用定时锁，使用lock.tryLock(timeout)来替代使用内部锁机制。
* 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会解锁失败。

### 1.3 资源限制的挑战

略

## 第2章 Java并发机制的底层实现原理

### 2.1 volatile的应用

**1.volatile的定义与实现原理**

​	![](https://pic.imgdb.cn/item/5fa4d0571cd1bbb86b95617c.jpg)

![](https://pic.imgdb.cn/item/5fa4d08f1cd1bbb86b956d1b.jpg)

Lock前缀的指令在多个处理器下会引发两件事情：

1) 将当前处理器缓存行的数据写回到系统内存。

2) 这个写回内存的操作会使其他CPU里混存了该内存地址的数据无效。

​	每个处理器通过嗅探在总显示传播的数据来检查自己的缓存的值是不是过期了，当发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置为无效状态。

volatile的两条实现原则：

1) Lock前缀指令会引起处理器缓存会写到内存。

​	Lock 前缀 指令 导致 在 执行 指令 期间， 声言 处理器 的 LOCK# 信号。 在 多 处理器 环境 中， LOCK# 信号 确保 在 声言 该 信号 期间， 处理器 可以 独占 任何 共享 内存[ 2]。 但是， 在最 近的 处理器 里， LOCK＃ 信号 一般 不 锁 总线， 而是 锁 缓存， 毕竟 锁 总线 开销 的 比较 大。 在 8. 1. 4 节 有 详细 说明 锁定 操作 对 处理器 缓存 的 影响， 对于 Intel486 和 Pentium 处理器， 在 锁 操作 时， 总是 在 总线 上 声言 LOCK# 信号。 但在 P6 和 目前 的 处理器 中， 如果 访问 的 内存 区域 已经 缓存 在 处理器 内部， 则 不会 声言 LOCK# 信号。 相反， 它 会 锁定 这块 内存 区域 的 缓存 并 回 写到 内存， 并使 用 缓存 一致性 机制 来 确保 修改 的 原子 性， 此 操作 被称为“ 缓存 锁定”， 缓存 一致性 机制 会 阻止 同时 修改 由 两个 以上 处理器 缓存 的 内存 区域 数据。

2) 一个处理器的缓存回写到内存会导致其他处理器的缓存无效。

​	IA- 32 处理器 和 Intel 64 处理器 使用 MESI（ 修改、 独占、 共享、 无效） 控制 协议 去 维护 内部 缓存 和 其他 处理器 缓存 的 一致性。 在 多 核 处理器 系统 中进 行 操作 的 时候， IA- 32 和 Intel 64 处理器 能 嗅探 其他 处理器 访问 系统 内存 和 它们 的 内部 缓存。 处理器 使用 嗅探 技术 保证 它的 内部 缓存、 系统 内存 和 其他 处理器 的 缓存 的 数据 在 总线 上 保持一致。 例如， 在 Pentium 和 P6 family 处理器 中， 如果 通过 嗅探 一个 处理器 来 检测 其他 处理器 打算 写 内存 地址， 而这 个 地址 当前 处于 共享 状态， 那么 正在 嗅探 的 处理器 将使 它的 缓存 行 无效， 在下 次 访问 相同 内存 地址 时， 强制 执行 缓存 行 填充。

**2. volatile的使用优化**

​	Doug lea在JDK 7 并发包立新增了一个队列集合类`LinkedTransferQueue`。它使用volatile变量时，通过一种追加字节的方式来优化队列出队和入队的性能。

​	jdk7中没看到这个代码。。。，所以相关笔记不记录。应该在com.google.code.yanf4j.util下。

![](https://pic.imgdb.cn/item/5fa4d78d1cd1bbb86b970919.jpg)



### 2.2 synchronized的实现原理与引用

* 普通同步方法，锁时当前实例对象。
* 对于静态同步方法，锁时当前类的Class对象。
* 对于同步代码块，锁时Synchronized括号里配置的对象。

​	代码块同步是使用monitor enter和monitor exit指令实现的。方法同步是使用另外一种方式实现的(规范里没有详细说明，但是也可以通过这两个指令来实现)。

#### 2.2.1 Java对象头

​	synchronized锁时存在Java对象头里面。如果是数组类型，虚拟机用3个字宽的对象头，如果是非数组类型，则用2字宽对象头(32位虚拟机)。

​	![](https://pic.imgdb.cn/item/5fa4dcc01cd1bbb86b982ffe.jpg)

![](https://pic.imgdb.cn/item/5fa4dce51cd1bbb86b9838ec.jpg)

#### 2.2.2 锁的升级与对比

​	Java 1.6 为了减少获得锁和释放锁带来的性能小号，引入了"偏向锁"和"轻量级锁"。

​	无锁 > 偏向锁 > 轻量级锁 > 重量级锁。锁只能升级不能降级。

**1. 偏向锁**

​	锁不仅不存在多线程竞争，而且总是由同一线程多次获得。所以引入偏向锁。当一个线程访问同步块并获取锁时，会在对象头和栈帧中的锁记录里存储锁偏向的线程ID, 以后该线程在进入和退出代码块时，不需要CAS操作来加锁和解锁，只需简单地测试一下对象头的Mark Word里是否存储指向当前线程的偏向锁。如果测试成功，就代表获取了锁。失败时，需要测试一下Mark Word中偏向锁的表示是否设置为1：如果没有设置，则使用CAS竞争锁；如果设置了，则尝试使用CAS将对象的偏向锁指向当前线程。

​	(1) 偏向锁的撤销

​	当其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁。

​	它 会首 先 暂停 拥有 偏向 锁 的 线程， 然后 检查 持有 偏向 锁 的 线程 是否 活着， 如果 线程 不 处于 活动 状态， 则 将对 象头 设置 成 无 锁 状态； 如果 线程 仍然 活着， 拥有 偏向 锁 的 栈 会被 执行， 遍历 偏向 对象 的 锁 记录， 栈 中的 锁 记录 和 对 象头 的 Mark Word 要么 重新 偏向 于 其他 线程， 要么 恢复 到 无 锁 或者 标记 对象 不适合 作为 偏向 锁， 最后 唤醒 暂停 的 线程。

​	![](https://pic.imgdb.cn/item/5fa5f52a1cd1bbb86bd24e80.jpg)

​	(2) 关闭偏向锁

在jdk6和7中默认启用偏向锁，但是要程序启动几秒之后才激活。可以关闭延迟：`- XX: `BiasedLockingStartupDelay= 0`。如果大部分情况下锁处于竞争状态，可以关闭偏向锁：

`- XX:- UseBiasedLocking= false`, 那么会默认进入到轻量级锁。

**2. 轻量级锁**

​	(1) 轻量级锁加锁

​	JVM在执行同步块前会在当前线程栈帧中创建用于存储锁记录的空间，并将对象头中的Mark Word复制到锁记录中。然后尝试使用CAS将对象头中的Mark Word替换成指向锁记录的指针。如果成功，当前线程获得锁，失败，代表有竞争。那么使用自旋来获取锁。

​	(2) 轻量级锁解锁

​	解锁时，会使用原子的CAS操作将Mark Word替换回到对象头。成功，没有竞争发生。如果失败，表示由竞争，锁就会膨胀成重量级锁。

![](https://pic.imgdb.cn/item/5fa5f8381cd1bbb86bd2e8e6.jpg)

​	因为自旋会消耗CPU，为了避免无用的自旋。一旦升级成重量级锁，不会降级为轻量级锁。当 锁 处于 这个 状态 下， 其他 线程 试图 获取 锁 时， 都会 被 阻塞 住， 当 持有 锁 的 线程 释放 锁 之后 会 唤醒 这些 线程， 被 唤醒 的 线程 就会 进行 新 一轮 的 夺 锁 之争。

**3.锁的优点和缺点**

![](https://pic.imgdb.cn/item/5fa5fa2d1cd1bbb86bd33687.jpg)

### 2.3 原子操作的实现原理

**1.术语定义**

![](https://pic.imgdb.cn/item/5fa605641cd1bbb86bd4dafe.jpg)

**2.处理器如何实现原子操作**

​	32位IA-32处理器使用基于对缓存加锁或对总线加锁的方式来实现多处理器之间的原子操作。处理器自动保证基本的内存操作的原子性。意思是当一个处理器读取一个字节时，其他处理器不能访问这个字节的内存地址。Pentium 6最新的处理器可以自动保证单处理器对同一缓存行里进行16/32/64位的操作是原子的，但是复杂的内存操作处理器不能自动保证其原子性，比如跨总线宽度、跨多个缓存行的跨页表的访问。处理器提供总线锁定和缓存锁定两个机制来保证复杂内存操作的原子性。

​	(1) 使用总线锁保证原子性

​	总线锁就是使用处理器提供的一个LOCK#信号，当一个处理器在总线上输出此型号，其他处理器的请求将被阻塞住，那么该处理器可以独占共享锁。

​	(2) 使用缓存锁保证原子性

​	在同一时刻，只需要保证某个内存地址的操作是原子性的。频繁使用的内存会缓存在处理器的L1, L2, L3的高速缓存里，那么原子操作就可以直接在处理器内部缓存中进行。在Pentium 6和目前的处理器中可以使用"缓存锁定"的方式来实现复杂的原子性。

​	所谓“ 缓存 锁定” 是指 内存 区域 如果 被 缓存 在 处理器 的 缓存 行中， 并且 在 Lock 操作 期间 被 锁定， 那么 当它 执行 锁 操作 回 写到 内存 时， 处理器 不在 总线 上 声言 LOCK＃ 信号， 而是 修改 内部 的 内存 地址， 并 允许 它的 缓存 一致性 机制 来 保证 操作 的 原子 性， 因为 缓存 一致性 机制 会 阻止 同时 修改 由 两个 以上 处理器 缓存 的 内存 区域 数据， 当 其他 处理器 回 写 已被 锁定 的 缓存 行的 数据 时， 会使 缓存 行 无效，

​	有两种情况不会使用缓存锁定。

* 操作数据不能被缓存在处理器内部，或操作的数据跨多个缓存行时。
* 有些处理器不支持缓存。



​	针对以上两条机制，可以通过Intel处理器提供了很多Lock前缀的指令来实现。

**3. Java如何实现原子操作**

​	(1) 使用循环CAS实现原子操作。

​	JVM的CAS操作利用了处理器提供的CMPXCHG指令实现的。基本思路是循环进行CAS操作直到成功为止。

```java
public class Counter {

    private AtomicInteger atomicI = new AtomicInteger(0);
    private int           i       = 0;

    public static void main(String[] args) {
        final Counter cas = new Counter();
        List<Thread> ts = new ArrayList<Thread>(600);
        long start = System.currentTimeMillis();
        for (int j = 0; j < 100; j++) {
            Thread t = new Thread(new Runnable() {
                @Override
                public void run() {
                    for (int i = 0; i < 10000; i++) {
                        cas.count();
                        cas.safeCount();
                    }
                }
            });
            ts.add(t);
        }
        for (Thread t : ts) {
            t.start();

        }
        // �ȴ������߳�ִ�����
        for (Thread t : ts) {
            try {
                t.join();
            } catch (InterruptedException e) {
                e.printStackTrace();
            }

        }
        System.out.println(cas.i);
        System.out.println(cas.atomicI.get());
        System.out.println(System.currentTimeMillis() - start);
    }

    /**
     * ʹ��CASʵ���̰߳�ȫ������
     */
    private void safeCount() {
        for (;;) {
            int i = atomicI.get();
            boolean suc = atomicI.compareAndSet(i, ++i);
            if (suc) {
                break;
            }
        }
    }

    /**
     * ���̰߳�ȫ������
     */
    private void count() {
        i++;
    }

}
```

(2) CAS实现原子操作的三大问题

* ABA问题：原来是A，变成了B，后面有变成了A。可以使用版本号来解决。1A->2B>3A。JDK的Atomic包提供了一个类`AtomicStampedReference`来解决ABA问题。
* 循环时间长开销大：如果JVM能支持处理器提供的pause指令，效率会有一定提升。第一，可以延迟流水线执行指令，使CPU不会消耗过多的执行资源，在一些处理器上延迟时间是0; 第二，可以避免在退出循环时，因内存顺序冲突引起的CPU流水线被清空，从而提高CPU的执行效率。
* 只能保证一个共享变量的原子操作。

(3) 使用锁机制来实现原子操作

​	锁机制保证了只有获得锁的线程才能够操作锁定的内存区域。除了偏向锁，JVM实现锁的方式都用了循环CAS。

## 第3章 Java内存模型

### 3.1 Java内存模型的基础

#### 3.1.1 并发编程模型的两个关键维问题。

​	并发编程需要处理两个关键问题：线程之间如何通信以及线程之间如何同步的问题。在命令式编程汇总，线程之间通信的机制有两种：共享内存和消息传递。

​	在共享内存的并发模型里，线程之间共享程序的公共状态，通过写-读内存中的公共状态进行隐式通信。在消息传递的模型里面，线程之间没有公共状态，线程之间必须通过发送消息来进行显示通信。

​	同步是指程序中用于控制不同线程之间操作发生相对顺序的机制。在共享内存的模型中，同步是显示进行的。在消息传递的模型中，消息发送必须在消息接收之前是隐式的。

​	Java的并发采用的是共享内存模型。线程之间的通信是隐式进行的，但是对程序员完全透明。

#### 3.1.2 Java内存模型的抽象结构

​	局部变量、方法定义参数和异常处理器参数不会在线程之间共享。

​	Java线程之间的通信由JMM控制：线程之间的共享变量存储在主内存中，每个线程都有一个私有的本地内存，本地内存中存储了该现场以读/写共享的变量的副本。

![](https://pic.imgdb.cn/item/5fa614cb1cd1bbb86bd77a9d.jpg)

线程AB要通信。

1) 线程A把本地内存A汇总更新过的共享变量刷新到主内存中去。

2) 线程B到主内存中去读取线程A之前已经更新过的共享变量。

![](https://pic.imgdb.cn/item/5fa625d61cd1bbb86bda9f9b.jpg)

#### 3.1.3 从源代码到指令的重排序。

​	重排序分3种：

1) 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。

2) 指令集并行的重排序。现在处理器采用了指令集并行技术来讲多条指令重叠执行。如果不存在数据依赖项，处理器可以改变语句对应的机器指令的执行顺序。

3) 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，使得加载和存储操作看上去可能在乱序执行。

![](https://pic.imgdb.cn/item/5fa629a71cd1bbb86bdb72e6.jpg)





​	1属于编译器重排序，2和3处于处理器重排序。对于编译器，JMM的编译重排序规则会禁止特定类型的编译器重排序。对于处理器重排序，JMM的处理器重排序规则会要求Java编译器在生成指令集序列时，插入特定类型的内存屏障。

#### 3.1.4 并发编程模型的分类

​		现代处理器使用写缓冲区临时保存向内存才能写入数据。每个处理器上的写缓冲区仅仅对它所在的处理器可见：处理器对内存的读/写的执行顺序，不一定与内存的发生的读/写顺序一样。

![](https://pic.imgdb.cn/item/5fa639fe1cd1bbb86bdf18d4.jpg)

原因如下图所示：

![](https://pic.imgdb.cn/item/5fa63a551cd1bbb86bdf29cd.jpg)



​	从内存操作实际发生的顺序来看，知道处理器A执行A3来刷新自己的写缓存区，写操作A1才算真正执行了。

![](https://pic.imgdb.cn/item/5fa63c3d1cd1bbb86bdf8801.jpg)

​	常见的处理器都允许Store-Load重排序。常见的都不允许对存在数据依赖的操作做重排序。sparc-TSO和x86有用较强的处理器内存模型，他们仅允许对写-读操作做重排序。

​	为了保证内存可见性，Java会插入内存屏障。

![](https://pic.imgdb.cn/item/5fa63cff1cd1bbb86bdfabe4.jpg)

​	StoreLoad是一个全能型的屏障，现代的多处理器大多支持。执行该屏障开销会很昂贵，因为要把写缓冲区的数据全部刷新到内存中。

#### 3.1.5 happens-before简介

​	从JDK5开始，Java使用新的JSR-133内存模型。它用happens-before的概念来阐述操作之间的内存可见性。在JMM中，如果一个操作的执行的结果需要对另一个操作可见，那么就必须存在happens-before关系。

* 程序顺序规则：一个线程中的每个操作，happens-before与该线程中的任意后续操作。
* 监视锁规则：一个锁的解锁，happens-before于随后对这个锁的加锁。
* volatile规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。
* 传递性：如果A happens-before B， B happens-before C，那么 A happens-before C。

![](https://pic.imgdb.cn/item/5fa640621cd1bbb86be0585d.jpg)

### 3.2 重排序

#### 3.2.1 数据依赖性

​	如果两个操作访问同一个变量，且两个操作中有一个为写操作，此时两个操作之间就存在数据依赖性：

![](https://pic.imgdb.cn/item/5fa641bd1cd1bbb86be0a828.jpg)

#### 3.2.2 as-if-serial语义

​	意思是：不管怎么重排序，(单线程)程序的执行结果不会被改变。

#### 3.2.3 程序顺序规则

1） A 　 happens- before B。 
2） B 　 happens- before C。 
3） A 　 happens- before C。



​	这里的A happens-before B，但是实际执行的B却可以排在A之前执行。JMM不一定要求A一定要在B之前执行，仅仅要求前一个操作的执行结果对后一个操作可见，且前一个操作按顺序排在第二个操作之前。如果操作A的执行结果不需要对B可见，而且重排序后的操作A和操作B的执行结果一致，就允许这种重排序。

#### 3.2.4 重排序对多线程的影响

​	略

### 3.3 顺序一致性

#### 3.3.1  数据竞争与顺序一致性

​	数据未正确同步，就可能存在数据竞争。数据竞争定义如下：

​	在一个线程中写一个变量，

​	在另一个线程读一个变量，

​	而且写和读没有通过同步来排序。

​	JMM对正确同步的多线程程序的内存一致性做了如下保证：

​	如果程序时正确同步的，程序的执行将具有一致性——即程序的执行结果与该程序在顺序一致性内存模型中的执行结果相同。

#### 3.3.2 顺序一致性内存模型

1) 一个线程中所有操作必须按照程序的顺序来执行。

2）所有线程只能看到一个单一的操作执行顺序。在顺序一致性内存模型中，每个操作都必须原子执行且立刻对所有线程看见。

![](https://pic.imgdb.cn/item/5fa64a0d1cd1bbb86be26c82.jpg)

​	概念上，顺序一致性模型有一个单一的全局内存，这个内存通过一个开关可以连接到任意一个线程，同时每个线程必须按程序的顺序进行内存读写。(在顺序一致性模型中，所有操作具有全序关系)。

​	有两个线程A,B。操作是A1->A2->A3, B1->B2->B3。

​	如果正确同步，A操作后释放监视器锁，然后B线程获取同一个监视器锁。

![](https://pic.imgdb.cn/item/5fa64bd51cd1bbb86be2cb32.jpg)



如果没有做同步。

![](https://pic.imgdb.cn/item/5fa64bf21cd1bbb86be2d1d5.jpg)

#### 3.3.3 同步程序的顺序一致性效果

​	![](https://pic.imgdb.cn/item/5fa64fdb1cd1bbb86be3bf01.jpg)

![](https://pic.imgdb.cn/item/5fa6500c1cd1bbb86be3c766.jpg)

#### 3.3.4 未同步程序的执行特性

​	对于未同步或者未正确同步的多线程程序，JMM只提供最小安全性：读取到的值，要么是某个线程写入的值，要么是默认值。为了实现最小安全性，JVM在堆上分配对象时，首先会对内存空间进行清零，然后才会在上面分配对象。

​	JMM不保证未同步的执行结果与该程序在顺序一致性模型中执行的结果一致。如果要一致，JMM要禁用大量的处理器和编译器优化。

​	

​	1） 顺序 一致性 模型 保证 单 线程 内 的 操作 会 按 程序 的 顺序 执行， 而 JMM 不保 证 单 线程 内 的 操作 会 按 程序 的 顺序 执行。

​	 2） 顺序 一致性 模型 保证 所有 线程 只能 看到 一致 的 操作 执行 顺序， 而 JMM 不保 证 所有 线程 能看 到 一致 的 操作 执行 顺序。

​	3） JMM 不 保证 对 64 位 的 long 型 和 double 型 变量 的 写 操作 具有 原子 性， 而 顺序 一致性 模型 保证 对 所有 的 内存 读/ 写 操作 都 具有 原子 性。

​	和处理器的总线工作机制有关。总线会同步试图并发使用总线的事务。在每一个处理器执行总线事务期间，总线会禁用其他处理器和I/O设备执行内存的读/写。

![](https://pic.imgdb.cn/item/5fa652231cd1bbb86be43675.jpg)

​	如果A,B,C同时发起总线事务，总线仲裁会对竞争做出裁决。

​	这个工作机制可以把所有处理器对内存的访问以串行化来执行。在任意时间点，最多只有一个处理器可以访问你内存。这个特性确保了单个总线事务中的内存读/写操作的原子性。当JVM在32位处理器上允许时，可能会把一个64位long/double变量的写操作拆分为两个32位的写操作来执行。这两个写操作可能分配到不同的总线事务中执行，此时对这个64位变量的写操作将不具有原子性。

​	![](https://pic.imgdb.cn/item/5fa653321cd1bbb86be47467.jpg)



​	在JSR-133之前的旧内存模型中，64位的读写操作都可以拆分成两个操作。JSR-133开始，只允许拆分写操作。

### 3.4 volatile的内存语义

#### 3.4.1 volatile的特性

​	理解volatile特性的一个好方法是把volatile变量的单个读/写，看成是使用同一个锁对这些单个读/写操作做了同步。

* 可见性：对一个volatile变量的读，总是能看到(任意线程)对这个volatile变量最后的写入。
* 原子性：对任意单个volatile变量的读/写具有原子性，但类似于a++这种复合操作不具有原子性。

#### 3.4.2 volatile写-读简历的happens-before关系

​	对程序员来说，volatile对线程的内存可见性的影响比volatile自身的特性更为重要。

​	JSR-133开始, volatile变量的写-读可以实现线程之间的通信。volatile的写和锁的释放由相同的内存语义；读和锁的获取有相同语义。

​	![](https://pic.imgdb.cn/item/5fa65cc01cd1bbb86be68238.jpg)

假设线程A执行writer()之后，线程B执行reader()方法。

1） 根据 程序 次序 规则， 1 happens- before 2; 3 happens- before 4。

 2） 根据 volatile 规则， 2 happens- before 3。 

3） 根据 happens- before 的 传递性 规则， 1 happens- before 4。

![](https://pic.imgdb.cn/item/5fa65d091cd1bbb86be68f74.jpg)

#### 3.4.3 volatile写-读的内存意义

​	当写一个volatile变量时，JMM会把该线程对应的本地内存的共享变量值刷新到主内存。

![](https://pic.imgdb.cn/item/5fa65da51cd1bbb86be6aab7.jpg)



volatile读的内存语义如下：

​	当读一个volatile变量时，JMM会把该线程对应的本地内存置为无效。线程接下来从主内存中读取共享变量。

​	对volatile写和读的内存语义做个总结

* 线程A写一个volatile变量，实际上是线程A向接下来要读这个volatile变量的某个线程发出了消息。
* 线程B读一个volatile变量，实际上是线程B接收了某个之前线程发出的消息。
* 线程A写一个volatile变量，虽有线程B读这个变量，实际上是线程A通过主内存向线程B发送消息。

![](https://pic.downk.cc/item/5faa73e51cd1bbb86bbd2709.jpg)

#### 3.4.4 volatile内存语义的实现

​	![](https://pic.downk.cc/item/5faa741b1cd1bbb86bbd3bf3.jpg)

* 当第二个操作是volatile写，不管第一个操作是什么，都不能重排序。
* 当第一个操作是volatile读时，不管第二个操作是什么，都不能重排序。
* 当第一个操作是volatile写，第二个操作是volatile读/写时，不能重排序。



​	一个最优不止来最小化插入屏障总数几乎不可能。为此,JMM采取保守策略。

* 每一个volatile写的前面，插入一个StoreStore屏障。
* 每一个volatile写后面，插入StoreLoad屏障。
* 在每个volatile读操作的后面插入一个LoadLoad屏障。
* 在每个volatile读操作后面插入一个LoadStore屏障。

![](https://pic.downk.cc/item/5faa76bd1cd1bbb86bbe0f65.jpg)

​	volatile写后面的StoreLoad屏障。编译器无法准确判断volatile写后面是否需要插入一个StoreLoad屏障。JMM采取保守策略：要么在volatile写后面或者每个volatile读前面插入一个StoreLoad屏障。最后选择了写后面加。

![](https://pic.downk.cc/item/5faa773a1cd1bbb86bbe2cc8.jpg)

示例：

![](https://pic.downk.cc/item/5faa778b1cd1bbb86bbe42cd.jpg)

![](https://pic.downk.cc/item/5faa77a91cd1bbb86bbe4d7a.jpg)

​	上面 的 优化 针对 任意 处理器 平台， 由于 不同 的 处理器 有 不同“ 松紧 度” 的 处理器 内存 模型， 内存 屏障 的 插入 还可以 根据 具体 的 处理器 内存 模型 继续 优化。 以 X86 处理器 为例， 图  中 除 最后 的 StoreLoad 屏障 外， 其他 的 屏障 都会 被 省略。

​	前文 提 到过， X86 处理器 仅 会对 写- 读 操作 做 重 排序。 X86 不 会对 读- 读、 读- 写 和 写- 写 操作 做 重 排序， 因此 在 X86 处理器 中会 省略 掉 这 3 种 操作 类型 对应 的 内存 屏障。 在 X86 中， JMM 仅 需 在 volatile 写 后面 插入 一个 StoreLoad 屏障 即可 正确 实现 volatile 写- 读 的 内存 语义。 这 意味着 在 X86 处理器 中， volatile 写的 开销 比 volatile 读 的 开销 会 大 很多。

#### 3.4.5 JSR-133为什么要增强volatile内存语义

​	为了 提供 一种 比 锁 更轻 量级 的 线程 之间 通信 的 机制， JSR- 133 专家组 决定 增强 volatile 的 内存 语义。如果 读者 想在 程序 中用 volatile 代替 锁， 请 一定 谨慎， 具体 详情 请参阅 Brian Goetz 的 文章《 Java 理论 与 实践： 正确 使用 Volatile 变量》。

### 3.5 锁的内存语义

#### 3.5.1 锁的释放-获取简历的happens-before关系

​	锁除了让临界区互斥执行之外，还可以让释放锁的线程向获取同一个锁的线程发送消息。

​	![](https://pic.downk.cc/item/5faa95a21cd1bbb86bc58371.jpg)

假设 线程 A 执行 writer() 方法， 随后 线程 B 执行 reader() 方法。 

根据 happens- before 规则， 这个 过程 包含 的 happens- before 关系 可以 分为 3 类。

 1） 根据 程序 次序 规则， 1 happens- before 2, 2 happens- before 3; 4 happens- before 5, 5 happens- before 6。 

2） 根据 监视器 锁 规则， 3 happens- before 4。 

3） 根据 happens- before 的 传递性， 2 happens- before 5。



![](https://pic.downk.cc/item/5faa95b81cd1bbb86bc587fc.jpg)

#### 3.5.2 锁的释放和获取的内存语义

​	![](https://pic.downk.cc/item/5faa96331cd1bbb86bc5aa9a.jpg)

​	当线程获取锁时，JMM会把该现场对应的本地内存置为无效。

![](https://pic.downk.cc/item/5faa96c61cd1bbb86bc5cee9.jpg)

* 线程 A 释放 一个 锁， 实质上 是 线程 A 向 接下来 将要 获取 这个 锁 的 某个 线程 发出 了（ 线程 A 对 共享 变量 所做 修改 的） 消息。 
* 线程 B 获取 一个 锁， 实质上 是 线程 B 接收 了 之前 某个 线程 发出 的（ 在 释放 这个 锁 之前 对 共享 变量 所做 修改 的） 消息。 
* 线程 A 释放 锁， 随后 线程 B 获取 这个 锁， 这个 过程 实质上 是 线程 A 通过 主 内存 向 线程 B 发送 消息。

#### 3.5.3 锁内存语义的实现

​	![](https://pic.downk.cc/item/5faa98ac1cd1bbb86bc649f4.jpg)

![](https://pic.downk.cc/item/5faa98c71cd1bbb86bc65086.jpg)

ReentrantLock 的 实现 依赖于 Java 同步 器 框架 AbstractQueuedSynchronizer（ 本文 简称 之为 AQS）。 AQS 使用 一个 整型 的 volatile 变量（ 命名为 state） 来 维护 同步 状态， 马上 我们 会 看到， 这个 volatile 变量 是 ReentrantLock 内存 语义 实现 的 关键。

ReentrantLock分为公平锁和非公平锁。

公平锁加锁轨迹如下：

1) ReetrantLock:lock();

2) FairSync:lock();

3) AbstractQueuedSynchronizer:acquire(int arg);

4) ReetrantLock:tryAcquire(int acquires);

第四步才真正开始加锁。

![](https://pic.downk.cc/item/5faa99421cd1bbb86bc66c17.jpg)

加锁时先读volatile变量state。

公平锁的释放如下：

1） ReentrantLock:unlock()。 

2） AbstractQueuedSynchronizer:release( int arg)。 

3） Sync:tryRelease( int releases)。

第三步才是真正释放锁。

![](https://pic.downk.cc/item/5faa9a341cd1bbb86bc6a63c.jpg)



非公平锁加锁：

1） ReentrantLock:lock()。

2） NonfairSync:lock()。 

3） AbstractQueuedSynchronizer:compareAndSetState( int expect, int update)。

 <img src="https://pic.downk.cc/item/5faa9bb71cd1bbb86bc70a61.jpg" style="zoom:200%;" />

​	编译器 不 会对 volatile 读 与 volatile 读后面 的 任意 内存 操作 重 排序； 编译器 不 会对 volatile 写 与 volatile 写 前面 的 任意 内存 操作 重 排序。意味着 为了 同时 实现 volatile 读 和 volatile 写的 内存 语义， 编译器 不 能对 CAS 与 CAS 前面 和 后面 的 任意 内存 操作 重 排序。

​	对 应于 intel X86 处理器 的 C++源 代码 的 片段。

![](https://pic.downk.cc/item/5faa9c461cd1bbb86bc72bfa.jpg)

intel 的 手册 对 lock 前缀 的 说明 如下。

 1） 确保 对 内存 的 读- 改- 写 操作 原子 执行。 在 Pentium 及 Pentium 之前 的 处理器 中， 带有 lock 前缀 的 指令 在 执行 期间 会 锁住 总线， 使得 其他 处理器 暂时 无法 通过 总线 访问 内存。 很 显然， 这 会 带来 昂贵 的 开销。 从 Pentium 4、 Intel Xeon 及 P6 处理器 开始， Intel 使用 缓存 锁定（ Cache Locking） 来 保证 指令 执行 的 原子 性。 缓存 锁定 将 大大 降低 lock 前缀 指令 的 执行 开销。 

2） 禁止 该 指令， 与 之前 和 之 后的 读 和 写 指令 重 排序。

 3） 把 写 缓冲区 中的 所有 数据 刷新 到 内存 中。

* 公平 锁 和 非 公平 锁 释放 时， 最后 都要 写 一个 volatile 变量 state。

* 

* 公平 锁 获取 时， 首先 会 去 读 volatile 变量。

* 非 公平 锁 获取 时， 首先 会用 CAS 更新 volatile 变量， 这个 操作 同时 具有 volatile 读 和 volatile 写的 内存 语义。

  

对 ReentrantLock 的 分析 可以 看出， 锁 释放- 获取 的 内存 语义 的 实现 至少 有 下面 两种 方式。 

1） 利用 volatile 变量 的 写- 读 所 具有 的 内存 语义。

 2） 利用 CAS 所 附带 的 volatile 读 和 volatile 写的 内存 语义。

#### 3.5.4 concurrent包的实现

现在Java线程通信有了下面4种方式：

* A线程写volatile变量，随后B线程读这个volatile变量。
* A线程写volatile变量，随后B线程用CAS更新这个volatile变量。
* A线程用CAS更新一个volatile变量，随后B线程用CAS更新这个volatile变量。
* A线程用CAS更新一个volatile变量，随后B线程读这个volatile变量。



​	现代处理器都会去支持某种能对内存执行原子性读-改-写操作的原子指令。把这些整合在一起，就形成了整个concurrent包得以实现的基石。

​	concurrent包有一个通用化的实现模式。

1. 声明共享变量为volatile。

2. 然后使用CAS原子条件更新来实现线程之间的同步。

3. 配合以volatile的读/写和CAS所具有的volatile读和谐的内存语义来试下你先吃之间的通信。

   ![](https://pic.imgdb.cn/item/5fab69941cd1bbb86bec096d.jpg)

### 3.6 final域的内存语义

#### 3.6.1 final域的重排序规则

​	编译器和处理器要遵循两个重排序规则

1. 在构造器内对一个final域的写入，与随后把这个被构造对象的引用赋值给一个引用变量，这两个操作不能重排序。
2. 除此读一个包含final域对象的引用，与随后初次读这个对象的final域，不能重排序。

#### 3.6.2 写final域的重排序规则

1. JMM禁止编译把final域的写重排序到构造函数之外。
2. 编译器会在final域写之后，构造器return之前，插入一个StoreStore屏障。这个屏障禁止fi把final域的写重排序到构造函数之外。

写 final 域 的 重排 序 规则 可以 确保： 在 对象 引用 为 任意 线程 可见 之前， 对象 的 final 域 已经 被 正确 初始化 过了。

#### 3.6.3 读final域的重排序规则

​		编译器会在读final域操作前面加入一个LoadLoad屏障。

​		初次读对应引用与初次读该对象包含的final域，这两个操作之间存在间接依赖关系。大多数处理器不会重排序这两个操作。但有少数处理器会(比如alpha处理器)，这个规则就是专门用来针对这种处理器的。

​	这个规则可以确保：在读一个对象final域之前，一定会先读这个final域对象的引用。

#### 3.6.4 fiinal域为引用类型

​	对于 引用 类型， 写 final 域 的 重排 序 规则 对 编译器 和 处理器 增加 了 如下 约束： 在 构造 函数 内 对 一个 final 引用 的 对象 的 成员 域 的 写入， 与 随后 在 构造 函数 外 把 这个 被 构造 对象 的 引用 赋值 给 一个 引用 变量， 这 两个 操作 之间 不能 重 排序。

![](https://pic.imgdb.cn/item/5fab71431cd1bbb86beda2f8.jpg)

​	假设 首先 线程 A 执行 writerOne() 方法， 执行 完 后 线程 B 执行 writerTwo() 方法， 执行 完 后 线程 C 执行reader() 方法。

​	1 是对 final 域 的 写入， 2 是对 这个 final 域 引用 的 对象 的 成员 域 的 写入， 3 是把 被 构造 的 对象 的 引用 赋值 给 某个 引用 变量。 这里 除了 前面 提到 的 1 不能 和 3 重 排序 外， 2 和 3 也不能 重 排序。

​	JMM 可以 确保 读 线程 C 至少 能 看到 写 线程 A 在 构造 函数 中 对 final 引用 对象 的 成员 域 的 写入。 即 C 至少 能看 到数 组 下标 0 的 值 为 1。 而 写 线程 B 对数 组 元素 的 写入， 读 线程 C 可能 看得 到， 也可 能看 不到。

#### 3.6.5 为什么final引用不能从构造函数内"逸出"

​	在 构造 函数 返回 前， 被 构造 对象 的 引用 不能 为 其他 线程 所见， 因为 此时 的 final 域 可能 还没有 被 初始化。 在 构造 函数 返回 后， 任意 线程 都将 保证 能看 到 final 域 正确 初始化 之 后的 值。

#### 3.6.6 final语义在处理器中的实现

​	由于 X86 处理器 不 会对 写- 写 操作 做 重 排序， 所以 在 X86 处理器 中， 写 final 域 需要 的 StoreStore 屏障会被 省略 掉。 同样， 由于 X86 处理器 不会 对 存在 间接 依赖 关系 的 操作 做 重 排序， 所以 在 X86 处理器 中， 读 final 域 需要 的 LoadLoad 屏障 也会 被 省略 掉。 也就是说， 在 X86 处理器 中， final 域 的 读/ 写 不会 插入 任何 内存 屏障！

#### 3.6.7 JSR-133为什么要增强final的语义

在 旧的 Java 内存 模型 中， 一个 最 严重 的 缺陷 就是 线程 可能 看到 final 域 的 值 会 改变。 比如， 一个 线程 当前 看到 一个 整型 final 域 的 值 为 0（ 还未 初始化 之前 的 默认值）， 过 一段时间 之后 这个 线程 再去 读 这个 final 域 的 值 时， 却 发现 值 变为 1（ 被 某个 线程 初始化 之 后的 值）。最 常见 的 例子 就是 在 旧的 Java 内存 模型 中， String 的 值 可能 会 改变。
	只要 对象 是 正确 构造 的（ 被 构造 对象 的 引用 在 构造 函数 中 没有“ 逸出”）， 那么 不需要 使用 同步（ 指 lock 和 volatile 的 使用） 就可以 保证 任意 线程 都能 看到 这个 final 域 在 构造 函数 中 被 初始化 之 后的 值。

###  3.7 happens-before

#### 3.7.1 JMM的设计

​	需要考虑两个关键因素：

* 程序员对内存模型的使用。易于理解，易于编程。希望一个强内存模型。
* 编译器和处理器对内存模型的实现。希望约束少，可以做更多优化。希望一个弱内存模型。



​	JMM采取如下设计：

* 对于 会 改变 程序 执行 结果 的 重 排序， JMM 要求 编译器 和 处理器 必须 禁止 这种 重 排序。 
* 对于 不会 改变 程序 执行 结果 的 重 排序， JMM 对 编译器 和 处理器 不做 要求（ JMM 允许 这种 重 排序）。

![](https://pic.imgdb.cn/item/5fab76e61cd1bbb86beeee05.jpg)

* JMM 向 程序员 提供 的 happens- before 规则 能 满足 程序员 的 需求。

* JMM 对 编译器 和 处理器 的 束缚 已经 尽可能 少。

  

#### 3.7.2 happens-before的定义

《JSR- 133: Java Memory Model and Thread Specification》 对 happens- before 关系 的 定义 如下。

1） 如果 一个 操作 happens- before 另一个 操作， 那么 第一个 操作 的 执行 结果 将对 第二个 操作 可见， 而且 第一个 操作 的 执行 顺序 排在 第二个 操作 之前。

 2） 两个 操作 之间 存在 happens- before 关系， 并不 意味着 Java 平台 的 具体 实现 必须 要 按照 happens- before 关系 指定 的 顺序 来 执行。 如果 重排 序 之后 的 执行 结果， 与 按 happens- before 关系 来 执行 的 结果 一致， 那么 这种 重排 序 并不 非法（ 也就是说， JMM 允许 这种 重 排序）。

#### 3.7.3 happens-before规则

1） 程序 顺序 规则： 一个 线程 中的 每个 操作， happens- before 于 该 线程 中的 任意 后续 操作。

2） 监视器 锁 规则： 对 一个 锁 的 解 锁， happens- before 于 随后 对 这个 锁 的 加锁。 

3） volatile 变量 规则： 对 一个 volatile 域 的 写， happens- before 于 任意 后续 对 这个 volatile 域 的 读。

4） 传递性： 如果 A happens- before B， 且 B happens- before C， 那么 A happens- before C。 

5） start() 规则： 如果 线程 A 执行 操作 ThreadB. start()（ 启动 线程 B）， 那么 A 线程 的 ThreadB. start() 操作 happens- before 于 线程 B 中的 任意 操作。

 6） join() 规则： 如果 线程 A 执行 操作 ThreadB. join() 并 成功 返回， 那么 线程 B 中的 任意 操作 happens- before 于 线程 A 从 ThreadB. join() 操作 成功 返回。

### 3.8 双重检查锁定与延迟初始化

​	双重 检查 锁定 是 常见 的 延迟 初始化 技术， 但它 是一 个 错误 的 用法。 本文 将 分析 双重 检查 锁定 的 错误 根源， 以及 两种 线程 安全 的 延迟 初始化 方案。

#### 3.8.1  双重检查锁定的由来

![](https://pic.imgdb.cn/item/5fab78281cd1bbb86bef2f77.jpg)

* 多个线程视图在同一时间创建对象，会通过加锁来保证只有一个线程能创建对象。
* 在对象创建好之后，执行getInstance()方法不需要获取锁，直接返回已创建好的对象。

#### 3.8.2 问题的根源

​	第7行创建对象代码可以分解为如下三行伪代码：

![](https://pic.imgdb.cn/item/5fab78f21cd1bbb86bef538d.jpg)

​	2和3之间可能会发生重排序。

#### 3.8.3 预计volatile的解决方案

​	![](https://pic.imgdb.cn/item/5fab79da1cd1bbb86bef80d8.jpg)

​	注意该解决方案需要基于JDK5及以上

#### 3.8.4 基于类初始化的解决方案

​	![](https://pic.imgdb.cn/item/5fab7a521cd1bbb86bef9fba.jpg)

![](https://pic.imgdb.cn/item/5fab7a761cd1bbb86befb006.jpg)

​	Java 语言 规范 规定， 对于 每一个 类 或 接口 C， 都有 一个 唯一 的 初始化 锁 LC 与之 对应。 从 C 到 LC 的 映射， 由 JVM 的 具体 实现 去 自由 实现。 JVM 在 类 初始化 期间 会 获取 这个 初始化 锁， 并且 每个 线程 至少 获取 一次 锁 来 确保 这个 类 已经 被 初始化 过了（ 事实上， Java 语言 规范 允许 JVM 的 具体 实现 在这里 做 一些 优化， 见 后文 的 说明）。

根据 Java 语言 规范， 在 首次 发生 下列 任意 一种 情况 时， 一个 类 或 接口 类型 T 将被 立即 初始化。 

1） T 是 一个 类， 而且 一个 T 类型 的 实例 被 创建。 

2） T 是 一个 类， 且 T 中 声明 的 一个 静态 方法 被 调用。 

3） T 中 声明 的 一个 静态 字段 被 赋值。 

4） T 中 声明 的 一个 静态 字段 被 使用， 而且 这个 字段 不是 一个 常量 字段。

5） T 是 一个 顶 级 类（ Top Level Class， 见 Java 语言 规范 的 § 7. 6）， 而且 一个 断言 语句 嵌套 在 T 内部 被 执行。

​	第 1 阶段： 通过 在 Class 对象 上 同步（ 即 获取 Class 对象 的 初始化 锁）， 来 控制 类 或 接口 的 初始化。 这个 获取 锁 的 线程 会 一直 等待， 直到 当前 线程 能够 获取 到 这个 初始化 锁。

​	第 2 阶段： 线程 A 执行 类 的 初始化， 同时 线程 B 在 初始化 锁 对应 的 condition 上 等待。

​	第 3 阶段： 线程 A 设置 state= initialized， 然后 唤醒 在 condition 中 等待 的 所有 线程。

​	第 4 阶段： 线程 B 结束 类 的 初始化 处理。

​	第 5 阶段： 线程 C 执行 类 的 初始化 的 处理。

但 基于 volatile 的 双重 检查 锁定 的 方案 有一个 额外 的 优势： 除了 可以 对 静态 字段 实现 延迟 初始化 外， 还可 以对 实例 字段 实现 延迟 初始化。

### 3.9 Java内存模型综述

#### 3.9.1 处理器内存的模型

​	常见处理器的内存模型划分为如下几种类型。

* 放松 程序 中写- 读 操作 的 顺序， 由此 产生了 Total Store Ordering 内存 模型（ 简称 为 TSO）。

* 在上面的基础上，继续放松程序中写-写操作的顺序，因此产生了Partial StoreOrder内存模型(简称PSO)。

* 在前面 两条 的 基础上， 继续 放松 程序 中 读- 写 和 读- 读 操作 的 顺序， 由此 产生了 Relaxed Memory Order 内存 模型（ 简称 为 RMO） 和 PowerPC 内存 模型。

  ![](https://pic.imgdb.cn/item/5fab80421cd1bbb86bf0e8f8.jpg)

  

![](https://pic.imgdb.cn/item/5fab80771cd1bbb86bf0f47d.jpg)

#### 3.9.2 各种内存模型之间的关系

​	![](https://pic.imgdb.cn/item/5fab81491cd1bbb86bf126ba.jpg)

​	JMM 是一 个 语言 级 的 内存 模型， 处理器 内存 模型 是 硬件 级 的 内存 模型， 顺序 一致性 内存 模型 是一 个 理论 参考 模型。

#### 3.9.3 JMM的内存可见性

* 单线 程 程序。 单线 程 程序 不会 出现 内存 可见 性 问题。 编译器、 runtime 和 处理器 会 共同 确保 单线 程 程序 的 执行 结果 与 该 程序 在 顺序 一致性 模型 中的 执行 结果 相同。 
* 正确 同步 的 多 线程 程序。 正确 同步 的 多 线程 程序 的 执行 将 具有 顺序 一致性（ 程序 的 执行 结果 与 该 程序 在 顺序 一致性 内存 模型 中的 执行 结果 相同）。 这是 JMM 关注 的 重点， JMM 通过 限制 编译器 和 处理器 的 重 排序 来 为 程序员 提供 内存 可见 性 保证。 
* 未 同步/ 未 正确 同步 的 多 线程 程序。 JMM 为 它们 提供 了 最小 安全性 保障： 线程 执行 时 读取 到 的 值， 要么 是 之前 某个 线程 写入 的 值， 要么 是 默认值（ 0、 null、 false）。

![](https://pic.imgdb.cn/item/5fab82931cd1bbb86bf17c2c.jpg)

#### 3.9.4 JSR-133对旧内存模型的修补

​	主要有两个：

* 增强 volatile 的 内存 语义。 旧 内存 模型 允许 volatile 变量 与 普通 变量 重 排序。 JSR- 133 严格 限制 volatile 变量 与 普通 变量 的 重 排序， 使 volatile 的 写- 读 和 锁 的 释放- 获取 具有 相同 的 内存 语义。 
* 增强 final 的 内存 语义。 在 旧 内存 模型 中， 多次 读取 同一个 final 变量 的 值 可能 会 不 相同。 为此， JSR- 133 为 final 增加 了 两个 重排 序 规则。 在 保证 final 引用 不会 从 构造 函数 内 逸出 的 情况下， final 具有 了 初始化 安全性。

## 第4章 Java并发编程基础

### 4.1 线程简介

#### 4.1.1 什么是线程

​	现代操作系统调度的最小单元是线程，也叫轻量级进程。

```java
public class MultiThread {

    public static void main(String[] args) {
        ThreadMXBean threadMXBean = ManagementFactory.getThreadMXBean();
        ThreadInfo[] threadInfos = threadMXBean.dumpAllThreads(false, false);
       
        for (ThreadInfo threadInfo : threadInfos) {
            System.out.println("[" + threadInfo.getThreadId() + "] " + threadInfo.getThreadName());
        }
    }
}
/**
[8] JDWP Command Reader
[7] JDWP Event Helper Thread
[6] JDWP Transport Listener: dt_socket
[5] Attach Listener
[4] Signal Dispatcher
[3] Finalizer
[2] Reference Handler
[1] main
*/
```

#### 4.1.2 为什么要使用多线程

(1) 更多的处理器核心

(2) 更快的响应时间

(3) 更好的变成模型

#### 4.1.3 线程优先级

​	在Java线程中，通过一个整型成员变量priority来控制优先级，范围1~10。默认是5。针对频繁阻塞(休眠或IO操作)的线程需要设置较高优先级，而偏重计算的线程则设置较低的优先级。在不同的JVM以及操作系统上，线程规划会存在差异，有些操作系统甚至会忽略对线程优先级的设定。

```java
public class Priority {
    private static volatile boolean notStart = true;
    private static volatile boolean notEnd   = true;

    public static void main(String[] args) throws Exception {
        List<Job> jobs = new ArrayList<Job>();
        for (int i = 0; i < 10; i++) {
            int priority = i < 5 ? Thread.MIN_PRIORITY : Thread.MAX_PRIORITY;
            Job job = new Job(priority);
            jobs.add(job);
            Thread thread = new Thread(job, "Thread:" + i);
            thread.setPriority(priority);
            thread.start();
        }
        notStart = false;
        Thread.currentThread().setPriority(8);
        System.out.println("done.");
        TimeUnit.SECONDS.sleep(10);
        notEnd = false;

        for (Job job : jobs) {
            System.out.println("Job Priority : " + job.priority + ", Count : " + job.jobCount);
        }

    }

    static class Job implements Runnable {
        private int  priority;
        private long jobCount;

        public Job(int priority) {
            this.priority = priority;
        }

        public void run() {
            while (notStart) {
                Thread.yield();
            }
            while (notEnd) {
                Thread.yield();
                jobCount++;
            }
        }
    }
}
/**
done.
Job Priority : 1, Count : 3944787
Job Priority : 1, Count : 3957283
Job Priority : 1, Count : 4208014
Job Priority : 1, Count : 4160023
Job Priority : 1, Count : 3975817
Job Priority : 10, Count : 4528591
Job Priority : 10, Count : 4712696
Job Priority : 10, Count : 4685421
Job Priority : 10, Count : 4534104
Job Priority : 10, Count : 4546863
*/
```



​	比较接近，说明程序的正确性不能依赖线程的优先级高低。

#### 4.1.4 线程的状态

![](https://pic.imgdb.cn/item/5fabe1ac1cd1bbb86b0958ef.jpg)

下面使用jstack 尝试查看示例代码运行时的线程状态

```java
public class ThreadState {

    private static Lock lock = new ReentrantLock();

    public static void main(String[] args) {
        new Thread(new TimeWaiting(), "TimeWaitingThread").start();
        new Thread(new Waiting(), "WaitingThread").start();
        // ʹ������Blocked�̣߳�һ����ȡ���ɹ�����һ��������
        new Thread(new Blocked(), "BlockedThread-1").start();
        new Thread(new Blocked(), "BlockedThread-2").start();
        new Thread(new Sync(), "SyncThread-1").start();
        new Thread(new Sync(), "SyncThread-2").start();
    }

    /**
     * ���̲߳��ϵĽ���˯��
     */
    static class TimeWaiting implements Runnable {
        @Override
        public void run() {
            while (true) {
                SleepUtils.second(100);
            }
        }
    }

    /**
     * ���߳���Waiting.classʵ���ϵȴ�
     */
    static class Waiting implements Runnable {
        @Override
        public void run() {
            while (true) {
                synchronized (Waiting.class) {
                    try {
                        Waiting.class.wait();
                    } catch (InterruptedException e) {
                        e.printStackTrace();
                    }
                }
            }
        }
    }

    /**
     * ���߳���Blocked.classʵ���ϼ����󣬲����ͷŸ���
     */
    static class Blocked implements Runnable {
        public void run() {
            synchronized (Blocked.class) {
                while (true) {
                    SleepUtils.second(100);
                }
            }
        }
    }

    static class Sync implements Runnable {

        @Override
        public void run() {
            lock.lock();
            try {
                SleepUtils.second(100);
            } finally {
                lock.unlock();
            }

        }

    }
}
```

输入jps

![](https://pic.imgdb.cn/item/5fabe4271cd1bbb86b0a1eb9.jpg)

然后输入jstack 18576

![](https://pic.imgdb.cn/item/5fabe4721cd1bbb86b0a2d2f.jpg)

Java线程状态变迁如图

![](https://pic.imgdb.cn/item/5fabe49c1cd1bbb86b0a35e9.jpg)

#### 4.1.5 Daemon线程

​	是一种支持性线程。当一个虚拟机中不存在非Daemon线程，Java虚拟机才会退出。可以调用Thread.setDaemon(true)将线程设置为Daemon线程。

​	Daemon 线程 被用 作 完成 支持 性 工作， 但是 在 Java 虚拟 机 退出 时 Daemon 线程 中的 finally 块 并不 一定 会 执行。

```java
public class Daemon {

    public static void main(String[] args) {
        Thread thread = new Thread(new DaemonRunner());
        thread.setDaemon(true);
        thread.start();
    }

    static class DaemonRunner implements Runnable {
        @Override
        public void run() {
            try {
                SleepUtils.second(100);
            } finally {
                System.out.println("DaemonThread finally run.");
            }
        }
    }
}
```

### 4.2 启动和终止线程

#### 4.2.1 构造线程

```java
private void init(ThreadGroup g, Runnable target, String name,
                  long stackSize, AccessControlContext acc) {
    if (name == null) {
        throw new NullPointerException("name cannot be null");
    }

    this.name = name;

    Thread parent = currentThread();
    SecurityManager security = System.getSecurityManager();
    if (g == null) {
        /* Determine if it's an applet or not */

        /* If there is a security manager, ask the security manager
           what to do. */
        if (security != null) {
            g = security.getThreadGroup();
        }

        /* If the security doesn't have a strong opinion of the matter
           use the parent thread group. */
        if (g == null) {
            g = parent.getThreadGroup();
        }
    }

    /* checkAccess regardless of whether or not threadgroup is
       explicitly passed in. */
    g.checkAccess();

    /*
     * Do we have the required permissions?
     */
    if (security != null) {
        if (isCCLOverridden(getClass())) {
            security.checkPermission(SUBCLASS_IMPLEMENTATION_PERMISSION);
        }
    }

    g.addUnstarted();

    this.group = g;
    this.daemon = parent.isDaemon();
    this.priority = parent.getPriority();
    if (security == null || isCCLOverridden(parent.getClass()))
        this.contextClassLoader = parent.getContextClassLoader();
    else
        this.contextClassLoader = parent.contextClassLoader;
    this.inheritedAccessControlContext =
            acc != null ? acc : AccessController.getContext();
    this.target = target;
    setPriority(priority);
    if (parent.inheritableThreadLocals != null)
        this.inheritableThreadLocals =
            ThreadLocal.createInheritedMap(parent.inheritableThreadLocals);
    /* Stash the specified stack size in case the VM cares */
    this.stackSize = stackSize;

    /* Set thread ID */
    tid = nextThreadID();
}
```

一个 新 构造 的 线程 对象 是 由其 parent 线程 来 进行 空间 分配 的， 而 child 线程 继承 了 parent 是否 为 Daemon、 优先级 和 加载 资源 的 contextClassLoader 以及 可继承 的 ThreadLocal， 同时 还会 分配 一个 唯一 的 ID 来 标识 这个 child 线程。

#### 4.2.2 启动线程。

​	调用start()方法。

> 注意 　 启动 一个 线程 前， 最好 为 这个 线程 设置 线程 名称， 因为 这样 在 使用 jstack 分析 程序 或者 进行 问题 排 查 时， 就会 给 开发 人员 提供 一些 提示， 自定义 的 线程 最好 能够 起 个 名字。

#### 4.2.3 理解中断

​	中断可以理解为线程的一个标识位属性，它表示一个运行中的线程是否被其他线程进行了中断操作。

​	可以通过方法isInterruptted()来判断是否被中断，也可以调用静态方法Thread.interrupted()对当前线程的中断标识位复位。如果 该 线程 已经 处于 终结 状态， 即使 该 线程 被 中断 过， 在 调用 该 线程 对象 的 isInterrupted() 时 依旧 会 返回 false。

```java
public class Interrupted {

    public static void main(String[] args) throws Exception {
        // sleepThread不停的尝试睡眠
        Thread sleepThread = new Thread(new SleepRunner(), "SleepThread");
        sleepThread.setDaemon(true);
        // busyThread不停的运行
        Thread busyThread = new Thread(new BusyRunner(), "BusyThread");
        busyThread.setDaemon(true);
        sleepThread.start();
        busyThread.start();
        // 休眠5秒，让sleepThread和busyThread充分运行
        TimeUnit.SECONDS.sleep(5);
        sleepThread.interrupt();
        busyThread.interrupt();
        System.out.println("SleepThread interrupted is " + sleepThread.isInterrupted());
        System.out.println("BusyThread interrupted is " + busyThread.isInterrupted());
        // 防止sleepThread和busyThread立刻退出
        TimeUnit.SECONDS.sleep(2);
    }

    static class SleepRunner implements Runnable {
        @Override
        public void run() {
            while (true) {
                SleepUtils.second(10);
            }
        }
    }

    static class BusyRunner implements Runnable {
        @Override
        public void run() {
            while (true) {
            }
        }
    }
}
/**
SleepThread interrupted is false
BusyThread interrupted is true
**/
```

​	从 结果 可以 看出， 抛出 InterruptedException 的 线程 SleepThread， 其中 断 标识 位 被 清除 了， 而 一直 忙碌 运作 的 线程 BusyThread， 中断 标识 位 没有 被 清除。

#### 4.2.4 过期的suspend()、resume()、和stop()

​	暂停、恢复和停止操作。

```java
public class Deprecated {
    @SuppressWarnings("deprecation")
    public static void main(String[] args) throws Exception {
        DateFormat format = new SimpleDateFormat("HH:mm:ss");
        Thread printThread = new Thread(new Runner(), "PrintThread");
        printThread.setDaemon(true);
        printThread.start();
        TimeUnit.SECONDS.sleep(3);
        // 将PrintThread进行暂停，输出内容工作停止
        printThread.suspend();
        System.out.println("main suspend PrintThread at " + format.format(new Date()));
        TimeUnit.SECONDS.sleep(20);
        // 将PrintThread进行恢复，输出内容继续
        printThread.resume();
        System.out.println("main resume PrintThread at " + format.format(new Date()));
        TimeUnit.SECONDS.sleep(3);
        // 将PrintThread进行终止，输出内容停止
        printThread.stop();
        System.out.println("main stop PrintThread at " + format.format(new Date()));
        TimeUnit.SECONDS.sleep(3);
    }

    static class Runner implements Runnable {
        @Override
        public void run() {
            DateFormat format = new SimpleDateFormat("HH:mm:ss");
            while (true) {
                System.out.println(Thread.currentThread().getName() + " Run at " + format.format(new Date()));
                SleepUtils.second(1);
            }
        }
    }
}
```

​	在执行过程中，PrintThread运行了3秒，随后被暂停，3秒后恢复，最后3秒被终止。

​	不建议使用的原因主要有：以suspend()为例，在调用后，线程不会释放已经占有的资源。容易引发死锁。

#### 4.2.5 安全地终止线程

​	可以中断或者利用一个boolean变量来控制是否是否需要停止任务并终止该线程。

```java
public class Shutdown {
    public static void main(String[] args) throws Exception {
        Runner one = new Runner();
        Thread countThread = new Thread(one, "CountThread");
        countThread.start();
        // 睡眠1秒，main线程对CountThread进行中断，使CountThread能够感知中断而结束
        TimeUnit.SECONDS.sleep(1);
        countThread.interrupt();
        Runner two = new Runner();
        countThread = new Thread(two, "CountThread");
        countThread.start();
        // 睡眠1秒，main线程对Runner two进行取消，使CountThread能够感知on为false而结束
        TimeUnit.SECONDS.sleep(1);
        two.cancel();
    }

    private static class Runner implements Runnable {
        private long             i;

        private volatile boolean on = true;

        @Override
        public void run() {
            while (on && !Thread.currentThread().isInterrupted()) {
                i++;
            }
            System.out.println("Count i = " + i);
        }

        public void cancel() {
            on = false;
        }
    }
}
```

### 4.3 线程间通信

#### 4.3.1 volatile和synchronized关键字

​	volatile关键字可以让所有线程感知到变化。过多使用会降低程序的执行效率。

​	关键字synchronized保证了线程对变量访问的可见性和排他性。中， 对于 同步 块 的 实现 使用 了 monitorenter 和 monitorexit 指令， 而 同步 方法 则是 依靠 方法 修饰 符 上 的 ACC_SYNCHRONIZED 来 完成 的。

![](https://pic.imgdb.cn/item/5fac92e41cd1bbb86b340eb7.jpg)

#### 4.3.2 等待/通知机制

​	以下代码可以完成消费者的工作

```java
while (value != desire) { 
	Thread. sleep( 1000); 
} 
doSomething();
```

​	存在如下问题

​	1) 难以确保及时性。

​	2) 难以降低开销。

​	使用Object的通知/等待方法可以解决该问题。

![](https://pic.imgdb.cn/item/5fac950b1cd1bbb86b348bb8.jpg)

```java
public class WaitNotify {
    static boolean flag = true;
    static Object  lock = new Object();

    public static void main(String[] args) throws Exception {
        Thread waitThread = new Thread(new Wait(), "WaitThread");
        waitThread.start();
        TimeUnit.SECONDS.sleep(1);

        Thread notifyThread = new Thread(new Notify(), "NotifyThread");
        notifyThread.start();
    }

    static class Wait implements Runnable {
        public void run() {
            // 加锁，拥有lock的Monitor
            synchronized (lock) {
                // 当条件不满足时，继续wait，同时释放了lock的锁
                while (flag) {
                    try {
                        System.out.println(Thread.currentThread() + " flag is true. wait @ "
                                           + new SimpleDateFormat("HH:mm:ss").format(new Date()));
                        lock.wait();
                    } catch (InterruptedException e) {
                    }
                }
                // 条件满足时，完成工作
                System.out.println(Thread.currentThread() + " flag is false. running @ "
                                   + new SimpleDateFormat("HH:mm:ss").format(new Date()));
            }
        }
    }

    static class Notify implements Runnable {
        public void run() {
            // 加锁，拥有lock的Monitor
            synchronized (lock) {
                // 获取lock的锁，然后进行通知，通知时不会释放lock的锁，
                // 直到当前线程释放了lock后，WaitThread才能从wait方法中返回
                System.out.println(Thread.currentThread() + " hold lock. notify @ " + new SimpleDateFormat("HH:mm:ss").format(new Date()));
                lock.notifyAll();
                flag = false;
                SleepUtils.second(5);
            }
            // 再次加锁
            synchronized (lock) {
                System.out.println(Thread.currentThread() + " hold lock again. sleep @ "
                                   + new SimpleDateFormat("HH:mm:ss").format(new Date()));
                SleepUtils.second(5);
            }
        }
    }
}
/**
Thread[WaitThread,5,main] flag is true. wait @ 09:58:16
Thread[NotifyThread,5,main] hold lock. notify @ 09:58:17
Thread[NotifyThread,5,main] hold lock again. sleep @ 09:58:22
Thread[WaitThread,5,main] flag is false. running @ 09:58:27
**/
```

1） 使用 wait()、 notify() 和 notifyAll() 时 需要 先 对调 用 对象 加锁。 

2） 调用 wait() 方法 后， 线程 状态 由 RUNNING 变为 WAITING， 并将 当前 线程 放置 到 对象 的 等待 队列。 

3） notify() 或 notifyAll() 方法 调 用后， 等待 线程 依旧 不会 从 wait() 返回， 需要 调用 notify() 或 notifAll() 的 线程 释放 锁 之后， 等待 线程 才有 机会 从 wait() 返回。 

4） notify() 方法 将 等待 队列 中的 一个 等待 线程 从 等待 队列 中 移到 同步 队列 中， 而 notifyAll() 方法 则是 将 等待 队列 中 所有 的 线程 全部 移到 同步 队列， 被 移动 的 线程 状态 由 WAITING 变为 BLOCKED。 

5） 从 wait() 方法 返回 的 前提 是 获得 了 调用 对象 的 锁。

#### 4.3.3 等待/通知的经典范式

等待 方 遵循 如下 原则。 

1） 获取 对象 的 锁。 

2） 如果 条件 不满足， 那么 调用 对象 的 wait() 方法， 被 通知 后仍 要 检查 条件。 

3） 条件 满足 则 执行 对应 的 逻辑。

![](https://pic.imgdb.cn/item/5fac99901cd1bbb86b359aaf.jpg)

通知 方 遵循 如下 原则。 

1） 获得 对象 的 锁。 

2） 改变 条件。 

3） 通知 所有 等待 在 对象 上 的 线程。

![](https://pic.imgdb.cn/item/5fac99af1cd1bbb86b35a529.jpg)

#### 4.3.4 管道输入/输出流

​	管道 输入/ 输出 流 和 普通 的 文件 输入/ 输出 流 或者 网络 输入/ 输出 流 不同 之处 在于， 它 主要 用于 线程 之间 的 数据 传输， 而 传输 的 媒介 为 内存。

```java
public class Piped {

    public static void main(String[] args) throws Exception {
        PipedWriter out = new PipedWriter();
        PipedReader in = new PipedReader();
        // 将输出流和输入流进行连接，否则在使用时会抛出IOException
        out.connect(in);

        Thread printThread = new Thread(new Print(in), "PrintThread");
        printThread.start();
        int receive = 0;
        try {
            while ((receive = System.in.read()) != -1) {
                out.write(receive);
            }
        } finally {
            out.close();
        }
    }

    static class Print implements Runnable {
        private PipedReader in;

        public Print(PipedReader in) {
            this.in = in;
        }

        public void run() {
            int receive = 0;
            try {
                while ((receive = in.read()) != -1) {
                    System.out.print((char) receive);
                }
            } catch (IOException ex) {
            }
        }
    }
}
```

#### 4.3.5 Thread.join()的使用

​	含义是：当前线程A等待thread终止之后才从thread.join()返回。还提供了`joing(long mills)`和`join(long millis, int nanos)`。两个超时方法表示，如果线程thread在给定的超时时间里没有终止，那么将会从该超时方法中返回。

```java
public class Join {
    public static void main(String[] args) throws Exception {
        Thread previous = Thread.currentThread();
        for (int i = 0; i < 10; i++) {
            // 每个线程拥有前一个线程的引用，需要等待前一个线程终止，才能从等待中返回
            Thread thread = new Thread(new Domino(previous), String.valueOf(i));
            thread.start();
            previous = thread;
        }

        TimeUnit.SECONDS.sleep(5);
        System.out.println(Thread.currentThread().getName() + " terminate.");
    }

    static class Domino implements Runnable {
        private Thread thread;

        public Domino(Thread thread) {
            this.thread = thread;
        }

        public void run() {
            try {
                thread.join();
            } catch (InterruptedException e) {
            }
            System.out.println(Thread.currentThread().getName() + " terminate.");
        }
    }
}
/**
main terminate.
0 terminate.
1 terminate.
2 terminate.
3 terminate.
4 terminate.
5 terminate.
6 terminate.
7 terminate.
8 terminate.
9 terminate.
**/
```

每个线程终止的前提是前驱线程的终止。

```java
public final synchronized void join(long millis)
throws InterruptedException {
    long base = System.currentTimeMillis();
    long now = 0;

    if (millis < 0) {
        throw new IllegalArgumentException("timeout value is negative");
    }

    if (millis == 0) {
        while (isAlive()) {
            wait(0);
        }
    } else {
        while (isAlive()) {
            long delay = millis - now;
            if (delay <= 0) {
                break;
            }
            wait(delay);
            now = System.currentTimeMillis() - base;
        }
    }
}
```

当线程终止时，会调用线程自身的notifyAll()方法，会通知所有等待在该现场对象上的线程。

#### 4.3.6 ThreadLocal的使用

​	ThreadLocal，即线程变量，是一个以ThreadLocal对象为键、任意对象为值的存储结构。这个结构被附带在线程上。

```java
public class Profiler {
    // 第一次get()方法调用时会进行初始化（如果set方法没有调用），每个线程会调用一次
    private static final ThreadLocal<Long> TIME_THREADLOCAL = new ThreadLocal<Long>() {
                                                                protected Long initialValue() {
                                                                    return System.currentTimeMillis();
                                                                }
                                                            };

    public static final void begin() {
        TIME_THREADLOCAL.set(System.currentTimeMillis());
    }

    public static final long end() {
        return System.currentTimeMillis() - TIME_THREADLOCAL.get();
    }

    public static void main(String[] args) throws Exception {
        Profiler.begin();
        TimeUnit.SECONDS.sleep(1);
        System.out.println("Cost: " + Profiler.end() + " mills");
    }
}
```

### 4.4 线程应用实例

#### 4.4.1 等待超时模式

​	如果方法能够在给定的时间段内得到结果，那么立刻放回，反之，超时返回默认结果。

![](https://pic.imgdb.cn/item/5facd2571cd1bbb86b449194.jpg)

​	等待超时模式就是在等待/通知范式基础上加了超时控制。

#### 4.4.2 一个简单的数据库连接池示例	

​	通过构造函数初始化连接的最大上限，通过一个双向队列来维护连接。调用放需要先调用fetchConnection(long) 方法来指定在多少毫秒内超时获取连接，当连接使用完成后，需要调用releaseConnection(Connection)方法将连接放回线程池。

```java
public class ConnectionPool {

    private LinkedList<Connection> pool = new LinkedList<Connection>();

    public ConnectionPool(int initialSize) {
        if (initialSize > 0) {
            for (int i = 0; i < initialSize; i++) {
                pool.addLast(ConnectionDriver.createConnection());
            }
        }
    }

    public void releaseConnection(Connection connection) {
        if (connection != null) {
            synchronized (pool) {
                // 添加后需要进行通知，这样其他消费者能够感知到链接池中已经归还了一个链接
                pool.addLast(connection);
                pool.notifyAll();
            }
        }
    }

    // 在mills内无法获取到连接，将会返回null
    public Connection fetchConnection(long mills) throws InterruptedException {
        synchronized (pool) {
            // 完全超时
            if (mills <= 0) {
                while (pool.isEmpty()) {
                    pool.wait();
                }

                return pool.removeFirst();
            } else {
                long future = System.currentTimeMillis() + mills;
                long remaining = mills;
                while (pool.isEmpty() && remaining > 0) {
                    pool.wait(remaining);
                    remaining = future - System.currentTimeMillis();
                }

                Connection result = null;
                if (!pool.isEmpty()) {
                    result = pool.removeFirst();
                }
                return result;
            }
        }
    }
}
```

#### 4.4.3 线程池技术及其示例

​	![](https://pic.imgdb.cn/item/5facda8a1cd1bbb86b46c915.jpg)

```java
public class DefaultThreadPool<Job extends Runnable> implements ThreadPool<Job> {
    // 线程池最大限制数
    private static final int      MAX_WORKER_NUMBERS     = 10;
    // 线程池默认的数量
    private static final int      DEFAULT_WORKER_NUMBERS = 5;
    // 线程池最小的数量
    private static final int      MIN_WORKER_NUMBERS     = 1;
    // 这是一个工作列表，将会向里面插入工作
    private final LinkedList<Job> jobs                   = new LinkedList<Job>();
    // 工作者列表
    private final List<Worker>    workers                = Collections.synchronizedList(new ArrayList<Worker>());
    // 工作者线程的数量
    private int                   workerNum              = DEFAULT_WORKER_NUMBERS;
    // 线程编号生成
    private AtomicLong            threadNum              = new AtomicLong();

    public DefaultThreadPool() {
        initializeWokers(DEFAULT_WORKER_NUMBERS);
    }

    public DefaultThreadPool(int num) {
        workerNum = num > MAX_WORKER_NUMBERS ? MAX_WORKER_NUMBERS : num < MIN_WORKER_NUMBERS ? MIN_WORKER_NUMBERS : num;
        initializeWokers(workerNum);
    }

    public void execute(Job job) {
        if (job != null) {
            // 添加一个工作，然后进行通知
            synchronized (jobs) {
                jobs.addLast(job);
                jobs.notify();
            }
        }
    }

    public void shutdown() {
        for (Worker worker : workers) {
            worker.shutdown();
        }
    }

    public void addWorkers(int num) {
        synchronized (jobs) {
            // 限制新增的Worker数量不能超过最大值
            if (num + this.workerNum > MAX_WORKER_NUMBERS) {
                num = MAX_WORKER_NUMBERS - this.workerNum;
            }
            initializeWokers(num);
            this.workerNum += num;
        }
    }

    public void removeWorker(int num) {
        synchronized (jobs) {
            if (num >= this.workerNum) {
                throw new IllegalArgumentException("beyond workNum");
            }
            // 按照给定的数量停止Worker
            int count = 0;
            while (count < num) {
                workers.get(count).shutdown();
                count++;
            }
            this.workerNum -= count;
        }
    }

    public int getJobSize() {
        return jobs.size();
    }

    // 初始化线程工作者
    private void initializeWokers(int num) {
        for (int i = 0; i < num; i++) {
            Worker worker = new Worker();
            workers.add(worker);
            Thread thread = new Thread(worker, "ThreadPool-Worker-" + threadNum.incrementAndGet());
            thread.start();
        }
    }

    // 工作者，负责消费任务
    class Worker implements Runnable {
        // 是否工作
        private volatile boolean running = true;

        public void run() {
            while (running) {
                Job job = null;
                synchronized (jobs) {
                    // 如果工作者列表是空的，那么就wait
                    while (jobs.isEmpty()) {
                        try {
                            jobs.wait();
                        } catch (InterruptedException ex) {
                            // 感知到外部对WorkerThread的中断操作，返回
                            Thread.currentThread().interrupt();
                            return;
                        }
                    }
                    // 取出一个Job
                    job = jobs.removeFirst();
                }
                if (job != null) {
                    try {
                        job.run();
                    } catch (Exception ex) {
                        // 忽略Job执行中的Exception
                    }
                }
            }
        }

        public void shutdown() {
            running = false;
        }
    }
}
```

​	线程 池 的 本质 就是 使用 了 一个 线程 安全 的 工作 队列 连接 工作者 线程 和 客户 端线 程， 客户 端 线程 将 任务 放入 工作队 列 后 便 返回， 而 工作者 线程 则 不断 地 从 工作队 列 上 取出 工作 并 执行。

#### 4.4.4 一个机遇线程池技术的简单Web服务器

```java
public class SimpleHttpServer {
    // 处理HttpRequest的线程池
    static ThreadPool<HttpRequestHandler> threadPool = new DefaultThreadPool<HttpRequestHandler>(11);
    // SimpleHttpServer的根路径
    static String                         basePath;
    static ServerSocket                   serverSocket;
    // 服务监听端口
    static int                            port       = 8080;

    public static void setPort(int port) {
        if (port > 0) {
            SimpleHttpServer.port = port;
        }
    }

    public static void setBasePath(String basePath) {
        if (basePath != null && new File(basePath).exists() && new File(basePath).isDirectory()) {
            SimpleHttpServer.basePath = basePath;
        }
    }

    // 启动SimpleHttpServer
    public static void start() throws Exception {
        serverSocket = new ServerSocket(port);
        Socket socket = null;
        while ((socket = serverSocket.accept()) != null) {
            // 接收一个客户端Socket，生成一个HttpRequestHandler，放入线程池执行
            threadPool.execute(new HttpRequestHandler(socket));
        }
        serverSocket.close();
    }

    static class HttpRequestHandler implements Runnable {

        private Socket socket;

        public HttpRequestHandler(Socket socket) {
            this.socket = socket;
        }

        @Override
        public void run() {
            String line = null;
            BufferedReader br = null;
            BufferedReader reader = null;
            PrintWriter out = null;
            InputStream in = null;
            try {
                reader = new BufferedReader(new InputStreamReader(socket.getInputStream()));
                String header = reader.readLine();
                // 由相对路径计算出绝对路径
                String filePath = basePath + header.split(" ")[1];
                out = new PrintWriter(socket.getOutputStream());
                // 如果请求资源的后缀为jpg或者ico，则读取资源并输出
                if (filePath.endsWith("jpg") || filePath.endsWith("ico")) {
                    in = new FileInputStream(filePath);
                    ByteArrayOutputStream baos = new ByteArrayOutputStream();
                    int i = 0;
                    while ((i = in.read()) != -1) {
                        baos.write(i);
                    }

                    byte[] array = baos.toByteArray();
                    out.println("HTTP/1.1 200 OK");
                    out.println("Content-Type: image/jpeg");
                    out.println("Content-Length: " + array.length);
                    out.println("");
                    socket.getOutputStream().write(array, 0, array.length);
                } else {
                    br = new BufferedReader(new InputStreamReader(new FileInputStream(filePath)));
                    out = new PrintWriter(socket.getOutputStream());
                    out.println("HTTP/1.1 200 OK");
                    out.println("Content-Type: text/html; charset=UTF-8");
                    out.println("");
                    while ((line = br.readLine()) != null) {
                        out.println(line);
                    }
                }
                out.flush();
            } catch (Exception ex) {
                out.println("HTTP/1.1 500");
                out.println("");
                out.flush();
            } finally {
                close(br, in, reader, out, socket);
            }
        }
    }

    // 关闭流或者Socket
    private static void close(Closeable... closeables) {
        if (closeables != null) {
            for (Closeable closeable : closeables) {
                try {
                    closeable.close();
                } catch (Exception ex) {
                    // 忽略
                }
            }
        }
    }
}
```

## 第5章 Java中的锁

### 5.1 Lock接口

```java
public class LockUseCase {
    public void lock() {
        Lock lock = new ReentrantLock();
        lock.lock();
        try {
        } finally {
            lock.unlock();
        }
    }
}
```

![](https://pic.imgdb.cn/item/5facdc951cd1bbb86b4757f4.jpg)

LockAPI如表所示：

![](https://pic.imgdb.cn/item/5facdcf71cd1bbb86b477557.jpg)

### 5.2 队列同步器

//todo 不是很明白

​	队列同步器AbstractQueuedSynchronizer, 是用来构建锁或其他同步组件的框架。它使用了一个int成员变量表示同步状态，通过内置的FIFO队列来完成资源获取线程的排队工作。

​	同步 器 的 主要 使用 方式 是 继承， 子类 通过 继承 同步 器 并 实现 它的 抽象 方法 来 管理 同步 状态， 在 抽象 方法 的 实现 过程中 免不了 要对 同步 状态 进行 更改， 这时 就 需要 使用 同步 器 提供 的 3 个 方法（ getState()、 setState( int newState) 和 compareAndSetState( int expect, int update)） 来 进行 操作， 因为 它们 能够 保证 状态 的 改变 是 安全 的。

#### 5.2.1 队列同步器的接口与示例

​	同步器的设计是基于模板模式的。

​	重写 同步 器 指定 的 方法 时， 需要 使用 同步 器 提供 的 如下 3 个 方法 来 访问 或 修改 同步 状态。 

* getState()： 获取 当前 同步 状态。 
* setState( int newState)： 设置 当前 同步 状态。 
* compareAndSetState( int expect, int update)： 使用 CAS 设置 当前 状态， 该 方法 能够 保证 状态 设置 的 原子 性。

![](https://pic.imgdb.cn/item/5face15f1cd1bbb86b48bf2e.jpg)





![](https://pic.imgdb.cn/item/5face1711cd1bbb86b48c384.jpg)

​	同步 器 提供 的 模板 方法 基本上 分为 3 类： 独占 式 获取 与 释放 同步 状态、 共享 式 获取 与 释放 同步 状态 和 查询 同步 队列 中的 等待 线程 情况。 自定义 同步 组件 将使 用 同步 器 提供 的 模板 方法 来 实现 自己的 同步 语义。

​	顾名思义， 独占 锁 就是 在 同一 时刻 只能 有一个 线程 获取 到 锁， 而 其他 获取 锁 的 线程 只能 处于 同步 队列 中 等待， 只有 获取 锁 的 线程 释 放了 锁， 后继 的 线程 才能 够 获取 锁。

​	

```java
public class Mutex implements Lock {
    // 静态内部类，自定义同步器
    private static class Sync extends AbstractQueuedSynchronizer {
        private static final long serialVersionUID = -4387327721959839431L;

        // 是否处于占用状态
        protected boolean isHeldExclusively() {
            return getState() == 1;
        }

        // 当状态为0的时候获取锁
        public boolean tryAcquire(int acquires) {
            assert acquires == 1; // Otherwise unused
            if (compareAndSetState(0, 1)) {
                setExclusiveOwnerThread(Thread.currentThread());
                return true;
            }
            return false;
        }

        // 释放锁，将状态设置为0
        protected boolean tryRelease(int releases) {
            assert releases == 1; // Otherwise unused
            if (getState() == 0)
                throw new IllegalMonitorStateException();
            setExclusiveOwnerThread(null);
            setState(0);
            return true;
        }

        // 返回一个Condition，每个condition都包含了一个condition队列
        Condition newCondition() {
            return new ConditionObject();
        }
    }

    // 仅需要将操作代理到Sync上即可
    private final Sync sync = new Sync();

    public void lock() {
        sync.acquire(1);
    }

    public boolean tryLock() {
        return sync.tryAcquire(1);
    }

    public void unlock() {
        sync.release(1);
    }

    public Condition newCondition() {
        return sync.newCondition();
    }

    public boolean isLocked() {
        return sync.isHeldExclusively();
    }

    public boolean hasQueuedThreads() {
        return sync.hasQueuedThreads();
    }

    public void lockInterruptibly() throws InterruptedException {
        sync.acquireInterruptibly(1);
    }

    public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException {
        return sync.tryAcquireNanos(1, unit.toNanos(timeout));
    }
```

#### 5.2.2 队列同步器的实现分析

**1.同步队列**

​	![](https://pic.imgdb.cn/item/5facec7d1cd1bbb86b4bcfc2.jpg)

![](https://pic.imgdb.cn/item/5facec8c1cd1bbb86b4bd483.jpg)

**2.独占式同步状态获取与释放**

​	通过 调用 同步 器 的 acquire( int arg) 方法 可以 获取 同步 状态， 该 方法 对 中断 不 敏感， 也就是 由于 线程 获取 同步 状态 失败 后进 入 同步 队列 中， 后续 对 线程 进行 中断 操作 时， 线程 不会 从 同步 队列 中 移出。

![](https://pic.imgdb.cn/item/5facee981cd1bbb86b4c8d6a.jpg)

![](https://pic.imgdb.cn/item/5faceed11cd1bbb86b4c9e82.jpg)

在 enq( final Node node) 方法 中， 同步 器 通过“ 死 循环” 来 保证 节点 的 正确 添加， 在“ 死 循环” 中 只有 通过 CAS 将 节点 设置 成为 尾 节点 之后， 当前 线程 才能 从 该 方法 返回， 否则， 当前 线程 不断 地 尝试 设置。

![](https://pic.imgdb.cn/item/5facef931cd1bbb86b4cd4cd.jpg)

​	在 acquireQueued( final Node node, int arg) 方法 中， 当前 线程 在“ 死 循环” 中 尝试 获取 同步 状态， 而 只有 前驱 节点 是 头 节点 才能 够 尝试 获取 同步 状态，

​	第一， 头 节点 是 成功 获取 到 同步 状态 的 节点， 而 头 节点 的 线程 释放 了 同步 状态 之后， 将会 唤醒 其后 继 节点， 后继 节点 的 线程 被 唤醒 后 需要 检查 自己的 前驱 节点 是否 是 头 节点。

​	第二， 维护 同步 队列 的 FIFO 原则。 该 方法 中， 节点 自 旋 获取 同步 状态 的 行为 如图所示。

![](https://pic.imgdb.cn/item/5facf1cf1cd1bbb86b4d7336.jpg)

 acquire( int arg) 方法 调用 流程如下：

![](https://pic.imgdb.cn/item/5facf2791cd1bbb86b4da082.jpg)

release代码如下：

![](https://pic.imgdb.cn/item/5facf3031cd1bbb86b4dcde8.jpg)

​	在 获取 同步 状态 时， 同步 器 维护 一个 同步 队列， 获取 状态 失败 的 线程 都会 被 加入 到 队列 中 并在 队列 中 进行 自 旋； 移出 队列（ 或 停止 自 旋） 的 条件 是 前驱 节点 为 头 节点 且 成功 获取 了 同步 状态。 在 释放 同步 状态 时， 同步 器 调用 tryRelease( int arg) 方法 释放 同步 状态， 然后 唤醒 头 节点 的 后继 节点。

**3. 共享式同步状态获取与释放**

​	共享 式 获取 与 独占 式 获取 最主要 的 区别 在于 同一 时刻 能否 有多 个 线程 同时 获取 到 同步 状态。

![](https://pic.imgdb.cn/item/5facf43e1cd1bbb86b4e3d7e.jpg)

通过 调用 同步 器 的 acquireShared( int arg) 方法 可以 共享 式 地 获取 同步 状态。

```java
public final void acquireShared(int arg) {
    if (tryAcquireShared(arg) < 0)
        doAcquireShared(arg);
}
private void doAcquireShared(int arg) {
        final Node node = addWaiter(Node.SHARED);
        boolean failed = true;
        try {
            boolean interrupted = false;
            for (;;) {
                final Node p = node.predecessor();
                if (p == head) {
                    int r = tryAcquireShared(arg);
                    if (r >= 0) {
                        setHeadAndPropagate(node, r);
                        p.next = null; // help GC
                        if (interrupted)
                            selfInterrupt();
                        failed = false;
                        return;
                    }
                }
                if (shouldParkAfterFailedAcquire(p, node) &&
                    parkAndCheckInterrupt())
                    interrupted = true;
            }
        } finally {
            if (failed)
                cancelAcquire(node);
        }
}
```

​	在 acquireShared( int arg) 方法 中， 同步 器 调用 tryAcquireShared( int arg) 方法 尝试 获取 同步 状态， tryAcquireShared( int arg) 方法 返回 值 为 int 类型， 当 返回 值 大于 等于 0 时， 表示 能够 获取 到 同步 状态。 因此， 在 共享 式 获 取的 自 旋 过程中， 成功 获取 到 同步 状态 并 退 出自 旋 的 条件 就是 tryAcquireShared( int arg) 方法 返回 值 大于 等于 0。 可以 看到， 在 doAcquireShared( int arg) 方法 的 自 旋 过程中， 如果 当前 节点 的 前驱 为 头 节点 时， 尝试 获取 同步 状态， 如果 返回 值 大于 等于 0， 表示 该 次 获取 同步 状态 成功 并从 自 旋 过程中 退出。

​	releaseShared也类似。

```java
public final boolean releaseShared(int arg) {
    if (tryReleaseShared(arg)) {
        doReleaseShared();
        return true;
    }
    return false;
}
```

**4.独占式超时获取同步状态**

​	通过 调用 同步 器 的 doAcquireNanos( int arg, long nanosTimeout) 方法 可以 超时 获取 同步 状态， 即在 指定 的 时间 段 内 获取 同步 状态， 如果 获取 到 同步 状态 则 返回 true， 否则， 返回 false。 该 方法 提供 了 传统 Java 同步 操作（ 比如 synchronized 关键字） 所不 具备 的 特性。

```java
rivate boolean doAcquireNanos(int arg, long nanosTimeout)
        throws InterruptedException {
    if (nanosTimeout <= 0L)
        return false;
    final long deadline = System.nanoTime() + nanosTimeout;
    final Node node = addWaiter(Node.EXCLUSIVE);
    boolean failed = true;
    try {
        for (;;) {
            final Node p = node.predecessor();
            if (p == head && tryAcquire(arg)) {
                setHead(node);
                p.next = null; // help GC
                failed = false;
                return true;
            }
            nanosTimeout = deadline - System.nanoTime();
            if (nanosTimeout <= 0L)
                return false;
            if (shouldParkAfterFailedAcquire(p, node) &&
                nanosTimeout > spinForTimeoutThreshold)
                LockSupport.parkNanos(this, nanosTimeout);
            if (Thread.interrupted())
                throw new InterruptedException();
        }
    } finally {
        if (failed)
            cancelAcquire(node);
    }
}
```

![](https://pic.imgdb.cn/item/5facf7be1cd1bbb86b4f5235.jpg)

**5. 自定义同步组件——TwinsLock**

​	只允许两个线程同时访问，超过两个线程将被阻塞。

```java
public class TwinsLock implements Lock {
    private final Sync sync = new Sync(2);

    private static final class Sync extends AbstractQueuedSynchronizer {
        private static final long serialVersionUID = -7889272986162341211L;

        Sync(int count) {
            if (count <= 0) {
                throw new IllegalArgumentException("count must large than zero.");
            }
            setState(count);
        }

        public int tryAcquireShared(int reduceCount) {
            for (;;) {
                int current = getState();
                int newCount = current - reduceCount;
                if (newCount < 0 || compareAndSetState(current, newCount)) {
                    return newCount;
                }
            }
        }

        public boolean tryReleaseShared(int returnCount) {
            for (;;) {
                int current = getState();
                int newCount = current + returnCount;
                if (compareAndSetState(current, newCount)) {
                    return true;
                }
            }
        }

        final ConditionObject newCondition() {
            return new ConditionObject();
        }
    }

    public void lock() {
        sync.acquireShared(1);
    }

    public void unlock() {
        sync.releaseShared(1);
    }

    public void lockInterruptibly() throws InterruptedException {
        sync.acquireSharedInterruptibly(1);
    }

    public boolean tryLock() {
        return sync.tryAcquireShared(1) >= 0;
    }

    public boolean tryLock(long time, TimeUnit unit) throws InterruptedException {
        return sync.tryAcquireSharedNanos(1, unit.toNanos(time));
    }

    @Override
    public Condition newCondition() {
        return sync.newCondition();
    }
}
```

### 5.3 重入锁

​	ReentrantLock 虽然 没能 像 synchronized 关键字 一样 支持 隐式 的 重 进入， 但是 在 调用 lock() 方法 时， 已经 获取 到 锁 的 线程， 能够 再次 调用 lock() 方法 获取 锁 而 不被 阻塞。

​	事实上， 公平 的 锁 机制 往往 没有 非 公平 的 效率高， 但是， 并不是 任何 场景 都是 以 TPS 作为 唯一 的 指标， 公平 锁 能够 减少“ 饥饿” 发生 的 概率， 等待 越 久 的 请求 越是 能够 得到 优先 满足。

**1. 实现重进入**

​	需要解决如下问题：

1） 线程再次获取锁。

2） 锁的最终释放。

```java
final boolean nonfairTryAcquire(int acquires) {
    final Thread current = Thread.currentThread();
    int c = getState();
    if (c == 0) {
        if (compareAndSetState(0, acquires)) {
            setExclusiveOwnerThread(current);
            return true;
        }
    }
    else if (current == getExclusiveOwnerThread()) {
        int nextc = c + acquires;
        if (nextc < 0) // overflow
            throw new Error("Maximum lock count exceeded");
        setState(nextc);
        return true;
    }
    return false;
}
```

```java
protected final boolean tryRelease(int releases) {
    int c = getState() - releases;
    if (Thread.currentThread() != getExclusiveOwnerThread())
        throw new IllegalMonitorStateException();
    boolean free = false;
    if (c == 0) {
        free = true;
        setExclusiveOwnerThread(null);
    }
    setState(c);
    return free;
}
```

**2. 公平与非公平获取锁的区别**

​	公平 性 与否 是 针对 获取 锁 而言 的， 如果 一个 锁 是 公平 的， 那么 锁 的 获取 顺序 就应 该 符合 请求 的 绝对 时间 顺序， 也就是 FIFO。

```java
protected final boolean tryAcquire(int acquires) {
    final Thread current = Thread.currentThread();
    int c = getState();
    if (c == 0) {
        if (!hasQueuedPredecessors() &&
            compareAndSetState(0, acquires)) {
            setExclusiveOwnerThread(current);
            return true;
        }
    }
    else if (current == getExclusiveOwnerThread()) {
        int nextc = c + acquires;
        if (nextc < 0)
            throw new Error("Maximum lock count exceeded");
        setState(nextc);
        return true;
    }
    return false;
}
```

### 5.4 读写锁

​	之前提到的都是排他锁。读写锁可以在同一时刻允许多个读线程访问。读写锁维护了一对锁，一个读锁，一个写锁，通过分离读锁和写锁，使得并发性相比一般的排他锁有了很大提升。

​	Java并发包提供读写锁的实现是ReentrantReadWriteLock

![](https://pic.imgdb.cn/item/5fad04291cd1bbb86b531ea5.jpg)

#### 5.4.1 读写锁的接口与示例

![](https://pic.imgdb.cn/item/5fad04421cd1bbb86b532627.jpg)

​	

```java
public class Cache {
    private static final Map<String, Object>    map = new HashMap<String, Object>();
    private static final ReentrantReadWriteLock rwl = new ReentrantReadWriteLock();
    private static final Lock                   r   = rwl.readLock();
    private static final Lock                   w   = rwl.writeLock();

    public static final Object get(String key) {
        r.lock();
        try {
            return map.get(key);
        } finally {
            r.unlock();
        }
    }

    public static final Object put(String key, Object value) {
        w.lock();
        try {
            return map.put(key, value);
        } finally {
            w.unlock();
        }
    }

    public static final void clear() {
        w.lock();
        try {
            map.clear();
        } finally {
            w.unlock();
        }
    }
}
```

​	需要 获取 读 锁， 这使 得 并发 访问 该 方法 时不 会被 阻塞。 写 操作 put( String key, Object value) 方法 和 clear() 方法， 在 更新 HashMap 时必 须 提前 获取 写 锁， 当 获取 写 锁 后， 其他 线程 对于 读 锁 和 写 锁 的 获取 均被 阻塞， 而 只 有写 锁 被 释放 之后， 其他 读写 操作 才能 继续。

#### 5.4.2 读写锁的实现分析

**1. 读写状态的设计**

​	读写锁将变量切分成了两个部分，高16位表示读，低16位表示写。

![](https://pic.imgdb.cn/item/5fad073b1cd1bbb86b53f4d8.jpg)

**2. 写锁的获取与释放**

```java
protected final boolean tryAcquire(int acquires) {
    /*
     * Walkthrough:
     * 1. If read count nonzero or write count nonzero
     *    and owner is a different thread, fail.
     * 2. If count would saturate, fail. (This can only
     *    happen if count is already nonzero.)
     * 3. Otherwise, this thread is eligible for lock if
     *    it is either a reentrant acquire or
     *    queue policy allows it. If so, update state
     *    and set owner.
     */
    Thread current = Thread.currentThread();
    int c = getState();
    int w = exclusiveCount(c);
    if (c != 0) {
        // (Note: if c != 0 and w == 0 then shared count != 0)
        if (w == 0 || current != getExclusiveOwnerThread())
            return false;
        if (w + exclusiveCount(acquires) > MAX_COUNT)
            throw new Error("Maximum lock count exceeded");
        // Reentrant acquire
        setState(c + acquires);
        return true;
    }
    if (writerShouldBlock() ||
        !compareAndSetState(c, c + acquires))
        return false;
    setExclusiveOwnerThread(current);
    return true;
}
```

**3. 读锁的获取与释放**

```java
protected final int tryAcquireShared(int unused) {
            /*
             * Walkthrough:
             * 1. If write lock held by another thread, fail.
             * 2. Otherwise, this thread is eligible for
             *    lock wrt state, so ask if it should block
             *    because of queue policy. If not, try
             *    to grant by CASing state and updating count.
             *    Note that step does not check for reentrant
             *    acquires, which is postponed to full version
             *    to avoid having to check hold count in
             *    the more typical non-reentrant case.
             * 3. If step 2 fails either because thread
             *    apparently not eligible or CAS fails or count
             *    saturated, chain to version with full retry loop.
             */
            Thread current = Thread.currentThread();
            int c = getState();
            if (exclusiveCount(c) != 0 &&
                getExclusiveOwnerThread() != current)
                return -1;
            int r = sharedCount(c);
            if (!readerShouldBlock() &&
                r < MAX_COUNT &&
                compareAndSetState(c, c + SHARED_UNIT)) {
                if (r == 0) {
                    firstReader = current;
                    firstReaderHoldCount = 1;
                } else if (firstReader == current) {
                    firstReaderHoldCount++;
                } else {
                    HoldCounter rh = cachedHoldCounter;
                    if (rh == null || rh.tid != getThreadId(current))
                        cachedHoldCounter = rh = readHolds.get();
                    else if (rh.count == 0)
                        readHolds.set(rh);
                    rh.count++;
                }
                return 1;
            }
            return fullTryAcquireShared(current);
        }
```

​	在 tryAcquireShared( int unused) 方法 中， 如果 其他 线程 已经 获取 了 写 锁， 则 当前 线程 获取 读 锁 失败， 进入 等待 状态。 如果 当前 线程 获取 了 写 锁 或者 写 锁 未被 获取， 则 当前 线程（ 线程 安全， 依靠 CAS 保证） 增加 读 状态， 成功 获取 读 锁。 读 锁 的 每次 释放（ 线程 安全 的， 可能有 多个 读 线程 同时 释放 读 锁） 均 减少 读 状态， 减 少的 值 是（ 1<< 16）。

**4. 锁降级**

​	锁降级指的是写锁降级为读锁。如果 当前 线程 拥 有写 锁， 然后 将其 释放， 最后 再 获取 读 锁， 这种 分段 完成 的 过程 不能 称之为 锁 降级。 锁 降级 是指 把持 住（ 当前 拥有 的） 写 锁， 再 获取 到 读 锁， 随后 释放（ 先前 拥有 的） 写 锁 的 过程。

​	RentrantReadWriteLock 不支持 锁 升级（ 把持 读 锁、 获取 写 锁， 最后 释放 读 锁 的 过程）。

### 5.5 LockSupport工具

​	![](https://pic.imgdb.cn/item/5fad0fbb1cd1bbb86b565de9.jpg)

### 5.6 Condition接口

![](https://pic.imgdb.cn/item/5fad10b91cd1bbb86b56b653.jpg)

#### 5.6.1 Condition接口与示例

```java
public class ConditionUseCase {
    Lock      lock      = new ReentrantLock();
    Condition condition = lock.newCondition();

    public void conditionWait() throws InterruptedException {
        lock.lock();
        try {
            condition.await();
        } finally {
            lock.unlock();
        }
    }

    public void conditionSignal() throws InterruptedException {
        lock.lock();
        try {
            condition.signal();
        } finally {
            lock.unlock();
        }
    }
}
```

![](https://pic.imgdb.cn/item/5fad12241cd1bbb86b5712cf.jpg)

通过BoundedQueue来深入了解Condition的使用方式

```java
public class BoundedQueue<T> {
    private Object[]  items;
    // 添加的下标，删除的下标和数组当前数量
    private int       addIndex, removeIndex, count;
    private Lock      lock     = new ReentrantLock();
    private Condition notEmpty = lock.newCondition();
    private Condition notFull  = lock.newCondition();

    public BoundedQueue(int size) {
        items = new Object[size];
    }

    // 添加一个元素，如果数组满，则添加线程进入等待状态，直到有“空位”
    public void add(T t) throws InterruptedException {
        lock.lock();
        try {
            while (count == items.length)
                notFull.await();
            items[addIndex] = t;
            if (++addIndex == items.length)
                addIndex = 0;
            ++count;
            notEmpty.signal();
        } finally {
            lock.unlock();
        }
    }

    // 由头部删除一个元素，如果数组空，则删除线程进入等待状态，直到有新添加元素
    @SuppressWarnings("unchecked")
    public T remove() throws InterruptedException {
        lock.lock();
        try {
            while (count == 0)
                notEmpty.await();
            Object x = items[removeIndex];
            if (++removeIndex == items.length)
                removeIndex = 0;
            --count;
            notFull.signal();
            return (T) x;
        } finally {
            lock.unlock();
        }
    }
}
```

#### 5.6.2 Condition的实现分析

ConditionObject 是 同步 器 AbstractQueuedSynchronizer 的 内 部类， 因为 Condition 的 操作 需要 获取 相 关联 的 锁， 所以 作为 同步 器 的 内 部类 也 较为 合理。

**1. 等待队列**

​	等待 队列 是 一个 FIFO 的 队列， 在 队列 中的 每个 节点 都 包含 了 一个 线程 引用， 该 线程 就是 在 Condition 对象 上 等待 的 线程， 如果 一个 线程 调用 了 Condition. await() 方法， 那么 该 线程 将会 释放 锁、 构造 成 节点 加入 等待 队列 并进 入 等待 状态。

![](https://pic.imgdb.cn/item/5fad14301cd1bbb86b57aa66.jpg)

![](https://pic.imgdb.cn/item/5fad14591cd1bbb86b57b2fe.jpg)

**2. 等待**

​	调用 该 方法 的 线程 成功 获取 了 锁 的 线程， 也就是 同步 队列 中的 首 节点， 该 方法 会 将 当前 线程 构造 成 节点 并 加入 等待 队列 中， 然后 释放 同步 状态， 唤醒 同步 队列 中的 后继 节点， 然后 当前 线程 会 进入 等待 状态。

```java
public final void await() throws InterruptedException {
    if (Thread.interrupted())
        throw new InterruptedException();
    Node node = addConditionWaiter();
    int savedState = fullyRelease(node);
    int interruptMode = 0;
    while (!isOnSyncQueue(node)) {
        LockSupport.park(this);
        if ((interruptMode = checkInterruptWhileWaiting(node)) != 0)
            break;
    }
    if (acquireQueued(node, savedState) && interruptMode != THROW_IE)
        interruptMode = REINTERRUPT;
    if (node.nextWaiter != null) // clean up if cancelled
        unlinkCancelledWaiters();
    if (interruptMode != 0)
        reportInterruptAfterWait(interruptMode);
}
```

**3. 通知**

![](https://pic.imgdb.cn/item/5fad15801cd1bbb86b57f3bb.jpg)

```java
public final void signal() {
    if (!isHeldExclusively())
        throw new IllegalMonitorStateException();
    Node first = firstWaiter;
    if (first != null)
        doSignal(first);
}
```

Condition 的 signalAll() 方法， 相当于 对等 待 队列 中的 每个 节点 均 执行 一次 signal() 方法， 效果 就是 将 等待 队列 中 所有 节点 全部 移动 到 同步 队列 中， 并 唤醒 每个 节点 的 线程。

## 第6章 Java并发容器和框架

### 6.1 ConcurrentHashMap的实现原理与使用

#### 6.1.1 为什么要使用ConcurrentHashMap

(1) 线程不安全的hashMap

​	在JDK1.7中，hashmap在并发情况下容易引起死循环。

(2) 效率低下的HashTable

(3) ConcurrentHashMap的锁分段技术(jdk1.7)可有效提升并发访问率

#### 6.1.2 ConcurrentHashMap的结构

![](https://pic.imgdb.cn/item/5fadd3e41cd1bbb86b7c47a9.jpg)

#### 6.1.3 ConcurrentHashMap的初始化

​	初始化方法是通过`initialCapacity`,`loadFactor`和`concurrencyLevel`几个参数来初始化segement数组、段偏移量segmentShift、段掩码segmentMask和每个segment的HashEntry数组。

**1. 初始化segments数组**

![](https://pic.imgdb.cn/item/5fadd5471cd1bbb86b7c7a59.jpg)

​	由上 面的 代码 可知， segments 数组 的 长度 ssize 是 通过 concurrencyLevel 计算 得出 的。 为了 能 通过 按 位 与 的 散 列 算法 来 定位 segments 数组 的 索引， 必须 保证 segments 数组 的 长度 是 2 的 N 次方（ power- of- two size）， 所以 必须 计算 出 一个 大于 或 等于 concurrencyLevel 的 最小 的 2 的 N 次方 值 来作 为 segments 数组 的 长度。 假如 concurrencyLevel 等于 14、 15 或 16， ssize 都会 等于 16， 即 容器 里 锁 的 个数 也是 16。

**2. 初始化segmentShift和segmentMask**


​	这 两个 全局 变量 需要 在 定位 segment 时 的 散 列 算法 里 使用， sshift 等于 ssize 从 1 向左 移位 的 次数， 在 默认 情况下 concurrencyLevel 等于 16， 1 需要 向左 移位 移动 4 次， 所以 sshift 等于 4。4。 segmentShift 用于 定位 参与 散 列 运算 的位数， segmentShift 等于 32 减 sshift， 所以 等于 28， 这里 之所 以用 32 是因为 ConcurrentHashMap 里 的 hash() 方法 输出 的 最大数 是 32 位 的， 后面 的 测试 中 我们 可以 看到 这点。 segmentMask 是 散 列 运算 的 掩 码， 等于 ssize 减 1， 即 15， 掩 码 的 二进制 各 个位 的 值 都是 1。 因为 ssize 的 最大 长度 是 65536， 所以 segmentShift 最大值 是 16， segmentMask 最大值 是 65535， 对应 的 二进制 是 16 位， 每个 位 都是 1。

**3. 初始化每个segment**

![](https://pic.imgdb.cn/item/5fadd6ca1cd1bbb86b7cb21c.jpg)

#### 6.1.4 定位Segment

​	ConcurrentHashMap 会首 先使 用 Wang/ Jenkins hash 的 变种 算法 对 元素 的 hashCode 进行 一次 再 散 列。

![](https://pic.imgdb.cn/item/5fadd74a1cd1bbb86b7cc7f8.jpg)

​	通过以下散列算法定位segment

![](https://pic.imgdb.cn/item/5fadd8351cd1bbb86b7cead8.jpg)

#### 6.1.5 ConcurrentHashMap的操作

**1. get操作**

![](https://pic.imgdb.cn/item/5fadd8ab1cd1bbb86b7cfd21.jpg)

​	效率很高，不需要加锁，除非读到的值为空。 get 方法 里 将要 使用 的 共享 变量 都 定义 成 volatile 类型， 如用 于 统计 当前 Segement 大小 的 count 字段 和 用于 存储 值 的 HashEntry 的 value。 定义 成 volatile 的 变量， 能够 在 线程 之间 保持 可见 性， 能够 被 多 线程 同时 读， 并且 保证 不会 读到 过期 的 值， 但是 只能 被单 线程 写（ 有 一种 情况 可以 被 多 线程 写， 就是 写入 的 值 不 依赖于 原值）， 在 get 操作 里 只 需要 读 不需要 写 共享 变量 count 和 value， 所以 可以 不用 加锁。

**2. put操作**

(1) 是否需要扩容

​	超过阈值就进行扩容。

(2) 如何扩容

​	在 扩容 的 时候， 首先 会 创建 一个 容量 是 原来 容量 两倍 的 数组， 然后 将 原 数组 里 的 元素 进行 再 散 列 后 插入 到 新的 数组 里。 为了 高效， ConcurrentHashMap 不会 对 整个 容器 进行 扩容， 而 只对 某个 segment 进行 扩容。

**3. size操作**

​	最 安全 的 做法 是在 统计 size 的 时候 把 所有 Segment 的 put、 remove 和 clean 方法 全部 锁住， 但是 这种 做法 显然 非常 低效。

​	因为 在 累加 count 操作 过程中， 之前 累加 过 的 count 发生 变化 的 几率 非常 小， 所以 ConcurrentHashMap 的 做法 是 先 尝试 2 次 通过 不 锁住 Segment 的 方式 来 统计 各个 Segment 大小， 如果 统计 的 过程中， 容器 的 count 发生了 变化， 则 再 采用 加锁 的 方式 来 统计 所有 Segment 的 大小。

https://www.jianshu.com/p/c0642afe03e0

### 6.2 ConcurrentLinkedQueue

​	实现一个线程安全的队列有两种方式：

* 阻塞算法：可以用出队入队都一个锁或者单独的两个锁来实现。

* 非阻塞算法：使用循环CAS的方式来实现。

  

​	ConcurrentLinkedQueue 是一 个 基于 链接 节点 的 无 界线 程 安全 队列， 它 采用 先进 先出 的 规则 对 节点 进行 排序， 当 我们 添加 一个 元素 的 时候， 它 会 添加 到 队列 的 尾部； 当 我们 获取 一个 元素 时， 它 会 返回 队列 头部 的 元素。 它 采用 了“ wait- free” 算法（ 即 CAS 算法） 来 实现， 该 算法 在 Michael& Scott 算法 上进 行了 一些 修改。

#### 6.2.1 ConcurrentLinkedQueue的结构

![](https://pic.imgdb.cn/item/5fadec351cd1bbb86b80a56b.jpg)

​	有head节点和tail节点组成。默认情况head存储的元素为空，tail节点等于head节点。

![](https://pic.imgdb.cn/item/5fadee271cd1bbb86b810924.jpg)

#### 6.2.2 入队列

**1. 入队列的过程**

​	入队 列 就是 将 入队 节点 添加 到 队列 的 尾部。

![](https://pic.imgdb.cn/item/5fadef7f1cd1bbb86b814905.jpg)

![](https://pic.imgdb.cn/item/5fadf03d1cd1bbb86b8170aa.jpg)

**2. 定位尾结点**

​	tail节点并不是总是尾结点，所以每次入队都必须通过tail来找到尾结点。获取 tail 节点 的 next 节点 需要 注意 的 是 p 节点 等于 p 的 next 节点 的 情况， 只有 一种 可能 就是 p 节点 和 p 的 next 节点 都 等于 空， 表示 这个 队列 刚 初始化， 正 准备 添加 节点， 所以 需要 返回 head 节点。 获取 p 节点 的 next 节点 代码 如下。

​	

**3. 设置入队节点为尾节点**

​	p. casNext（ null， n） 方法 用于 将 入队 节点 设置 为 当前 队列 尾 节点 的 next 节点， 如果 p 是 null， 表示 p 是 当前 队列 的 尾 节点， 如果 不为 null， 表示 有其 他 线程 更新 了 尾 节点， 则需 要 重新 获取 当前 队列 的 尾 节点。

**4. HOPS的设计意图**

​	doug lea的将入队节点设置为尾节点的代码还是有点复杂。如下方式也是否可行？

![](https://pic.imgdb.cn/item/5fae0c3a1cd1bbb86b876ded.jpg)

​	但是这么做有个缺点，每次都需要使用CAS更新tail节点。如果能减少CAS更新tail的次数，就能提高入队的效率。所以使用hops变量来控制并减少tail节点的更新频率，并不是每次节点入队都将tail节点更新成尾节点，而是当tail节点和尾节点的距离大于等于常量HOPS的值(默认为1)才更新tail节点。

#### 6.2.3 出队列

​	![](https://pic.imgdb.cn/item/5fae10a61cd1bbb86b88642b.jpg)

```java
public E poll() {
    restartFromHead:
    for (;;) {
        for (Node<E> h = head, p = h, q;;) {
            E item = p.item;

            if (item != null && p.casItem(item, null)) {
                // Successful CAS is the linearization point
                // for item to be removed from this queue.
                if (p != h) // hop two nodes at a time
                    updateHead(h, ((q = p.next) != null) ? q : p);
                return item;
            }
            else if ((q = p.next) == null) {
                updateHead(h, p);
                return null;
            }
            else if (p == q)
                continue restartFromHead;
            else
                p = q;
        }
    }
}
```

​	首先 获取 头 节点 的 元素， 然后 判断 头 节点 元素 是否 为 空， 如 果为 空， 表示 另外 一个 线程 已经 进行 了 一次 出 队 操作 将 该 节点 的 元素 取走， 如果不 为 空， 则 使用 CAS 的 方式 将 头 节点 的 引用 设置 成 null， 如果 CAS 成功， 则 直接 返回 头 节点 的 元素， 如果 不成功， 表示 另外 一个 线程 已经 进行 了 一次 出 队 操作 更新 了 head 节点， 导致 元素 发生了 变化， 需要 重新 获取 头 节点。

### 6.3 Java中的阻塞队列

#### 6.3.1 什么是阻塞队列

​	支持两个附加操作的队列：

1) 支持阻塞的插入方法：意思是当队列满时，会阻塞插入元素的线程，直到队列不满。

2) 支持阻塞的移除方法：意思是在队列为空时，获取元素的线程会等待队列变为非空。

![](https://pic.imgdb.cn/item/5fae12c91cd1bbb86b88cd37.jpg)

#### 6.3.2 Java的阻塞队列

java7提供了7个阻塞队列：

* ArrayBlockingQueue： 一个 由 数组 结构 组成 的 有 界 阻塞 队列。 
* LinkedBlockingQueue： 一个 由 链 表 结构 组成 的 有 界 阻塞 队列。 
* PriorityBlockingQueue： 一个 支持 优先级 排序 的 无 界 阻塞 队列。 
* DelayQueue： 一个 使用 优先级 队列 实现 的 无 界 阻塞 队列。 
* SynchronousQueue： 一个 不 存储 元素 的 阻塞 队列。 
* LinkedTransferQueue： 一个 由 链 表 结构 组成 的 无 界 阻塞 队列。 
* LinkedBlockingDeque： 一个 由 链 表 结构 组成 的 双向 阻塞 队列。

**1. ArrayBlockingQueue**

​	默认 情况下 不保 证 线程 公平 的 访问 队列， 所谓 公平 访问 队列 是指 阻塞 的 线程， 可以 按照 阻塞 的 先后 顺序 访问 队列， 即 先 阻塞 线程 先 访问 队列。 非 公平 性 是对 先 等待 的 线程 是非 公平 的， 当 队列 可用 时， 阻塞 的 线程 都可以 争夺 访问 队列 的 资格， 有可能 先 阻塞 的 线程 最后 才 访问 队列。 为了 保证 公平 性， 通常 会 降低 吞吐量。 我们 可以 使用 以下 代码 创建 一个 公平 的 阻塞 队列。

![](https://pic.imgdb.cn/item/5fae13ec1cd1bbb86b8904e0.jpg)

**2. LinkedBlockingQueue**

​	此 队列 的 默认 和 最大 长度 为 Integer. MAX_ VALUE。 此 队列 按照 先进 先出 的 原则 对 元素 进行 排序。

**3. PriorityBlockingQueue**

​	默认 情况下 元素 采取 自然 顺序 升序 排列。 也可以 自定义 类 实现 compareTo() 方法 来 指定 元素 排序 规则， 或者 初始化 PriorityBlockingQueue 时， 指定 构造 参数 Comparator 来 对 元素 进行 排序。 需要 注意 的 是 不能 保证 同 优先级 元素 的 顺序。

**4. DelayQueue**

​	DelayQueue 是一 个 支持 延时 获取 元素 的 无 界 阻塞 队列。 队列 使用 PriorityQueue 来 实现。 队列 中的 元素 必须 实现 Delayed 接口， 在 创建 元素 时 可以 指定 多久 才能 从 队列 中 获取 当前 元素。 只有 在 延迟 期满 时 才能 从 队列 中 提取 元素。

* 缓存系统的设计：能获取到就代表缓存有效期过了。
* 定时任务调度。

(1) 如何实现Delayed接口

​	有3步

​	第一步： 在 对象 创建 的 时候， 初始化 基本 数据。 使用 time 记录 当前 对象 延迟 到 什么时候 可以 使用， 使用 sequenceNumber 来 标识 元素 在 队列 中的 先后 顺序。

![](https://pic.imgdb.cn/item/5fae22d71cd1bbb86b8c443f.jpg)

​	第二步：实现getDelay方法，该方法返回当前元素还需要延时多长时间，单位是纳秒。

![](https://pic.imgdb.cn/item/5fae22d71cd1bbb86b8c443f.jpg)

​	第三步：实现compareTo方法来指定元素顺序。

(2) 如何实现延时阻塞队列

​	延时 阻塞 队列 的 实现 很 简单， 当 消费者 从 队列 里 获取 元素 时， 如果 元素 没有 达到 延时 时间， 就 阻塞 当前 线程。

![](https://pic.imgdb.cn/item/5fae24bc1cd1bbb86b8cc4c5.jpg)

**5. SynchronousQueue **

​	它是一个不存储元素的阻塞队列。每一个put操作必须等待一个take操作，否则不能继续添加。

​	它 支持 公平 访问 队列。 默认 情况下 线程 采用 非 公平 性 策略 访问 队列。 使用 以下 构造 方法 可以 创建 公平 性 访问 的 SynchronousQueue， 如果 设置 为 true， 则 等待 的 线程 会 采用 先进 先出 的 顺序 访问 队列。

![](https://pic.imgdb.cn/item/5fae25031cd1bbb86b8cd20d.jpg)

**6. LinkedTransferQueue**

​	LinkedTransferQueue 是 一个 由 链 表 结构 组成 的 无 界 阻塞 TransferQueue 队列。 相对于 其他 阻塞 队列， LinkedTransferQueue 多了 tryTransfer 和 transfer 方法。

(1) transfer方法

​	如果 当前 有 消费者 正在 等待 接收 元素（ 消费者 使用 take() 方法 或 带 时间 限制 的 poll() 方法 时）， transfer 方法 可以 把 生产者 传入 的 元素 立刻 transfer（ 传输） 给 消费者。 如果 没有 消费者 在等 待 接收 元素， transfer 方法 会 将 元素 存放 在 队列 的 tail 节点， 并 等到 该 元素 被 消费者 消费 了 才 返回。

(2) tryTransfer方法

​	tryTransfer 方法 是 用来 试探 生产者 传入 的 元素 是否 能 直接 传给 消费者。 如果 没有 消费者 等待 接收 元素， 则 返回 false。 和 transfer 方法 的 区别 是 tryTransfer 方法 无论 消费者 是否 接收， 方法 立即 返回， 而 transfer 方法 是 必须 等到 消费者 消费 了 才 返回。

​	对于 带有 时间 限制 的 tryTransfer（ E e， long timeout， TimeUnit unit） 方法， 试图 把 生产者 传入 的 元素 直接 传给 消费者， 但是 如果 没有 消费者 消费 该 元素 则 等待 指定 的 时间 再 返回， 如果 超时 还没 消费 元素， 则 返回 false， 如果 在 超时 时间 内 消费 了 元素， 则 返回 true。

**7. LinkedBlockingDeque**

​	LinkedBlockingDeque 是 一个 由 链 表 结构 组成 的 双向 阻塞 队列。 所谓 双向 队列 指的 是 可以 从 队列 的 两端 插入 和 移出 元素。 双向 队列 因为 多了 一个 操作 队列 的 入口， 在 多 线程 同时 入队 时， 也就 减少 了 一半 的 竞争。 相比 其他 的 阻塞 队列， LinkedBlockingDeque 多了 addFirst、 addLast、 offerFirst、 offerLast、 peekFirst 和 peekLast 等 方法， 以 First 单词 结尾 的 方法， 表示 插入、 获取（ peek） 或 移 除 双端 队列 的 第一个 元素。 以 Last 单词 结尾 的 方法，表示 插入、 获取 或 移 除 双端 队列 的 最后 一个 元素。 另外， 插入 方法 add 等 同于 addLast， 移 除 方法 remove 等效 于 removeFirst。 但是 take 方法 却 等 同于 takeFirst， 不知道 是不是 JDK 的 bug， 使用 时 还是 用带 有 First 和 Last 后缀 的 方法 更 清楚。 

​	在 初始化 LinkedBlockingDeque 时 可以 设置 容量 防止 其 过度 膨胀。 另外， 双向 阻塞 队列 可以 运用 在“ 工作 窃取” 模式 中。

#### 6.3.3 阻塞队列的实现原理

**使用通知模式实现**

​	所谓 通知 模式， 就是 当 生产者 往 满的 队列 里 添加 元素 时会 阻塞 住 生产者， 当 消费者 消费 了 一个 队列 中的 元素 后， 会 通知 生产者 当前 队列 可用。 通过 查看 JDK 源 码 发现 ArrayBlockingQueue 使用 了 Condition 来 实现，

```java
public void put(E e) throws InterruptedException {
    checkNotNull(e);
    final ReentrantLock lock = this.lock;
    lock.lockInterruptibly();
    try {
        while (count == items.length)
            notFull.await();
        enqueue(e);
    } finally {
        lock.unlock();
    }
}
 public E take() throws InterruptedException {
        final ReentrantLock lock = this.lock;
        lock.lockInterruptibly();
        try {
            while (count == 0)
                notEmpty.await();
            return dequeue();
        } finally {
            lock.unlock();
        }
    }

```

其他略

### 6.4 Fork/Join框架

#### 6.4.1 什么是Fork/Join框架

​	 Fork 就是 把 一个 大 任务 切分 为 若干 子 任务 并行 的 执行， Join 就是 合并 这些 子 任务 的 执行 结果， 最后 得到 这个 大 任务 的 结果。

![](https://pic.imgdb.cn/item/5fae31921cd1bbb86b9170e8.jpg)

#### 6.4.2 工作窃取算法

​	在 这时 它们 会 访问 同一个 队列， 所以 为了 减少 窃取 任务 线程 和 被窃 取 任务 线程 之间 的 竞争， 通常 会使 用 双端 队列， 被窃 取 任务 线程 永远 从 双端 队列 的 头部 拿 任务 执行， 而 窃取 任务 的 线程 永远 从 双端 队列 的 尾部 拿 任务 执行。

![](https://pic.imgdb.cn/item/5fae39e01cd1bbb86b939135.jpg)

​	工作 窃取 算法 的 优点： 充分 利用 线程 进行 并行 计算， 减少 了 线程 间的 竞争。 

​	工作 窃取 算法 的 缺点： 在 某些 情况下 还是 存在 竞争， 比如 双端 队列 里 只有 一个 任务 时。 并且 该 算法 会 消耗 了 更多 的 系统 资源， 比如 创建 多个 线程 和 多个 双端 队列。

#### 6.4.3 Fork/Join框架的设计

1. 分割任务。
2. 执行任务合并结果。



​	Fork/Join使用两个类来完成以上两件事情。

* ForkJoinTask: 我们 要 使用 ForkJoin 框架， 必须 首先 创建 一个 ForkJoin 任务。 它 提供 在 任务 中 执行 fork() 和 join() 操作 的 机制。 通常 情况下， 我们 不需要 直接 继承 ForkJoinTask 类， 只需 要 继承 它的 子类， Fork/ Join 框架 提供 了 以下 两个 子类。
  * RecursiveAction： 用于 没有 返回 结果 的 任务。 
  * RecursiveTask： 用于 有 返回 结果 的 任务。
* ForkJoinPool: ForkJoinTask需要通过ForkJoinPool。

#### 6.4.4 使用Fork/Join框架

 

```java
/**
 * 计数器任务
 * 
 * @author tengfei.fangtf
 * @version $Id: CountTask.java, v 0.1 2015-8-1 上午12:00:29 tengfei.fangtf Exp $
 */
public class CountTask extends RecursiveTask<Integer> {

    private static final int THRESHOLD = 2; // 阈值
    private int              start;
    private int              end;

    public CountTask(int start, int end) {
        this.start = start;
        this.end = end;
    }

    @Override
    protected Integer compute() {
        int sum = 0;

        // 如果任务足够小就计算任务
        boolean canCompute = (end - start) <= THRESHOLD;
        if (canCompute) {
            for (int i = start; i <= end; i++) {
                sum += i;
            }
        } else {
            // 如果任务大于阈值，就分裂成两个子任务计算
            int middle = (start + end) / 2;
            CountTask leftTask = new CountTask(start, middle);
            CountTask rightTask = new CountTask(middle + 1, end);
            //执行子任务
            leftTask.fork();
            rightTask.fork();
            //等待子任务执行完，并得到其结果
            int leftResult = leftTask.join();
            int rightResult = rightTask.join();
            //合并子任务
            sum = leftResult + rightResult;
        }
        return sum;
    }

    public static void main(String[] args) {
        ForkJoinPool forkJoinPool = new ForkJoinPool();
        // 生成一个计算任务，负责计算1+2+3+4
        CountTask task = new CountTask(1, 4);
        // 执行一个任务
        Future<Integer> result = forkJoinPool.submit(task);
        try {
            System.out.println(result.get());
        } catch (InterruptedException e) {
        } catch (ExecutionException e) {
        }
    }

}
```

#### 6.4.5 Fork/Join框架的异常处理

​	ForkJoinTask 提供 了 isCompletedAbnormally() 方法 来 检查 任务 是否 已经 抛出 异常 或 已经 被 取消 了， 并且 可以 通过 ForkJoinTask 的 getException 方法 获取 异常。

![](https://pic.imgdb.cn/item/5fae3cc01cd1bbb86b942c4a.jpg)

#### 6.4.6 Fork/Join框架的实现原理

​	ForkJoinPool 由 ForkJoinTask 数组 和 ForkJoinWorkerThread 数组 组成， ForkJoinTask 数组 负责 将 存放 程序 提 交给 ForkJoinPool 的 任务， 而 ForkJoinWorkerThread 数组 负责 执行 这些 任务。

(1) ForkJoinTask的fork方法实现原理

​	![](https://pic.imgdb.cn/item/5fae3e841cd1bbb86b94a739.jpg)

![](https://pic.imgdb.cn/item/5fae3e9a1cd1bbb86b94abbc.jpg)

​	pushTask 方法 把 当前任务 存放 在 ForkJoinTask 数组 队列 里。 然后 再 调用 ForkJoinPool 的 signalWork() 方法 唤醒 或 创建 一个 工作 线程 来 执行任务。

(2) ForkJoinTask的join方法实现原理

​	![](https://pic.imgdb.cn/item/5fae3f3c1cd1bbb86b94cb8c.jpg)

首先， 它 调用 了 doJoin() 方法， 通过 doJoin() 方法 得到 当前任务 的 状态 来 判断 返回 什么 结果， 任务 状态 有 4 种： 已完成（ NORMAL）、 被 取消（ CANCELLED）、 信号（ SIGNAL） 和 出现 异常（ EXCEPTIONAL）。 

* 如果 任务 状态 是 已完成， 则 直接 返回 任务 结果。 
* 如果 任务 状态 是 被 取消， 则 直接 抛出 CancellationException。 
* 如果 任务 状态 是 抛出 异常， 则 直接 抛出 对应 的 异常。

```java
private int doJoin() {
    int s; Thread t; ForkJoinWorkerThread wt; ForkJoinPool.WorkQueue w;
    return (s = status) < 0 ? s :
        ((t = Thread.currentThread()) instanceof ForkJoinWorkerThread) ?
        (w = (wt = (ForkJoinWorkerThread)t).workQueue).
        tryUnpush(this) && (s = doExec()) < 0 ? s :
        wt.pool.awaitJoin(w, this, 0L) :
        externalAwaitDone();
}
```

​	在 doJoin() 方法 里， 首先 通过 查看 任务 的 状态， 看 任务 是否 已经 执行 完成， 如果 执行 完成， 则 直接 返回 任务 状态； 如果 没有 执行 完， 则从 任务 数组 里 取出 任务 并 执行。 如果 任务 顺利 执行 完成， 则 设置 任务 状态 为 NORMAL， 如果 出现 异常， 则 记录 异常， 并将 任务 状态 设置 为 EXCEPTIONAL。

## 第7章 Java的13个原子操作类

### 7.1 原子更新基本类

使用 原子 的 方式 更新 基本 类型， Atomic 包 提供 了 以下 3 个 类。 

* AtomicBoolean： 原子 更新 布尔 类型。 
* AtomicInteger： 原子 更新 整型。 
* AtomicLong： 原子 更新 长整型。

常用方法如下：

![](https://pic.imgdb.cn/item/5fae5f421cd1bbb86b9d1991.jpg)

http://ifeve.com/how-does-atomiclong-lazyset-work/。

```java
public class AtomicIntegerTest {

    static AtomicInteger ai = new AtomicInteger(1);

    public static void main(String[] args) {
        System.out.println(ai.getAndIncrement());
        System.out.println(ai.get());
    }

}
```

通过 代码， 我们 发现 Unsafe 只 提供 了 3 种 CAS 方法： compareAndSwapObject、 compare- AndSwapInt 和 compareAndSwapLong， 再看 AtomicBoolean 源 码， 发现 它是 先把 Boolean 转换 成 整型， 再 使用 compareAndSwapInt 进行 CAS， 所以 原子 更新 char、 float 和 double 变量 也可 以用 类似 的 思路 来 实现。

### 7.2 原子更新数组

​	通过 原子 的 方式 更新 数组 里 的 某个 元素， Atomic 包 提供 了 以下 4 个 类。

* AtomicIntegerArray： 原子 更新 整型 数组 里 的 元素。 
* AtomicLongArray： 原子 更新 长整型 数组 里 的 元素。 
* AtomicReferenceArray： 原子 更新 引用 类型 数组 里 的 元素。 
* AtomicIntegerArray 类 主要 是 提供 原子 的 方式 更新 数组 里 的 整型， 其 常用 方法 如下。 
  * int addAndGet（ int i， int delta）： 以 原子 方式 将 输入 值 与 数组 中 索引 i 的 元素 相加。 
  * boolean compareAndSet（ int i， int expect， int update）： 如果 当前 值 等于 预期 值， 则以 原子 方式 将 数组 位置 i 的 元素 设置 成 update 值。

```java
public class AtomicIntegerArrayTest {

    static int[]              value = new int[] { 1, 2 };

    static AtomicIntegerArray ai    = new AtomicIntegerArray(value);

    public static void main(String[] args) {
        ai.getAndSet(0, 3);
        System.out.println(ai.get(0));
        System.out.println(value[0]);
    }
	
}

/**
3
1
**/
```

​	需要 注意 的 是， 数组 value 通过 构造 方法 传递 进去， 然后 AtomicIntegerArray 会 将 当前 数组 复制 一份， 所 以当 AtomicIntegerArray 对 内部 的 数组 元素 进行 修改 时， 不会 影响 传入 的 数组。

### 7.3 原子更新引用类型

* AtomicReference： 原子 更新 引用 类型。 
* AtomicReferenceFieldUpdater： 原子 更新 引用 类型 里 的 字段。 
* AtomicMarkableReference： 原子 更新 带有 标记 位 的 引用 类型。 可以 原子 更新 一个 布尔 类型 的 标记 位 和 引用 类型。 构造 方法 是 AtomicMarkableReference（ V initialRef， boolean initialMark）。

```java
public class AtomicReferenceTest {

    public static AtomicReference<User> atomicUserRef = new AtomicReference<User>();

    public static void main(String[] args) {
        User user = new User("conan", 15);
        atomicUserRef.set(user);
        User updateUser = new User("Shinichi", 17);
        atomicUserRef.compareAndSet(user, updateUser);
        System.out.println(atomicUserRef.get().getName());
        System.out.println(atomicUserRef.get().getOld());
    }

    public static class User {
        private String name;
        private int    old;

        public User(String name, int old) {
            this.name = name;
            this.old = old;
        }

        public String getName() {
            return name;
        }

        public int getOld() {
            return old;
        }
    }
}
```

### 7.4 原子更新字段类

* AtomicIntegerFieldUpdater： 原子 更新 整型 的 字段 的 更新 器。 
* AtomicLongFieldUpdater： 原子 更新 长整型 字段 的 更新 器。 
* AtomicStampedReference： 原子 更新 带有 版本 号的 引用 类型。 该类 将 整 数值 与 引用 关联 起来， 可用 于 原子 的 更新 数据 和 数据 的 版 本号， 可以 解决 使用 CAS 进行 原子 更新 时 可能 出现 的 ABA 问题。

```java
public class AtomicIntegerFieldUpdaterTest {

    private static AtomicIntegerFieldUpdater<User> a = AtomicIntegerFieldUpdater.newUpdater(User.class, "old");

    public static void main(String[] args) {
        User conan = new User("conan", 10);
        System.out.println(a.getAndIncrement(conan));
        System.out.println(a.get(conan));
    }

    public static class User {
        private String      name;
        public volatile int old;

        public User(String name, int old) {
            this.name = name;
            this.old = old;
        }

        public String getName() {
            return name;
        }

        public int getOld() {
            return old;
        }
    }
}
/**
10
11
**/
```

## 第8章 Java中的并发工具类

### 8.1 等待多线程完成的CountDownLatch

​	允许一个或多个线程等待其他线程完成操作。

​	解析多个Excel里的多个Sheet数据。最简单的做法是使用join()。

```java
public class JoinCountDownLatchTest {

    public static void main(String[] args) throws InterruptedException {
        Thread parser1 = new Thread(new Runnable() {
            @Override
            public void run() {
            }
        });

        Thread parser2 = new Thread(new Runnable() {
            @Override
            public void run() {
                System.out.println("parser2 finish");
            }
        });

        parser1.start();
        parser2.start();
        parser1.join();
        parser2.join();
        System.out.println("all parser finish");
    }

}
```

​	使用CounDownLatch也可以实现join的功能，并且功能更多。

```java
public class CountDownLatchTest {

    static CountDownLatch c = new CountDownLatch(2);

    public static void main(String[] args) throws InterruptedException {
        new Thread(new Runnable() {
            @Override
            public void run() {
                System.out.println(1);
                c.countDown();
                System.out.println(2);
                c.countDown();
            }
        }).start();

        c.await();
        System.out.println("3");
    }

}
```

### 8.2 同步屏障CyclicBarrier

​	让一组线程达到一个屏障时同时被阻塞，直到最后一个线程到达屏障时，屏障才会开门。

#### 8.3.1 CyclicBarrier简介

```java
public class CyclicBarrierTest {

    static CyclicBarrier c = new CyclicBarrier(2);

    public static void main(String[] args) {
        new Thread(new Runnable() {

            @Override
            public void run() {
                try {
                    c.await();
                } catch (Exception e) {

                }
                System.out.println(1);
            }
        }).start();

        try {
            c.await();
        } catch (Exception e) {

        }
        System.out.println(2);
    }
}
```

还提供一个更高级的构造参数`CyclicBarrier(int parties, Runnable barrierAction)`, 用于在线程到达屏障时，优先执行barrierAction。

```java
public class CyclicBarrierTest2 {

    static CyclicBarrier c = new CyclicBarrier(2, new A());

    public static void main(String[] args) {
        new Thread(new Runnable() {

            @Override
            public void run() {
                try {
                    c.await();
                } catch (Exception e) {

                }
                System.out.println(1);
            }
        }).start();

        try {
            c.await();
        } catch (Exception e) {

        }
        System.out.println(2);
    }

    static class A implements Runnable {

        @Override
        public void run() {
            System.out.println(3);
        }

    }

}
```

#### 8.2.2 CyclicBarrier的应用场景

​	多线程计算，最后合并结果。

#### 8.2.3 CyclicBarrier和ContDownLatch的区别。

​	ContDownLatch只能使用一次，CyclicBarrier的计算器可以使用reset()重置。

​	还提供了：`getNumberWaiting`获取CyclicBarrier阻塞的线程数量，`isBroken()`用来了解阻塞线程室友被中断。

```java
public class CyclicBarrierTest3 {

    static CyclicBarrier c = new CyclicBarrier(2);

    public static void main(String[] args) throws InterruptedException, BrokenBarrierException {
        Thread thread = new Thread(new Runnable() {

            @Override
            public void run() {
                try {
                    c.await();
                } catch (Exception e) {
                }
            }
        });
        thread.start();
        thread.interrupt();
        try {
            c.await();
        } catch (Exception e) {
            System.out.println(c.isBroken());
        }
    }
}
```

### 8.3 控制并发线程数的Semaphore

​	用来控制同时访问特定资源的线程数量。

**1. 应用场景**

​	可以用于做流量控制，特别是公用资源有限的应用场景，比如数据库连接。

```java
public class SemaphoreTest {

    private static final int       THREAD_COUNT = 30;

    private static ExecutorService threadPool   = Executors.newFixedThreadPool(THREAD_COUNT);

    private static Semaphore       s            = new Semaphore(10);

    public static void main(String[] args) {
        for (int i = 0; i < THREAD_COUNT; i++) {
            threadPool.execute(new Runnable() {
                @Override
                public void run() {
                    try {
                        s.acquire();
                        System.out.println("save data");
                        s.release();
                    } catch (InterruptedException e) {
                    }
                }
            });
        }

        threadPool.shutdown();
    }
}
```

**2. 其他方法**

* intavailablePermits()： 返回 此 信号 量 中 当前 可用 的 许可证 数。 ·
* intgetQueueLength()： 返回 正在 等待 获取 许可证 的 线程 数。 
* booleanhasQueuedThreads()： 是否 有线 程 正在 等待 获取 许可证。 
* void reducePermits（ int reduction）： 减少 reduction 个 许可证， 是个 protected 方法。 
* Collection getQueuedThreads()： 返回 所有 等待 获取 许可证 的 线程 集合， 是个 protected 方法。

### 8.4 线程间交换数据的Exchanger

​	用于线程间数据交换。提供一个同步点，在这个同步点，两个线程可以交换批次的数据。

​	可以用于遗传算法，也可以用于校对工作。

```java
public class ExchangerTest {

    private static final Exchanger<String> exgr       = new Exchanger<String>();

    private static ExecutorService         threadPool = Executors.newFixedThreadPool(2);

    public static void main(String[] args) {

        threadPool.execute(new Runnable() {
            @Override
            public void run() {
                try {
                    String A = "银行流水A";// A录入银行流水数据
                    exgr.exchange(A);
                } catch (InterruptedException e) {
                }
            }
        });

        threadPool.execute(new Runnable() {
            @Override
            public void run() {
                try {
                    String B = "银行流水B";// B录入银行流水数据
                    String A = exgr.exchange("B");
                    System.out.println("A和B数据是否一致：" + A.equals(B) + "，A录入的是：" + A + "，B录入是：" + B);
                } catch (InterruptedException e) {
                }
            }
        });

        threadPool.shutdown();

    }
}
```

可以设置第二个参数来设置最大等待时长：`exgr.exchange("B", 10, TimeUnit.SECONDS)`。

## 第9章 Java中的线程池

​	3个好处：

* 降低资源消耗。
* 提高响应速度。
* 提高线程的可管理性。

### 9.1 线程池的实现原理

​	处理流程如下：

1. 线程池判断核心线程池里的线程是否都在执行任务。如果不是，则创建一个新的线程来执行任务。如果核心线程池里的线程都在执行任务，则进入下一个线程。
2. 线程池判断工作队列是否已经满了。如果工作队列没满，则将新提交的任务存储在这个队列里。如果满了，进入下个流程。
3. 线程池判断线程池的线程是否都处于工作状态，如果没有，则创建一个新的工作线程来执行任务。如果已经满了，则交给饱和策略来处理这个任务。

![](https://pic.imgdb.cn/item/5fbbab68b18d627113fabf16.jpg)

![](https://pic.imgdb.cn/item/5fbbab94b18d627113facbd9.jpg)

execute方法分为4种情况：

* 当先运行线程少于`corePoolSize`，则创建新线程来执行任务。

* 如果大于等于`corePoolSize`,则将任务加入`BlockingQueue`。

* 如果无法将任务加入`BlockingQueue`，则创建新的线程来处理任务(执行这一步骤需要获取全局锁)。

* 如果创建新建村将使当前运行的线程超出`maximumPoolSize`, 任务将被拒绝，并调`用RejectedExecutionHanlder.rejectedExecution()`方法。

  ![](https://pic.imgdb.cn/item/5fbbac71b18d627113fb0e2f.jpg)

  

工作线程：线程池创建线程时，会将线程封装成工作线程Worker，Work在执行完任务后，还会循环获取工作队列里的任务来执行。

![](https://pic.imgdb.cn/item/5fbbacfbb18d627113fb3b4a.jpg)

线程执行任务示意图如下：

![](https://pic.imgdb.cn/item/5fbbad15b18d627113fb4356.jpg)

分两种情况：

1. 在`execute()`方法中创建一个线程时，会让这个线程执行当前任务。
2. 这个线程执行完上图的1任务后，会反复从BlockingQueue获取任务来执行。

### 9.2 线程池的使用

#### 9.2.1 线程池的创建

​	![](https://pic.imgdb.cn/item/5fbbae86b18d627113fbbc0a.jpg)

1） corePoolSize（ 线程 池 的 基本 大小）： 当 提交 一个 任务 到 线程 池 时， 线程 池 会 创建 一个 线程 来 执行任务， 即使 其他 空闲 的 基本 线程 能够 执行 新任务 也会 创建 线程， 等到 需要 执行 的 任务 数 大于 线程 池 基本 大小 时 就不 再 创建。 如果 调用 了 线程 池 的 prestartAllCoreThreads() 方法， 线程 池 会 提前 创建 并 启动 所有 基本 线程。

2） runnableTaskQueue（ 任务 队列）： 用于 保存 等待 执行 的 任务 的 阻塞 队列。 可以 选择 以下 几个 阻塞 队列。 

* ArrayBlockingQueue： 是一 个 基于 数组 结构 的 有 界 阻塞 队列， 此 队列 按 FIFO（ 先进 先出） 原则 对 元素 进行 排序。 
* LinkedBlockingQueue： 一个 基于 链 表 结构 的 阻塞 队列， 此 队列 按 FIFO 排序 元素， 吞吐量 通常 要 高于 ArrayBlockingQueue。 静态 工厂 方法 Executors. newFixedThreadPool() 使用 了 这个 队列。 
* SynchronousQueue： 一个 不 存储 元素 的 阻塞 队列。 每个 插入 操作 必须 等到 另一个 线程 调用 移 除 操作， 否则 插入 操作 一直 处于 阻塞 状态， 吞吐量 通常 要 高于 Linked- BlockingQueue， 静态 工厂 方法 Executors. newCachedThreadPool 使用 了 这个 队列。 
* PriorityBlockingQueue： 一个 具有 优先级 的 无限 阻塞 队列。

3） maximumPoolSize（ 线程 池 最大 数量）： 线程 池 允许 创建 的 最大 线程 数。 如果 队列 满了， 并且 已 创建 的 线程数 小于 最大 线程 数， 则 线程 池 会 再创 建 新的 线程 执行任务。 值得注意 的 是， 如果 使 用了 无 界 的 任务 队列 这个 参数 就 没什么 效果。 4） ThreadFactory： 用于 设置 创建 线程 的 工厂， 可以 通过 线程 工厂 给 每个 创建 出来 的 线程 设置 更有意义 的 名字。 使用 开源 框架 guava 提供 的 ThreadFactoryBuilder 可以 快速 给 线程 池 里 的 线程 设置 有意义 的 名字， 代码 如下。

​	![](https://pic.imgdb.cn/item/5fbbaf24b18d627113fbf81d.jpg)

5） RejectedExecutionHandler（ 饱和 策略）： 当 队列 和 线程 池 都 满了， 说明 线程 池 处于 饱和状态， 那么 必须 采取 一种 策略 处理 提交 的 新任务。 这个 策略 默认 情况下 是 AbortPolicy， 表示 无法 处理 新任 务时 抛出 异常。 在 JDK 1. 5 中 Java 线程 池 框架 提供 了 以下 4 种 策略。

* AbortPolicy： 直接 抛出 异常。 
* CallerRunsPolicy： 只用 调用 者 所在 线程 来 运行 任务。 
* DiscardOldestPolicy： 丢弃 队列 里 最近 的 一个 任务， 并 执行 当前任务。 ·
* DiscardPolicy： 不 处理， 丢弃 掉。

 当然， 也可以 根据 应用 场景 需要 来 实现 RejectedExecutionHandler 接口 自定义 策略。 如 记录 日志 或 持久 化 存储 不能 处理 的 任务。

* keepAliveTime（ 线程 活动 保持 时间）： 线程 池 的 工作 线程 空闲 后， 保持 存活 的 时间。 所以， 如果 任务 很多， 并且 每个 任务 执行 的 时间 比 较短， 可以 调 大 时间， 提高 线程 的 利用率。 
* TimeUnit（ 线程 活动 保持 时间 的 单位）： 可选 的 单位 有天（ DAYS）、 小时（ HOURS）、 分钟（ MINUTES）、 毫秒（ MILLISECONDS）、 微秒（ MICROSECONDS， 千分之一 毫秒） 和 纳秒（ NANOSECONDS， 千分之一 微秒）。

#### 9.2.2 向线程池提交任务

​	分为execute()和submit()方法。

​	execute()用于提交不需要返回值的任务，无法判断是否执行成功。

![](https://pic.imgdb.cn/item/5fbbafd4b18d627113fc363b.jpg)

​	submit()方法用于提交需要返回值的任务，会返回一个future对象。

![](https://pic.imgdb.cn/item/5fbbaffeb18d627113fc45d5.jpg)

#### 9.2.3 关闭线程池

​	通过`shutDown`或`shutDownNow`方法来关闭。原理是遍历工作线程，逐个调用线程的`intterrupt`方法来中断线程，所以无法响应中断的任务可能永远无法终止。区别， shutdownNow 首先 将 线程 池 的 状态 设置 成 STOP， 然后 尝试 停止 所有 的 正在 执行 或 暂停 任务 的 线程， 并 返回 等待 执行任务 的 列表， 而 shutdown 只是 将 线程 池 的 状态 设置 成 SHUTDOWN 状态， 然后 中断 所有 没有 正在 执行任务 的 线程。

​	只要 调 用了 这 两个 关闭 方法 中的 任意 一个， isShutdown 方法 就会 返回 true。 当 所有 的 任务 都已 关闭 后， 才 表示 线程 池 关闭 成功， 这时 调用 isTerminaed 方法 会 返回 true。

#### 9.2.4 合理地配置线程池

​	首先需要分析任务特性。

* 任务性质：CPU密集型任务、IO密集型任务和混合型任务。
* 任务的优先级：高、中和低。
* 任务的执行时间：长、中和短。
* 任务的依赖性：是否依赖其他系统资源。



​	性质 不同 的 任务 可以 用不 同 规模 的 线程 池 分开 处理。 CPU 密集型 任务 应 配置 尽可能 小的 线程， 如 配置 Ncpu+ 1 个 线程 的 线程 池。 由于 IO 密集型 任务 线程 并不是 一直 在 执行任务， 则应 配置 尽可能 多的 线程， 如 2* Ncpu。 混合 型 的 任务， 如果 可以 拆分， 将其 拆分 成 一个 CPU 密集型 任务 和 一个 IO 密集型 任务， 只要 这 两个 任务 执行 的 时间 相差 不是 太大， 那么 分解 后 执行 的 吞吐量 将 高于 串行 执行 的 吞吐量。 如果 这 两个 任务 执行 时间 相差 太大， 则 没 必要 进行 分解。 可以 通过 Runtime. getRuntime(). availableProcessors() 方法 获得 当前 设备 的 CPU 个数。

​	优先级 不同 的 任务 可以 使用 优先级 队列 PriorityBlockingQueue 来 处理。 它 可以 让 优先级 高的 任务 先 执行。

* 建议使用有界队列。有 界 队列 能 增加 系统 的 稳定性 和 预警 能力， 可以 根据 需要 设 大一 点儿， 比如 几千。

  

#### 9.2.5 线程池的监控

​	可以通过线程池提供的参数进行监控。

* taskCount： 线程 池 需要 执行 的 任务 数量。 
* completedTaskCount： 线程 池 在 运行 过程中 已完成 的 任务 数量， 小于 或 等于 taskCount。 
* largestPoolSize： 线程 池 里 曾经 创建 过 的 最大 线程 数量。 通过 这个 数据 可以 知道 线程 池 是否 曾经 满 过。 如 该数 值 等于 线程 池 的 最大 大小， 则 表示 线程 池 曾经 满 过。 
* getPoolSize： 线程 池 的 线程 数量。 如果 线程 池 不 销毁 的 话， 线程 池 里 的 线程 不会 自动 销毁， 所以 这个 大小 只 增 不减。 
* getActiveCount： 获取 活动 的 线程 数。 通过 扩展 线程 池 进行 监控。 可以 通过 继承 线程 池 来自 定义 线程 池， 重写 线程 池 的 beforeExecute、 afterExecute 和 terminated 方法， 也可 以在 任务 执行 前、 执行 后 和 线程 池 关闭 前 执行 一些 代码 来 进行 监控。 例如， 监控 任务 的 平均 执行 时间、 最大 执行 时间 和 最小 执行 时间等。 这 几个 方法 在 线程 池 里 是 空 方法。

. 

## 第10章 Executor框架

​	JDK5开始，把工作单元与执行机制分离开来。工作单元包括Runnable和Callable，而执行机制由Executor框架提供。

### 10.1 Executor框架简介

#### 10.1.1 Executor框架的两级调度模型

​	在HotSpot VM线程模型中，Java线程被一对一映射为本地操作系统线程。Java线程启动时会创建一个本地操作系统线程；当该线程终止时，操作系统线程也会被回收。操作系统会调度所有线程并将它们分配给可用的CPU。

#### 10.1.2 Executor框架的结构与成员

![](https://pic1.imgdb.cn/item/5fbd1c85b18d6271134fc50f.jpg)

**1. Executor框架的结构**

​	主要由3大部分组成：

* 任务。 包括 被 执行任务 需要 实现 的 接口： Runnable 接口 或 Callable 接口。 
* 任务 的 执行。 包括 任务 执行 机制 的 核心 接口 Executor， 以及 继承 自 Executor 的 ExecutorService 接口。 Executor 框架 有两 个 关键 类 实现 了 ExecutorService 接口（ ThreadPoolExecutor 和 ScheduledThreadPoolExecutor）。 
* 异步 计算 的 结果。 包括 接口 Future 和 实现 Future 接口 的 FutureTask 类。



​	下面是这些类和接口的简介。

* Executor 是一 个 接口， 它是 Executor 框架 的 基础， 它将 任务 的 提交 与 任务 的 执行 分离 开来。 
* ThreadPoolExecutor 是 线程 池 的 核心 实现 类， 用来 执行 被 提交 的 任务。 
* ScheduledThreadPoolExecutor 是一 个 实现 类， 可以 在给 定的 延迟 后 运行 命令， 或者 定期 执行 命令。 ScheduledThreadPoolExecutor 比 Timer 更 灵活， 功能 更 强大。 
* Future 接口 和 实现 Future 接口 的 FutureTask 类， 代表 异步 计算 的 结果。 
* Runnable 接口 和 Callable 接口 的 实现 类， 都可以 被 ThreadPoolExecutor 或 ScheduledThreadPoolExecutor 执行。

![](https://pic1.imgdb.cn/item/5fbd1d20b18d6271134ff727.jpg)

![](https://pic1.imgdb.cn/item/5fbd1d92b18d6271135019f6.jpg)

​	工具类Executors可以把一个Runnable对象封装为一个Callable对象对象（ Executors. callable（ Runnable task） 或 Executors. callable（ Runnable task， Object resule））。

​	如果 执行 ExecutorService. submit（…）， ExecutorService 将 返回 一个 实现 Future 接口 的 对象。由于 FutureTask 实现 了 Runnable， 程序员 也可以 创建 FutureTask， 然后 直接 交给 ExecutorService 执行。

**2. Executor框架的成员**

​	ThreadPoolExecutor、 ScheduledThreadPoolExecutor、 Future 接口、 Runnable 接口、 Callable 接口 和 Executors。

（1） ThreadPoolExecutor

​	ThreadPoolExecutor 通常 使用 工厂 类 Executors 来 创建。 Executors 可以 创建 3 种 类型 的 ThreadPoolExecutor： SingleThreadExecutor、 FixedThreadPool 和 CachedThreadPool。

​	Executors 可以 创建 3 种 类型 的 ThreadPoolExecutor： SingleThreadExecutor、 FixedThreadPool 和 CachedThreadPool。

（2） ScheduledThreadPoolExecutor

ScheduledThreadPoolExecutor 通常 使用 工厂 类 Executors 来 创建。 Executors 可以 创建 2 种 类型 的 ScheduledThreadPoolExecutor， 如下。 

* ScheduledThreadPoolExecutor。 包含 若干个 线程 的 ScheduledThreadPoolExecutor。 
* SingleThreadScheduledExecutor。 只 包含 一个 线程 的 ScheduledThreadPoolExecutor。

（3） Future 接口

​	Future 接口 和 实现 Future 接口 的 FutureTask 类 用来 表示 异步 计算 的 结果。 当 我们 把 Runnable 接口 或 Callable 接口 的 实现 类 提交（ submit） 给 ThreadPoolExecutor 或 ScheduledThreadPoolExecutor 时， ThreadPoolExecutor 或 ScheduledThreadPoolExecutor 会 向我 们 返回 一个 FutureTask 对象。

（4） Runnable 接口 和 Callable 接口

​	Runnable 接口 和 Callable 接口 的 实现 类， 都可以 被 ThreadPoolExecutor 或 Scheduled- ThreadPoolExecutor 执行。 它们 之间 的 区别 是 Runnable 不会 返回 结果， 而 Callable 可以 返回 结果。

### 10.2 ThreadPoolExecutor详解

​	最核心的类是ThreadPoolExecutor。是线程池的实现类

​	Executor 框架 最 核心 的 类 是 ThreadPoolExecutor， 它是 线程 池 的 实现 类， 主要 由 下列 4 个 组件 构成。 

* corePool： 核心 线程 池 的 大小。 
* maximumPool： 最大 线程 池 的 大小。 
* BlockingQueue： 用来 暂时 保存 任务 的 工作 队列。 
* RejectedExecutionHandler： 当 ThreadPoolExecutor 已经 关闭 或 ThreadPoolExecutor 已经 饱和 时（ 达到 了 最大 线程 池 大小 且 工作 队列 已满）， execute() 方法 将要 调用 的 Handler。



#### 10.2.1 FixedThreadPool详解

![](https://pic1.imgdb.cn/item/5fbd20a2b18d6271135100dd.jpg)

![](https://pic1.imgdb.cn/item/5fbd20c4b18d627113510a9b.jpg)

1） 如果 当前 运行 的 线程 数 少于 corePoolSize， 则 创建 新 线程 来 执行任务。 

2） 在 线程 池 完成 预热 之后（ 当前 运行 的 线程 数 等于 corePoolSize）， 将 任务 加入 LinkedBlockingQueue。 

3） 线程 执行 完 1 中的 任务 后， 会在 循环 中 反复 从 LinkedBlockingQueue 获取 任务 来 执行。

FixedThreadPool 使用 无 界 队列 LinkedBlockingQueue 作为 线程 池 的 工作 队列（ 队列 的 容量 为 Integer. MAX_ VALUE）。

1） 当 线程 池 中的 线程 数 达到 corePoolSize 后， 新任务 将 在 无 界 队列 中 等待， 因此 线程 池 中的 线程 数 不会 超过 corePoolSize。 

2） 由于 1， 使用 无 界 队列 时 maximumPoolSize 将是 一个 无效 参数。 

3） 由于 1 和 2， 使用 无 界 队列 时 keepAliveTime 将是 一个 无效 参数。 

4） 由于 使用 无 界 队列， 运行 中的 FixedThreadPool（ 未 执行 方法 shutdown() 或 shutdownNow()） 不会 拒绝 任务（ 不会 调用 RejectedExecutionHandler. rejectedExecution 方法）。

#### 10.2.2 SingleThreadExecutor详解

​	

```java
public static ExecutorService newSingleThreadExecutor() {
    return new FinalizableDelegatedExecutorService
        (new ThreadPoolExecutor(1, 1,
                                0L, TimeUnit.MILLISECONDS,
                                new LinkedBlockingQueue<Runnable>()));
}
```

1） 如果 当前 运行 的 线程 数 少于 corePoolSize（ 即 线程 池 中 无 运行 的 线程）， 则 创建 一个 新 线程 来 执行任务。 

2） 在 线程 池 完成 预热 之后（ 当前 线程 池 中有 一个 运行 的 线程）， 将 任务 加入 Linked- BlockingQueue。 

3） 线程 执行 完 1 中的 任务 后， 会在 一个 无限 循环 中 反复 从 LinkedBlockingQueue 获取 任务 来 执行。

#### 10.2.3 CachedThreadPool详解

![](https://pic1.imgdb.cn/item/5fbfbdc815e771908423f18c.jpg)

​	CachedThreadPool 的 corePoolSize 被 设置 为 0， 即 corePool 为 空； maximumPoolSize 被 设置 为 Integer. MAX_ VALUE， 即 maximumPool 是 无 界 的。 这里 把 keepAliveTime 设置 为 60L， 意味着 CachedThreadPool 中的 空闲 线程 等待 新任务 的 最长 时间 为 60 秒， 空闲 线程 超过 60 秒 后 将会 被 终止。 

​	FixedThreadPool 和 SingleThreadExecutor 使用 无 界 队列 LinkedBlockingQueue 作为 线程 池 的 工作 队列。 CachedThreadPool 使用 没有 容量 的 SynchronousQueue 作为 线程 池 的 工作 队列， 但 CachedThreadPool 的 maximumPool 是 无 界 的。 这 意味着， 如果 主线 程 提交 任务 的 速度 高于 maximumPool 中线 程 处理 任务 的 速度 时， CachedThreadPool 会 不断 创建 新 线程。 极端 情况下， CachedThreadPool 会 因为 创建 过多 线程 而 耗尽 CPU 和 内存 资源。

![](https://pic1.imgdb.cn/item/5fbfbe3315e7719084240d0c.jpg)

1） 首先 执行 SynchronousQueue. offer（ Runnable task）。 如果 当前 maximumPool 中有 空闲 线程 正在 执行 SynchronousQueue. poll（ keepAliveTime， TimeUnit. NANOSECONDS）， 那么 主线 程 执行 offer 操作 与 空闲 线程 执行 的 poll 操作 配对 成功， 主 线程 把 任务 交给 空闲 线程 执行， execute() 方法 执行 完成； 否则 执行 下面 的 步骤 2）。

2） 当初 始 maximumPool 为 空， 或者 maximumPool 中 当前 没有 空闲 线程 时， 将 没有 线程 执行 SynchronousQueue. poll（ keepAliveTime， TimeUnit. NANOSECONDS）。 这种 情况下， 步骤 1） 将 失败。 此时 CachedThreadPool 会 创建 一个 新 线程 执行任务， execute() 方法 执行 完成。

3） 在 步骤 2） 中 新 创建 的 线程 将 任务 执行 完 后， 会 执行 SynchronousQueue. poll（ keepAliveTime， TimeUnit. NANOSECONDS）。 这个 poll 操作 会 让 空闲 线程 最多 在 SynchronousQueue 中 等待 60 秒钟。 如果 60 秒钟 内主 线程 提交 了 一个 新任务（ 主线 程 执行 步骤 1））， 那么 这个 空闲 线程 将 执行 主线 程 提交 的 新任务； 否则， 这个 空闲 线程 将 终止。 由于 空闲 60 秒 的 空闲 线程 会被 终止， 因此 长时间 保持 空闲 的 CachedThreadPool 不会 使用 任何 资源。

### 10.3 ScheduledThreadPoolExecutor

​	它 主要 用来 在给 定的 延迟 之后 运行 任务， 或者 定期 执行任务。 ScheduledThreadPoolExecutor 的 功能 与 Timer 类似， 但 ScheduledThreadPoolExecutor 功能 更 强大、 更 灵活。

#### 10.3.1 ScheduledThreadPoolExecutor的运作机制

​	DelayQueue 是 一个 无 界 队列， 所以 ThreadPoolExecutor 的 maximumPoolSize 在 Scheduled- ThreadPoolExecutor 中 没有 什么 意义。

1） 当 调用 ScheduledThreadPoolExecutor 的 scheduleAtFixedRate() 方法 或者 scheduleWith- FixedDelay() 方法 时， 会 向 ScheduledThreadPoolExecutor 的 DelayQueue 添加 一个 实现 了 RunnableScheduledFutur 接口 的 ScheduledFutureTask。 

2） 线程 池 中的 线程 从 DelayQueue 中 获取 ScheduledFutureTask， 然后 执行任务。

![](https://pic1.imgdb.cn/item/5fbfc23115e771908425b1ed.jpg)

#### 10.3.2 ScheduledThreadPoolExecutor的实现

3个成员变量：

* long型成员变量time，表示这个任务将要被执行的具体时间。
* long型成员变量sequenceNumber，便是这个任务被添加到ScheduledThreadPoolExecutor中的序号。
* long型成员变量period，表示任务执行的间隔周期。

DelayQueue 封装 了 一个 PriorityQueue， 这个 PriorityQueue 会对 队列 中的 Scheduled- FutureTask 进行 排序。 排序 时， time 小的 排在 前面（ 时间 早的 任务 将被 先 执行）。 如果 两个 ScheduledFutureTask 的 time 相同， 就 比较 sequenceNumber， sequenceNumber 小的 排在 前面（ 也就是说， 如果 两个 任务 的 执行 时间 相同， 那么 先 提交 的 任务 将被 先 执行）。

![](https://pic1.imgdb.cn/item/5fbfc32a15e77190842652e7.jpg)

1） 线程 1 从 DelayQueue 中 获取 已 到期 的 ScheduledFutureTask（ DelayQueue. take()）。 到期 任务 是指 ScheduledFutureTask 的 time 大于 等于 当前 时间。 

2） 线程 1 执行 这个 ScheduledFutureTask。

3） 线程 1 修改 ScheduledFutureTask 的 time 变量 为 下次 将要 被 执行 的 时间。

4） 线程 1 把这 个 修改 time 之后 的 ScheduledFutureTask 放回 DelayQueue 中（ Delay- Queue. add()）。

![](https://pic1.imgdb.cn/item/5fbfc39615e7719084269dba.jpg)

![](https://pic1.imgdb.cn/item/5fc111c615e77190847cc20c.jpg)

获取任务分为3大步骤

1) 获取Lock

2）获取周期任务

* 如果 PriorityQueue 为 空， 当前 线程 到 Condition 中 等待； 否则 执行 下面 的 2. 2。

* 如果 PriorityQueue 的 头 元素 的 time 时间 比 当前 时间 大， 到 Condition 中等 待到 time 时间； 否则 执行 下面 的 2. 3。

* 获取 PriorityQueue 的 头 元素（ 2. 3. 1）； 如果 PriorityQueue 不为 空， 则 唤醒 在 Condition 中 等待 的 所有 线程（ 2. 3. 2）。

3）释放Lock。

  ScheduledThreadPoolExecutor 在 一个 循环 中 执行 步骤 2， 直到 线程 从 PriorityQueue 获取 到 一个 元素 之后（ 执行 2. 3. 1 之后）， 才会 退出 无限 循环（ 结束 步骤 2）。

![](https://pic1.imgdb.cn/item/5fc1125015e77190847cecf2.jpg)

![](https://pic1.imgdb.cn/item/5fc1127815e77190847cf865.jpg)

​	添加 任务 分为 3 大 步骤。 

1） 获取 Lock。 

2） 添加 任务。 

* 向 PriorityQueue 添加 任务。 
* 如果 在上面 2. 1 中 添加 的 任务 是 PriorityQueue 的 头 元素， 唤醒 在 Condition 中 等待 的 所有 线程。

3） 释放锁。

### 10.4 FutureTask详解

​	Future 接口 和 实现 Future 接口 的 FutureTask 类， 代表 异步 计算 的 结果。

#### 10.4.1 FutureTask简介

​	除了实现Future接口，害实现了Runnable接口，因此可以交给Executor执行。也可以调用线程直接执行。

1） 未 启动。 FutureTask. run() 方法 还没 有被 执行 之前， FutureTask 处于 未 启动 状态。 当 创建 一个 FutureTask， 且 没有 执行 FutureTask. run() 方法 之前， 这个 FutureTask 处于 未 启动 状态。 

2） 已 启动。 FutureTask. run() 方法 被 执行 的 过程中， FutureTask 处于 已 启动 状态。 3） 已完成。 FutureTask. run() 方法 执行 完 后 正常 结束， 或 被 取消（ FutureTask. cancel（…））， 或 执行 FutureTask. run() 方法 时 抛出 异常 而异 常 结束， FutureTask 处于 已完成 状态。

![](https://pic1.imgdb.cn/item/5fc1155e15e77190847dc668.jpg)

​	当 FutureTask 处于 未 启动 或 已 启动 状态 时， 执行 FutureTask. get() 方法 将 导致 调用 线程 阻塞； 当 FutureTask 处于 已完成 状态 时， 执行 FutureTask. get() 方法 将 导致 调用 线程 立即 返回 结果 或 抛出 异常。

​	当 FutureTask 处于 未 启动 状态 时， 执行 FutureTask. cancel() 方法 将 导致 此 任务 永远 不会 被 执行； 当 FutureTask 处于 已 启动 状态 时， 执行 FutureTask. cancel（ true） 方法 将以 中断 执行 此 任务 线程 的 方式 来试 图 停止 任务； 当 FutureTask 处于 已 启动 状态 时， 执行 FutureTask. cancel（ false） 方法 将不 会对 正在 执行 此 任务 的 线程 产生 影响（ 让 正在 执行 的 任务 运行 完成）； 当 FutureTask 处于 已完成 状态 时， 执行 FutureTask. cancel（…） 方法 将 返回 false。

![](https://pic1.imgdb.cn/item/5fc1159915e77190847dd9aa.jpg)

#### 10.4.2 FutureTask的使用

​	可以 把 FutureTask 交给 Executor 执行； 也可以 通过 ExecutorService. submit（…） 方法 返回 一个 FutureTask， 然后 执行 FutureTask. get() 方法 或 FutureTask. cancel（…） 方法。 除此以外， 还可以 单独 使用 FutureTask。

​	当 一个 线程 需要 等待 另一个 线程 把 某个 任务 执行 完 后 它 才能 继续 执行， 此时 可以 使用 FutureTask。 假设 有多 个 线程 执行 若干 任务， 每个 任务 最多 只能 被 执行 一次。 当 多个 线程 试图 同时 执行 同一个 任务 时， 只 允许 一个 线程 执行任务， 其他 线程 需要 等待 这个 任务 执行 完 后才 能 继续 执行。

```java
public class ConcurrentTask {

    private final ConcurrentMap<Object, Future<String>> taskCache = new ConcurrentHashMap<Object, Future<String>>();

    private String executionTask(final String taskName) throws ExecutionException, InterruptedException {
        while (true) {
            Future<String> future = taskCache.get(taskName); //1.1,2.1
            if (future == null) {
                Callable<String> task = new Callable<String>() {
                    public String call() throws InterruptedException {
                        //......
                        return taskName;
                    }
                };
                //1.2创建任务
                FutureTask<String> futureTask = new FutureTask<String>(task);
                future = taskCache.putIfAbsent(taskName, futureTask); //1.3
                if (future == null) {
                    future = futureTask;
                    futureTask.run(); //1.4执行任务
                }
            }

            try {
                return future.get(); //1.5,2.2线程在此等待任务执行完成
            } catch (CancellationException e) {
                taskCache.remove(taskName, future);
            }
        }
    }

}
```

![](https://pic1.imgdb.cn/item/5fc37f1cd590d4788ac8e0fb.jpg)

#### 10.4.3 FurtureTask的实现

​	实现基于AbstractQueuedSychronizer(以下简称AQS)。队列。 JDK 6 中 AQS 被 广泛 使用， 基于 AQS 实现 的 同步 器 包括： ReentrantLock、 Semaphore、 ReentrantReadWriteLock、 CountDownLatch 和 FutureTask。

​	每一个 基于 AQS 实现 的 同步 器 都会 包含 两种 类型 的 操作， 如下。 

* 至少 一个 acquire 操作。 这个 操作 阻塞 调用 线程， 除非/ 直到 AQS 的 状态 允许 这个 线程 继续 执行。 FutureTask 的 acquire 操作 为 get()/ get（ long timeout， TimeUnit unit） 方法 调用。 

* 至少 一个 release 操作。 这个 操作 改变 AQS 的 状态， 改变 后的 状态 可 允许 一个 或 多个 阻塞 线程 被 解除 阻塞。 FutureTask 的 release 操作 包括 run() 方法 和 cancel（…） 方法。 



​	基于“ 复合 优先于 继承” 的 原则， FutureTask 声明 了 一个 内部 私有 的 继承 于 AQS 的 子类 Sync， 对 FutureTask 所有 公有 方法 的 调用 都会 委托 给 这个 内部 子类。 

​	AQS 被 作为“ 模板 方法 模式” 的 基础 类 提 供给 FutureTask 的 内部 子类 Sync， 这个 内部 子类 只需 要 实现 状态 检查 和 状态 更新 的 方法 即可， 这些 方法 将 控制 FutureTask 的 获取 和 释放 操作。 具体来说， Sync 实现 了 AQS 的 tryAcquireShared（ int） 方法 和 tryReleaseShared（ int） 方法， Sync 通过 这 两个 方法 来 检查 和 更新 同步 状态。

![](https://pic1.imgdb.cn/item/5fc38245d590d4788ac9df75.jpg)

​	FutureTask. get() 方法 会 调用 AQS. acquireSharedInterruptibly（ int arg） 方法， 这个 方法 的 执行 过程 如下。 

​	1） 调用 AQS. acquireSharedInterruptibly（ int arg） 方法， 这个 方法 首先 会 回 调 在 子类 Sync 中 实现 的 tryAcquireShared() 方法 来 判断 acquire 操作 是否 可以 成功。 acquire 操作 可以 成功 的 条件 为： state 为 执行 完成 状态 RAN 或 已 取消 状态 CANCELLED， 且 runner 不为 null。 

​	2） 如果 成功 则 get() 方法 立即 返回。 如果 失败 则 到 线程 等待 队列 中去 等待 其他 线程 执行 release 操作。 

​	3） 当 其他 线程 执行 release 操作（ 比如 FutureTask. run() 或 FutureTask. cancel（…）） 唤醒 当前 线程 后， 当前 线程 再次 执行 tryAcquireShared() 将 返回 正值 1， 当前 线程 将 离开 线程 等待 队列 并 唤醒 它的 后继 线程（ 这里 会 产生 级 联 唤醒 的 效果， 后面 会 介绍）。 

​	4） 最后 返回 计算 的 结果 或 抛出 异常。 

​	FutureTask. run() 的 执行 过程 如下。 

​	1） 执行 在 构造 函数 中指 定的 任务（ Callable. call()）。 

​	2） 以 原子 方式 来 更新 同步 状态（ 调用 AQS. compareAndSetState（ int expect， int update）， 设置 state 为 执行 完成 状态 RAN）。 如果 这个 原子 操作 成功， 就 设置 代表 计算 结果 的 变量 result 的 值 为 Callable. call() 的 返回 值， 然后 调用 AQS. releaseShared（ int arg）。 

​	3） AQS. releaseShared（ int arg） 首先 会 回 调 在 子类 Sync 中 实现 的 tryReleaseShared（ arg） 来 执行 release 操作（ 设置 运行 任务 的 线程 runner 为 null， 然 会 返回 true）； AQS. releaseShared（ int arg）， 然后 唤醒 线程 等待 队列 中的 第一个 线程。 

​	4） 调用 FutureTask. done()。

![](https://pic1.imgdb.cn/item/5fc3898cd590d4788acbb683.jpg)

​	假设 开始时 FutureTask 处于 未 启动 状态 或 已 启动 状态， 等待 队列 中 已经 有 3 个 线程（ A、 B 和 C） 在 等待。 此时， 线程 D 执行 get() 方法 将 导致 线程 D 也 到 等待 队列 中去 等待。 

​	当 线程 E 执行 run() 方法 时， 会 唤醒 队列 中的 第一个 线程 A。 线程 A 被 唤醒 后， 首先 把 自己 从 队列 中 删除， 然后 唤醒 它的 后继 线程 B， 最后 线程 A 从 get() 方法 返回。 线程 B、 C 和 D 重复 A 线程 的 处理 流程。 最终， 在 队列 中 等待 的 所有 线程 都被 级 联 唤醒 并从 get() 方法 返回。



# JUC

## 并发包工具类

### CountDownLatch

​	CountDownLatch是一个同步助手，允许一个或者多个线程等待一系列的其他线程执行结束。

```java
public class CountDownLatchTest {

    public static void main(String[] args) throws InterruptedException {
        List<String> stringList = Arrays.asList("a", "b", "c");
        CountDownLatch countDownLatch = new CountDownLatch(stringList.size());
        stringList.forEach(p -> new Thread(() -> {
            System.out.println(p);
            countDownLatch.countDown();
        }).start());
        countDownLatch.await();
        System.out.println("finish");
    }
}
```

### CyclicBarrier

​	CyclicBarrier也非常适合用于某个串行化任务被分拆成若干个并行执行的子任务，当所有的子任务都执行结束之后再继续接下来的工作。从这一点来看，Cyclic Barrier与CountDownLatch非常类似，但是它们之间的运行方式以及原理还是存在着比较大的差异的，并且CyclicBarrier所能支持的功能CountDownLatch是不具备的。比如，CyclicBarrier可以被重复使用，而CountDownLatch当计数器为0的时候就无法再次利用。

```java
public class CyclicBarrierTest {
    public static void main(String[] args) {
        List<String> stringList = Arrays.asList("a", "b", "c");
        CyclicBarrier cyclicBarrier = new CyclicBarrier(stringList.size() + 1);
        stringList.forEach(p -> new Thread(() -> {
            System.out.println(p);
            try {
                TimeUnit.SECONDS.sleep(current().nextInt(10));
                cyclicBarrier.await();
            } catch (InterruptedException e) {
                e.printStackTrace();
            } catch (BrokenBarrierException e) {
                e.printStackTrace();
            }
            System.out.println("Other Thing" + p);
        }).start());
        try {
            cyclicBarrier.await();
            TimeUnit.SECONDS.sleep(10);
        } catch (InterruptedException e) {
            e.printStackTrace();
        } catch (BrokenBarrierException e) {
            e.printStackTrace();
        }
    }
}
```

* 可以被循环使用。
  * 通过reset。
  * CyclicBarrier内部维护了一个count。当所有的await调用导致其值为0的时候，reset相关的操作会被默认执行。
* reset方法中调用了breakBarrier方法和唤醒其他新阻塞线程，但是它们都会被忽略掉，根本不会影响到dowait方法中的线程（因为执行该方法的线程已经没有了），紧接着generation又会被重新创建。

* CyclicBarrier(int parties)构造器：构造CyclicBarrier并且传入parties
* CyclicBarrier(int parties, Runnable barrierAction)构造器：构造CyclicBarrier不仅传入parties，而且指定一个Runnable接口，当所有的线程到达barrier point的时候，该Runnable接口会被调用，有时我们需要在所有任务执行结束之后执行某个动作，这时就可以使用这种CyclicBarrier的构造方式了。
* int getParties()方法：获取CyclicBarrier在构造时的parties，该值一经CyclicBarrier创建将不会被改变。
* await()方法：我们使用最多的一个方法，调用该方法之后，当前线程将会进入阻塞状态，等待其他线程执行await()方法进入barrier point，进而全部退出阻塞状态，当CyclicBarrier内部的count为0时，调用await()方法将会直接返回而不会进入阻塞状态。
* isBroken()：返回barrier的broken状态，某个线程由于执行await方法而进入阻塞状态，如果该线程被执行了中断操作，那么isBroken()方法将会返回true。

![](https://pic.imgdb.cn/item/60a7954a6ae4f77d35ac5d2e.jpg)

1）当一个线程由于在执行CyclicBarrier的await方法而进入阻塞状态时，这个时候对该线程执行中断操作会导致CyclicBarrier被broken。

2）被broken的CyclicBarrier此时已经不能再直接使用了，如果想要使用就必须使用reset方法对其重置。

3）如果有其他线程此时也由于执行了await方法而进入阻塞状态，那么该线程会被唤醒并且抛出BrokenBarrierException异常。

* getNumberWaiting()方法： 该方法返回当前barrier有多少个线程执行了await方法而不是还有多少个线程未到达barrier point，这一点需要注意。
*  reset()方法：前面已经详细地介绍过这个方法，其主要作用是中断当前barrier，并且重新生成一个generation，还有将barrier内部的计数器count设置为parties值，但是需要注意的是，如果还有未到达barrier point的线程，则所有的线程将会被中断并且退出阻塞，此时isBroken()方法将返回false而不是true。

**CyclicBarrier VS. CountDownLatch**

* CoundDownLatch的await方法会等待计数器被count down到0，而执行CyclicBarrier的await方法的线程将会等待其他线程到达barrier point。
* CyclicBarrier内部的计数器count是可被重置的，进而使得CyclicBarrier也可被重复使用，而CoundDownLatch则不能。
* CyclicBarrier是由Lock和Condition实现的，而CountDownLatch则是由同步控制器AQS（AbstractQueuedSynchronizer）来实现的。
* 在构造CyclicBarrier时不允许parties为0，而CountDownLatch则允许count为0。

### Exchanger

​	Exchanger简化了两个线程之间的数据交互，并且提供了两个线程之间的数据交换点，Exchanger等待两个线程调用其exchange()方法。调用此方法时，交换机会交换两个线程提供给对方的数据。

```java
public class ExchangerTest {
    public static void main(String[] args) {
        Exchanger<String> exchanger = new Exchanger<>();
        new Thread(() -> {
            try {
                String data = exchanger.exchange("I'm a");
                System.out.println(data);
                System.out.println("This is a, say " + data);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }).start();

        new Thread(() -> {
            try {
                String data = exchanger.exchange("I'm b");
                System.out.println("This is b, say " + data);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }).start();
    }
}
```

​	只有成对的线程执行了exchange调用之后才会退出阻塞，如果一个线程不需要数据，传递null即可。

* public V exchange(V x) throws InterruptedException：数据交换方法，该方法的作用是将数据x交换至搭档线程，执行该方法后，当前线程会进入阻塞状态，只有当搭档线程也执行了exchange方法之后，该当前线程才会退出阻塞状态进行下一步的工作，与此同时，该方法的返回值代表着搭档线程所传递过来的交换数据。 
* public V exchange(V x, long timeout, TimeUnit unit) throwsInterruptedException, TimeoutException：该方法的作用与前者类似，只不过增加了超时的功能，也就是说在指定的时间内搭档线程没有执行exchange方法，当前线程会退出阻塞，并且返回值为null。
* 我们虽然可以在exchange方法中传入null值，但是Exchanger会为我们提供一个默认的Object（NULL_ITEM）值，在最后返回值时会根据交换数据与NULL_ITEM进行匹配，并将交换数据重新返回为null、、



​	如果使用Exchanger的两个线程，其中一个由于某种原因意外退出，那么此时另外一个线程将会永远处于阻塞状态，进而导致JVM进程出现假死的情况。当然使用了超时功能的exchange在设定时间到达时会退出阻塞，因此在使用Exchanger时中断数据交换线程的操作是非常重要的。

​	如果当前线程被执行过中断方法，并且从未捕获过中断信号，那么在执行exchange方法的时候会立即被中断

### Semaphore

​	用于在一个时刻允许多个线程对共享资源进行并行操作的场景。

* 如果此时Semaphore内部的计数器大于零，那么线程将可以获得小于该计数器数量的许可证，同时还会导致Semaphore内部的计数器减少所发放的许可证数量。
* 如果此时Semaphore内部的计数器等于0，也就是说没有可用的许可证，那么当前线程有可能会被阻塞（使用tryAcquire方法时不会阻塞）。
* 当线程不再使用许可证时，需要立即将其释放以供其他线程使用，所以建议将许可证的获取以及释放动作写在try..finally语句块中。

![](https://pic.imgdb.cn/item/60a7a05d6ae4f77d35f6934e.jpg)

```java
public class SemaphoreTest {
    public static void main(String[] args) {
        Semaphore semaphore = new Semaphore(5);
        IntStream.range(0, 20).forEach(p ->new Thread(() -> {
            try {
                // semaphore.acquire(); // 阻塞
                boolean t = semaphore.tryAcquire(); // 不阻塞
                TimeUnit.SECONDS.sleep(current().nextInt(5));
                System.out.format("user %d login %b \n", p, t);
                if (t) {
                    semaphore.release();
                }
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }).start());
    }
}
```



**Semaphore的构造 **

* public Semaphore(int permits)：定义Semaphore指定许可证数量，并且指定非公平的同步器，因此new Semaphore(n)实际上是等价于newSemaphore(n，false)的。
* public Semaphore(int permits, boolean fair)：定义Semaphore指定许可证数量的同时给定非公平或是公平同步器



**tryAcquire方法**

tryAcquire方法尝试向Semaphore获取许可证，如果此时许可证的数量少于申请的数量，则对应的线程会立即返回，结果为false表示申请失败，tryAcquire包含如下四种重载方法。

* tryAcquire()：尝试获取Semaphore的许可证，该方法只会向Semaphore申请一个许可证，在Semaphore内部的可用许可证数量大于等于1的情况下，许可证将会获取成功，反之获取许可证则会失败，并且返回结果为false。
* boolean tryAcquire(long timeout, TimeUnit unit) throwsInterruptedException：该方法与tryAcquire无参方法类似，同样也是尝试获取一个许可证，但是增加了超时参数。如果在超时时间内还是没有可用的许可证，那么线程就会进入阻塞状态，直到到达超时时间或者在超时时间内有可用的证书（被其他线程释放的证书），或者阻塞中的线程被其他线程执行了中断。
* boolean tryAcquire(int permits)：在使用无参的tryAcquire时只会向Semaphore尝试获取一个许可证，但是该方法会向Semaphore尝试获取指定数目的许可证。
* boolean tryAcquire(int permits, long timeout, TimeUnit unit)：该方法与第二个方法类似，只不过其可以指定尝试获取许可证数量的参数，这里就不再赘述了，读者可以自行测试。



**acquire方法**

* void acquire()：该方法会向Semaphore获取一个许可证，如果获取不到就会一直等待，直到Semaphore有可用的许可证为止，或者被其他线程中断。当然，如果有可用的许可证则会立即返回。
* void acquire(int permits)：该方法会向Semaphore获取指定数量的许可证，如果获取不到就会一直等待，直到Semaphore有可用的相应数量的许可证为止，或者被其他线程中断。同样，如果有可用的permits个许可证则会立即返回。

**acquireUninterruptibly**

* void acquireUninterruptibly()：该方法会向Semaphore获取一个许可证，如果获取不到就会一直等待，与此同时对该线程的任何中断操作都会被无视，直到Semaphore有可用的许可证为止。当然，如果有可用的许可证则会立即返回
* void acquireUninterruptibly(int permits)：该方法会向Semaphore获取指定数量的许可证，如果获取不到就会一直等待，与此同时对该线程的任何中断操作都会被无视，直到Semaphore有可用的许可证为止，或者被其他线程中断。同样，如果有可用的permits个许可证则会立即返回。

**正确使用release**

* void release()：释放一个许可证，并且在Semaphore的内部，可用许可证的计数器会随之加一，表明当前有一个新的许可证可被使用。
* void release(int permits)：释放指定数量（permits）的许可证，并且在Semaphore内部，可用许可证的计数器会随之增加permits个，表明当前又有permits个许可证可被使用。

**其他方法**

* boolean isFair()：对Semaphore许可证的争抢采用公平还是非公平的方式，对应到内部的实现类为FairSync（公平）和NonfairSync（非公平）。
* int availablePermits()：当前的Semaphore还有多少个可用的许可证。
* int drainPermits()：排干Semaphore的所有许可证，以后的线程将无法获取到许可证，已经获取到许可证的线程将不受影响。
* boolean hasQueuedThreads()：当前是否有线程由于要获取Semaphore许可证而进入阻塞？（该值为预估值。）
* int getQueueLength()：如果有线程由于获取Semaphore许可证而进入阻塞，那么它们的个数是多少呢？（该值为预估值。）

### Phaser

​	Phaser是在JDK1.7版本中才加入的。Phaser同样也是一个多线程的同步助手工具，它是一个可被重复使用的同步屏障，它的功能非常类似于本章已经学习过的CyclicBarrier和CountDownLatch的合集，但是它提供了更加灵活丰富的用法和方法，同时它的使用难度也要略微大于前两者。

​	CountDownLatch可以很好地控制等待多个线程执行完子任务，但是它有一个缺点，那就是内部的计数器无法重置，也就是说CountDownLatch属于一次性的，使用结束后就不能再次使用。CyclicBarrier倒是可以重复使用，但是一旦parties在创建的时候被指定，就无法再改变。Phaser则取百（两）家之所长于一身引入了两者的特性。

**当CountDownLatch来用**

```java
public class PhaserExample1 {
    public static void main(String[] args) throws InterruptedException {
        final Phaser phaser = new Phaser();
        for (int i = 0; i < 10; i++) {
            new Thread(() -> {
                // 内部parties+1
                phaser.register();
                try {
                    TimeUnit.SECONDS.sleep(current().nextInt(20));
                    phaser.arrive();
                    System.out.println(new Date() + ":" + currentThread() + " finish");
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
            }, "T-" + i).start();
        }
        TimeUnit.SECONDS.sleep(10);
        // 注册
        phaser.register();
        // 等待下一阶段，下一个阶段的前提是内部当前的unarrived数量为0
        phaser.arriveAndAwaitAdvance();
        assert phaser.getArrivedParties() == 11 : "total 11 parties is registered";
        System.out.println(new Date() + ":all finish");
    }
}
```

​	在主线程进行register操作之前，请务必保证所有的子线程都能够顺利register，否则就会出现phaser只注册了一个parties，并且很快arrive的情况，这会导致后面的断言语句出现失败的情况，因此我们在主线程进行register操作之前，需要通过休眠的方式确保所有的子线程顺利register（当然这并不是一种非常严谨的方式，给出的休眠时间也是来自我们日常的经验值，更加合理的方式是在定义Phaser的时候指定parties的值，关于这一点，后文中会为大家详细介绍）。

​	也可以主线程注册。

**将Phaser当作CyclicBarrier来使用**

​	可以借助于Phaser来完成CyclicBarrier的主要功能，即所有的子线程共同到达一个barrier point。

```java
public class PhaserExample2 {
    public static void main(String[] args) throws InterruptedException {
        Phaser phaser = new Phaser();
        for (int i = 0; i < 10; i++) {
            new Thread(() -> {
                phaser.register();
                try {
                    TimeUnit.SECONDS.sleep(current().nextInt(20));
                    phaser.arriveAndAwaitAdvance();
                    System.out.println(new Date() + ":" + currentThread() + " finish");
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
            }, "T-" + i).start();
        }
        TimeUnit.SECONDS.sleep(10);
        // 注册
        phaser.register();
        // 等待下一阶段，下一个阶段的前提是内部当前的unarrived数量为0
        phaser.arriveAndAwaitAdvance();
        assert phaser.getArrivedParties() == 11 : "total 11 parties is registered";
        System.out.println(new Date() + ":all finish");
    }
}
```

​	arriveAndAwaitAdvance方法，该方法会等待在当前Phaser中所有的part（子线程）都完成了任务才能使线程退出阻塞，当然也包括主线程自身，因为主线程也进行了register操作。运行上面的程序我们会发现，几乎所有的输出语句都是在同一时间输出的，这也就完全符合CyclicBarrier等待所有的子线程都到达barrier point这一特性了。

**重写onAdvance方法**

​	在构造CyclicBarrier的时候，如果给定一个Runnable作为回调，那么待所有的任务线程都到达barrier point之后，该Runnable接口的run方法将会被调用。同样，我们可以通过重写Phaser的onAdvance方法来实现类似的功能。在Phaser中，onAdvance方法是非常重要的，它在每一个Phase（阶段）中除了会在所有的分片都到达之后执行一次调用之外，更重要的是，它还会决定该Phaser是否被终止（当onAdvance方法的返回值为true时，则表明该Phaser将被终止，接下来将不能再使用）。

```java
public class PhaserExample3 {
    public static void main(String[] args) throws InterruptedException {
        MyPhaser phaser = new MyPhaser(() -> {
            System.out.println("wow");
        });
        for (int i = 0; i < 10; i++) {
            new Thread(() -> {
                phaser.register();
                try {
                    TimeUnit.SECONDS.sleep(current().nextInt(20));
                    phaser.arriveAndAwaitAdvance();
                    System.out.println(new Date() + ":" + currentThread() + " finish");
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
            }, "T-" + i).start();
        }
        TimeUnit.SECONDS.sleep(10);
        // 注册
        phaser.register();
        // 等待下一阶段，下一个阶段的前提是内部当前的unarrived数量为0
        phaser.arriveAndAwaitAdvance();
        assert phaser.getArrivedParties() == 11 : "total 11 parties is registered";
        System.out.println(new Date() + ":all finish");
    }
    static class MyPhaser extends Phaser {
        private final Runnable runnable;
        private MyPhaser(Runnable runnable) {
            this.runnable = runnable;
        }

        @Override
        protected boolean onAdvance(int phase, int registeredParties) {
            this.runnable.run();
            return super.onAdvance(phase, registeredParties);
        }
    }
}

```

​	该方法更重要的作用其实是决定Phaser的生死:





# MySQL

## MySql 执行引擎

* InnoDB

  * 存储在表空间，由数据文件组成。
  * mysql4.1后，可以将每个表的数据和索引存放在单独的文件中。
  * MVCC。
  * 支持四个隔离级别。
    * 默认为REPEATABLE READ(可重复度)，并且通过间隙锁(next-key locking)策略防止幻读。
  * 基于聚簇索引。
  * 支持事务。
* MyISAM

  * 不支持事务、行锁。
  * 不支持崩溃后安全恢复。
  * 表存储在两个文件，数据文件(.MYD)和索引文件(.MYI)。
  * 整表加锁。
    * 读会加共享锁，写加排他锁。
    * 读时可以并发插入。
  * 修复
    * 可以手动或自动检查和修复。
      * CHECK TABLE MYTABLE
      * REPAIR TABLE MYTABLE
      * 执行表的修复比较慢，也可能导致一些数据丢失。
  * 索引特性
    * 支持全文搜索。
    * 对于BLOB和TEXT字段，也可以基于前500字符创建索引。
  * 延迟更新索引键。
    * 创建 MyISAM 表 的 时候， 如果 指定 了 DELAY_ KEY_ WRITE 选项， 在 每次 修改 执行 完成时， 不会 立刻 将 修改 的 索引 数据 写入 磁盘， 而是 会 写到 内存 中的 键 缓冲区（ in- memory key buffer）， 只有 在 清理 键 缓冲区 或者 关闭 表 的 时候 才会 将 对应 的 索引 块 写入 到 磁盘。
    * 极大提升性能但是崩溃时造成索引损坏。
  * 压缩表
    * 无法修改，除非先解除压缩，再修改数据，再压缩。
    * 极大减少磁盘空间占用。
    * 减少磁盘I/O。
    * 索引只读。
  * 性能
    * 数据以紧密格式存储，有些场景下性能很好。
* Federated
* Memory
* Merge
* NDB
* 选择执行引擎

  * 事务

    * 需要: InnoDB(XtraDB)。
    * 不需要: MyISAM。

  * 备份

    * 可以关闭服务器来备份，可以忽略该因素。
    * 热备份: InnoDB。

  * 崩溃恢复

    * 系统崩溃后快速恢复: InnoDB

  * 特有特性

  * 日志型应用

    * MyISAM和Archive

  * 只读或大部分情况下只读的表

    * 不介意崩溃恢复可以选MyISAM

  * 订单处理

    * InnoDB

  * 电子公告牌和主题讨论论坛。

  * CD-ROM应用

    * 可以考虑使用MyISAM或者MyISAM压缩表。

  * 大数据量

    * 数据量3-5TB: InnoDB运行得还不错。
    * 增长到10TB，就需要建立数据仓库。InfoBright是最成功的方案，一些大数据库也可以采用TokuDB。
  
* 转换引擎
  * ALTER TABLE mytable ENGINE= InnoDB;
  * 导出和导入(使用mysqldump工具，然后修改CREATE TABLE语句) 
  * 创建和查询(INSERT..SELECT语法)
    * 小数据量ok。
    * 大数据量考虑分批处理。
      * 每一段数据执行事务提交操作。



## MySQL执行计划

**如何查看执行计划**

https://www.cnblogs.com/klvchen/p/10137117.html

​	EXPLAIN + sql

```sql
explain select * from user where uid=2;
```

![](https://img2018.cnblogs.com/blog/1334255/201812/1334255-20181218161456575-1277410542.png)

**ID**

Id:包含一组数字，表示查询中执行select子句或操作表的顺序

id不同的时候：当id不同的时，id越大的越先执行；



| select_type  | 说明         |
| ------------ | ------------ |
| SIMPLE       | 简单查询     |
| PRIMARY      | 最外层查询   |
| SUBQUERY     | 映射为子查询 |
| DERIVED      | 子查询       |
| UNION        | 联合         |
| UNION RESULT | 使用联合结果 |

table 访问的表名。

| type        | 说明                                                         |
| ----------- | ------------------------------------------------------------ |
| ALL         | 全数据表扫描                                                 |
| index       | 全所有表扫描                                                 |
| RANGE       | 对所有列进行范围查找                                         |
| INDEX_MERGE | 合并索引，使用多个单列索引搜索                               |
| REF         | 根据索引查找一个或多个值                                     |
| EQ_REF      | 搜索时使用primary key或unique类型                            |
| CONST       | 常量，表最多有一个匹配行，因为仅有一行，在这行的列值可被优化器剩余部分认为是常数，const很快，因为只读取一次。 |
| SYSTEM      | 系统，表仅有一行。是const类型的一个特例。                    |

性能：`all` < `index` < `range` < `index_merge` < `ref_or_null` < `ref` < `eq_ref` < `system/const`
性能在 range 之下基本都可以进行调优。

**possible_keys** : 可能使用的索引

------

**key** : 真实使用的

------

**key_len** : MySQL中使用索引字节长度

------

**rows** : mysql 预估为了找到所需的行而要读取的行数

| extra                                       | 说明                                                         |
| ------------------------------------------- | ------------------------------------------------------------ |
| Using index                                 | 此值表示mysql将使用覆盖索引，以避免访问表。                  |
| Using where                                 | mysql 将在存储引擎检索行后再进行过滤，许多where条件里涉及索引中的列，当(并且如果)它读取索引时，就能被存储引擎检验，因此不是所有带where子句的查询都会显示“Using where”。有时“Using where”的出现就是一个暗示：查询可受益于不同的索引。 |
| Using temporary                             | mysql 对查询结果排序时会使用临时表。                         |
| Using filesort                              | mysql会对结果使用一个外部索引排序，而不是按索引次序从表里读取行。mysql有两种文件排序算法，这两种排序方式都可以在内存或者磁盘上完成，explain不会告诉你mysql将使用哪一种文件排序，也不会告诉你排序会在内存里还是磁盘上完成。 |
| Range checked for each record(index map: N) | 没有好用的索引，新的索引将在联接的每一行上重新估算，N是显示在possible_keys列中索引的位图，并且是冗余的 |

**limit**

limit 匹配后就不会继续进行扫描

### 关键字执行顺序

![](https://upload-images.jianshu.io/upload_images/24195226-750c885b0865f632.png?imageMogr2/auto-orient/strip|imageView2/2/w/926/format/webp)



### 优化准则

```
禁用select *
使用select count(*) 统计行数
尽量少运算
尽量避免全表扫描，如果可以，在过滤列建立索引
尽量避免在where子句对字段进行null判断
尽量避免在where子句使用!= 或者<>
尽量避免在where子句使用or连接
尽量避免对字段进行表达式计算
尽量避免对字段进行函数操作
尽量避免使用不是复合索引的前缀列进行过滤连接
尽量少排序，如果可以，建立索引
尽量少join
尽量用join代替子查询
尽量避免在where子句中使用in,not in或者having，使用exists,not exists代替
尽量避免两端模糊匹配 like %***%
尽量用union all代替union
尽量早过滤
避免类型转换
尽量批量insert
优先优化高并发sql，而不是频率低的大sql
尽可能对每一条sql进行explain
尽可能从全局出发
```



## 索引

### B-Tree索引

​	B-Tree通常意味着所有的值都是按顺序存储的，每一个叶子节点到根的节点距离相同。

![](https://pic.imgdb.cn/item/60a8a56b6ae4f77d355717dd.jpg)

​	B- Tree 索引 能够 加快 访问 数据 的 速度， 因为 存储 引擎 不再 需要 进行 全 表 扫描 来 获取 需要 的 数据， 取而代之 的 是 从 索引 的 根 节点（ 图示 并未 画出） 开始 进行 搜索。

​	B- Tree 对 索引 列 是 顺序 组织 存储 的， 所以 很 适合 查找 范围 数据。

​	请注意， 索引 对 多个 值 进行 排序 的 依据 是 CREATE TABLE 语句 中 定义 索引 时 列 的 顺序。

* 适用于全键值、键值范围、键前查找
  * 全值匹配
  * 匹配最左前缀
  * 匹配列前缀。
  * 匹配范围值。
  * 精确匹配某一列或范围匹配另外一列。
  * 值访问所有的查询。
* 限制
  * 如果 不是 按照 索引 的 最 左列 开始 查找， 则 无法 使用 索引。
  * 不能跳过索引的列。
  * 如果查询中有某个列的范围查询，则右边所有列无法使用优化查找。
    * 查找。 例如 有 查询 WHERE last_ name=' Smith' AND frst_ name LIKE 'J％' AND dob=' 1976- 12- 23'， 这个 查询 只能 使用 索引 的 前 两 列， 因为 这里 LIKE 是一 个 范围 条件（ 但是 服务器 可以 把 其余 列 用于 其他 目的）。 如果 范围 查询 列 值 的 数量 有限， 那么 可以 通过 使用 多个 等于 条件 来 代替 范围 条件。



### Hash索引

* 只有 精确 匹配 索引 所有 列 的 查询 才 有效。
* 只有Memory引擎显示支持。
* Memory并非唯一支持的。



```sql
CREATE TABLE testhash ( 
    fname VARCHAR( 50) NOT NULL, 
    lname VARCHAR( 50) NOT NULL, 
    KEY USING HASH(fname) 
) ENGINE= MEMORY;
mysql> SELECT lname FROM testhash WHERE fname=' Peter';
```

​	MySQL 先 计算' Peter' 的 哈 希 值， 并 使用 该 值 寻找 对应 的 记录 指针。 因为 f（' Peter'）= 8784， 所以 MySQL 在 索引 中 查找 8784， 可以 找到 指向 第 3 行的 指针， 最后 一步 是 比较 第三 行的 值 是否 为' Peter'， 以 确保 就是 要 查找 的 行。

**优点**

* 结构紧凑，速度非常快。

**限制**

* 值包含哈希值和行指针，不存储字段值，所以不能使用索引中的值来避免读取行。大部分这一点对性能影响不明显。
* 哈希索引数据并不是按照索引值顺序存储的，所以也就无法用于排序。
* 哈希所有也不支持部分索引列匹配查找。
* 哈希索引值支持等值比较，=，in，<=>。不支持任何范围查找。
* 访问哈希所有的速度非常快，除非有很多哈希冲突。
* 如果哈希冲突很多，一些所有维护操作的代价也很高。



* NDB集群引擎也支持唯一哈希索引，且在NDB集群引擎中作用非常特殊。
* InnoDB有一个自适应哈希索引。它注意到某些索引值被使用得非常频繁的时候，会在B-Tree所有上再创建一个Hash索引。
  * 可以关闭该索引。

**实例**

```sql
mysql> SELECT id FROM url WHERE url=" http:// www. mysql. com";
# 若 删除 原来 URL 列 上 的 索引， 而 新增 一个 被 索引 的 url_ crc 列， 使用 CRC32 做 哈 希， 就可以 使用 下面 的 方式 查询：
mysql> SELECT id FROM url WHERE url=" http:// www. mysql. com" -> AND url_ crc= CRC32(" http:// www. mysql. com");
# 性能会非常高，缺点是要手动维护或触发器实现。
```

​	如果 采用 这种 方式， 记住 不要 使用 SHA1() 和 MD5() 作为 哈 希 函数。 因为 这 两个 函数 计算 出 来的 哈 希 值 是非 常 长的 字符串， 会 浪费 大量 空间， 比较 时 也会 更 慢。 SHA1() 和 MD5() 是 强加 密函 数， 设计 目标 是 最大限度 消除 冲突， 但这 里 并不 需要 这样 高的 要求。

​	如果 数据表 非常 大， CRC32() 会 出现 大量 的 哈 希 冲突， 则 可以 考虑 自己 实现 一个 简单 的 64 位 哈 希 函数。一个 简单 的 办法 可以 使用 MD5() 函数 返回 值 的 一部分 来作 为 自定义 哈 希 函数。 这 可能 比 自己 写 一个 哈 希 算法 的 性能 要 差（， 不过 这样 实现 最简单。

### 空间数据所有(R-Tree)

​	略

### 全文索引

​	全文 索引 是一 种 特殊 类型 的 索引， 它 查找 的 是 文本 中的 关键 词， 而 不是 直接 比较 索引 中的 值。 全文 搜索 和 其他 几类 索引 的 匹配 方式 完全 不一样。 它有 许多 需要 注意 的 细节， 如 停 用词、 词干 和 复数、 布尔 搜索 等。 全文 索引 更 类似于 搜索 引擎 做 的 事情， 而 不是 简单 的 WHERE 条件 匹配。 在 相同 的 列 上 同时 创建 全文 索引 和 基于 值 的 B- Tree 索引 不会有 冲突， 全文 索引 适用于 MATCH AGAINST 操作， 而 不是 普通 的 WHERE 条件 操作。

### 索引优点

* 减少服务器需要扫描的数据量
* 帮助服务器避免排序和临时表。
* 随机IO变为顺序IO

### 高性能索引策略

* 独立的列使用索引
* 多列索引
  * 多个列使用单独索引大部分情况下无法提高查询性能。
  * mysql索引合并策略。
  * 服务器出现对多个索引AND操作，通常意味需要一个包含所有相关列的多列索引，而不是多个单独的单列索引。
  * 服务器需要对多个索引做or操作时，通常要耗费大量CPU和内存资源。
  * 执行计划会低估查询成本，还不是写成UNION。

如果 在 EXPLAIN 中看 到有 索引 合并， 应该 好好 检查 一下 查询 和 表 的 结构， 看 是不是 已经 是 最优 的。 也可以 通过 参数 optimizer_ switch 来 关闭 索引 合并 功能。 也可以 使用 IGNORE INDEX 提示 让 优化 器 忽略 掉 某些 索引。

* 选择合适的索引列顺序。
  * 将选择性最高的列放在最前列，有些场景下有帮助，不如避免随机IO和排序那么重要。
  * 当 不需要 考虑 排序 和 分组 时， 将 选择性 最 高的 列 放在 前面 通常 是 很好 的。 这时候 索引 的 作用 只是 用于 优化 WHERE 条件 的 查找。 在 这种 情况下， 这样 设计 的 索引 确实 能够 最快 地 过滤 出 需 要的 行， 对于 在 WHERE 子句 中 只 使用 了 索引 部分 前缀 列 的 查询 来说 选择性 也 更高。 然而， 性能 不只 是 依赖于 所有 索引 列 的 选择性（ 整体 基数）， 也和 查询 条件 的 具体 值 有关， 也就是 和 值 的 分布 有关。 这 和 前面 介绍 的 选择 前缀 的 长度 需要 考虑 的 地方 一样。 可能 需要 根据 那些 运行 频率 最高 的 查询 来 调整 索引 列 的 顺序， 让 这种 情况下 索引 的 选择性 最高。
  * 当 使用 前缀 索引 的 时候， 在某 些 条件 值 的 基数 比 正常值 高的 时候， 问题 就来 了。 例如， 在某 些 应用 程序 中， 对于 没有 登录 的 用户， 都将 其 用户 名 记录 为“ guset”， 在 记录 用户 行为 的 会话（ session） 表 和 其他 记录 用户 活动 的 表中“ guest” 就成 为了 一个 特殊 用户 ID。 一旦 查询 涉及 这个 用户， 那么 和 对于 正常 用户 的 查询 就 大 不同 了， 因为 通常 有很 多 会话 都是 没有 登录 的。 系统 账号 也会 导致 类似 的 问题。 一个 应用 通常 都有 一个 特殊 的 管理员 账号， 和 普通 账号 不同， 它 并不是 一个 具体 的 用户， 系统 中 所有 的 其他 用户 都是 这个 用户 的 好友， 所以 系统 往往 通过 它 向 网 站的 所有 用户 发送 状态 通知 和 其他 消息。 这个 账号 的 巨大 的 好友 列表 很容易 导致 网 站 出现 服务器 性能 问题。

### 聚簇索引

* 不是单独的索引类型，是一种存储方式。
* 聚簇表示数据行和相邻键值紧凑地存储在一起。
* 一个表只能有一个聚簇索引(覆盖索引可以模拟多个聚簇索引的情况)。
* 不是所有的引擎都支持聚簇索引。
  * MySQL的引擎不支持选择聚簇索引。
  * InnoDB通过主键聚集数据。
    * 没有定义主键，会选择一个唯一的非空索引。
    * 如果没有这样的所有，会隐式定义一个组件来做聚簇索引。
* 聚簇索引可能对性能有帮助，也可能导致严重问题。
* 可以把相关数据保存在一块。
* 数据访问更快。
* 使用覆盖索引扫描查询可以直接使用页节点中的主键值。

**缺点**

* 最大限度提高了I/O密集型应用的性能，但如果数据全部都放在内存中，则访问顺序没这么重要了。
* 插入数据严重依赖于插入顺序。按主键的插入数据需是加载到InnoDB最快的方式。如果不是按主键顺序加载数据，加载完成后最好使用OPTIMIZE TABLE重新组织下标。
* 更新聚簇索引列代价很高，会强制每个被更新的行移动到新的位置。
* 基于聚簇索引的表再插入新行，或主键被更新导致需要移动行的时候，可能面临"页分裂"的问题。会占用更大的空间。
* 聚簇索引可能导致全表扫描变慢，尤其是行比较极速，或者由于页分裂导致数据不连续的时候。
* 二级索引(非聚簇索引)可能比想象更大，因为二级索引的叶子节点包含了引用行的主键列。
* 二级索引访问需要两次索引查找，而不是一次。
* 通过 二级 索引 查找 行， 存储 引擎 需要 找到 二级 索引 的 叶子 节点 获得 对应 的 主 键值， 然后 根据 这个 值 去 聚 簇 索引 中 查找 到 对应 的 行。

**InnoDB和MyISAM数据分布对比**

​	MyISAM按插入数据顺序存储在磁盘上，可以跳过所需字节找到需要的行。

​	InnoDB聚 簇 索引 的 每一个 叶子 节点 都 包含 了 主 键值、 事务 ID、 用于 事务 和 MVCC的 回 滚 指针 以及 所有 的 剩余 列（ 在这 个 例子 中 是 col2）。

​	InnoDB 的 二级 索引 和 聚 簇 索引 很不 相同。 InnoDB 二级 索引 的 叶子 节点 中 存储 的 不是“ 行 指针”， 而是 主 键值， 并以 此 作为 指向 行的“ 指针”。 这样 的 策略 减少 了 当 出现 行 移动 或者 数据 页 分裂 时 二级 索引 的 维护 工作。 使用 主 键值 当作 指针 会 让 二级 索引 占用 更多 的 空间， 换 来的 好处 是， InnoDB 在 移动 行 时 无须 更新 二级 索引 中的 这个“ 指针”。



* InnoDB表中按主键顺序插入行。

* 自定义主键列，用AUTO_INCREMENT自增列。

* 用UUID来作为聚簇索引会很糟糕，会使插入变得完全随机。

* 向 UUID 主 键 插入 行不 仅 花费 的 时间 更长， 而且 索引 占用 的 空间 也 更大。 这一 方面 是 由于 主 键 字段 更长； 另一方面 毫无疑问 是 由于 页 分裂 和 碎片 导致 的。

**UUID缺点**

* 写入 的 目标 页 可能 已经 刷 到 磁盘 上 并从 缓存 中 移 除， 或者是 还没 有被 加载 到 缓存 中， InnoDB 在 插入 之前 不得不 先 找到 并从 磁盘 读取 目标 页 到 内存 中。 这 将 导致 大量 的 随机 I/ O。
* 因为 写入 是 乱 序 的， InnoDB 不得不 频繁 地做 页 分裂 操作， 以便 为 新的 行 分配 空间。 页 分裂 会 导致 移动 大量 数据， 一次 插入 最少 需要 修改 三个 页 而 不是 一个 页。

* 由于 频繁 的 页 分裂， 页 会变 得 稀疏 并被 不规则 地 填充， 所以 最终 数据 会有 碎片。

**顺序主键什么时候会造成更快的结果**

> 对于 高 并发 工作 负载， 在 InnoDB 中 按 主 键 顺序 插入 可能 会 造成 明显 的 争用。 主 键 的 上界 会 成为“ 热点”。 因为 所有 的 插入 都 发生 在这里， 所以 并发 插入 可能 导致 间隙 锁 竞争。 另一个 热点 可能 是 AUTO_ INCREMENT 锁 机制； 如果 遇到 这个 问题， 则 可能 需要 考虑 重新 设计 表 或者 应用， 或者 更改 innodb_autoinc_lock_mode 配置。 如果 你的 服务器 版本 还不 支持 innodb_autoinc_lock_mode 参数， 可以 升级 到 新版本 的 InnoDB， 可能 对这 种 场景 会 工作 得 更好。

### 覆盖索引

​	如果一个索引覆盖所有需要查询字段的值，我们称之为覆盖索引。

* 可以极大提高性能。
* 索引条目通常远小于行大小，所以如果只需要读所有，MySQL就会极大地减少数据访问量。
* 所有是按照列值顺序存储的(至少单个页内如此)，所以IO密集型的范围查询会比随机从磁盘读取每一行数据的IO要少的多。
* 一些存储引擎(MyISAM)只会缓存索引，数据则依赖操作系统来缓存，因此要访问数据需要一次系统调用。
* 由于InnoDB的聚簇索引，覆盖索引对InnoDB特别有用。它的二级索引在叶子节点中保存了行的主键值，如果二级主键能够覆盖查询，则可以避免对主键索引的二次查询。
* 当发起一个被索引覆盖的查询是，在EXPLAIN的Extra列可以看到"Using index"的信息。
* 索引覆盖查询无法无法实现优化的陷阱
  * 索引覆盖了WHERE条件中给的字段，但不是整个查询涉及到的字段，MYSQL5.5和更早版本总是会取回数据行。
  * 索引覆盖无法覆盖该查询，有两个原因。
    * 没有任何索引能够覆盖这个查询。
    * MySQL不能再索引中执行LIKE操作
      * 可以用比较巧妙的设计避免该问题

先 将 索引 扩展 至 覆盖 三个 数据 列（ artist， title， prod_ id）， 然后 按 如下 方式 重写 查询：

```sql
EXPLAIN SELECT
	*
FROM
	products
JOIN (
	SELECT
		prod_ id
	FROM
		products
	WHERE
		actor = ' SEAN CARREY'
	AND title LIKE '%APOLLO%'
) AS t1 ON (
	t1.prod_ id = products.prod_ id
)
```

​	在 大多数 存储 引擎 中， 覆盖 索引 只能 覆盖 那些 只 访问 索引 中部 分列 的 查询。 不过， 可以 更进一步 优化 InnoDB。 回想 一下， InnoDB 的 二级 索引 的 叶子 节点 都 包含 了 主 键 的 值， 这 意味着 InnoDB 的 二级 索引 可以 有效地 利用 这些“ 额外” 的 主 键 列 来 覆盖 查询。

### 使用索引扫描来做排序

有两种方式生成有序结果：

* 排序
* 按索引顺序扫描。



* 执行计划的type列为index，就是来用索引扫描来做了排序。
* 只有 当 索引 的 列 顺序 和 ORDER BY 子句 的 顺序 完全 一致， 并且 所有 列 的 排序 方向（ 倒序 或 正 序） 都 一样 时， MySQL 才能 够 使用 索 引来 对 结果 做 排序。 如果 查询 需要 关联 多 张 表， 则 只有 当 ORDER BY 子句 引用 的 字段 全部 为 第一个 表 时， 才能 使用 索引 做 排序。
* 即使 ORDER BY 子句 不满足 索引 的 最 左 前缀 的 要求， 也可以 用于 查询 排序， 这是 因为 索引 的 第一 列 被指 定为 一个 常数。
* 压缩 块 使用 更少 的 空间， 代价 是 某些 操作 可能 更 慢。 因为 每个 值 的 压缩 前缀 都 依赖 前 面的 值， 所以 MyISAM 查找 时 无法 在 索引 块 使用 二分 查找 而 只能 从头开始 扫描。 正 序 的 扫描 速度 还 不错， 但是 如果 是 倒序 扫描—— 例如 ORDER BY DESC—— 就不 是 很好 了。
* 测试 表明， 对于 CPU 密集型 应用， 因为 扫描 需要 随机 查找， 压缩 索引 使得 MyISAM 在 索引 查找 上 要 慢 好 几倍。 压缩 索引 的 倒序 扫描 就 更 慢了。 压缩 索引 需要 在 CPU 内存 资源 与 磁盘 之间 做 权衡。 压缩 索引 可能 只需 要 十分之一 大小 的 磁盘 空间， 如果 是 I/ O 密集型 应用， 对 某些 查询 带来 的 好处 会 比 成本 多 很多。
* 可以 在 CREATE TABLE 语句 中 指定 PACK_ KEYS 参数 来 控制 索引 压缩 的 方式。


### 冗余和重复索引

​	MySQL 允许 在 相同 列 上 创建 多个 索引， 无论是 有意 的 还是 无意 的。 MySQL 需要 单独 维护 重复 的 索引， 并且 优化 器 在 优化 查询 的 时候 也需 要 逐个 地 进行 考虑， 这 会 影响 性能。

​	重复 索引 是指 在 相同 的 列 上 按照 相同 的 顺序 创建 的 相同 类型 的 索引。 应该 避免 这样 创建 重复 索引， 发现 以后 也应该 立即 移 除。

​	冗余 索引 和 重复 索引 有 一些 不同。 如果 创建 了 索引（ A， B）， 再创 建 索引（ A） 就是 冗余 索引， 因为 这 只是 前一 个 索引 的 前缀 索引。 因此 索引（ A， B） 也可以 当作 索引（ A） 来 使用（ 这种 冗余 只是 对 B- Tree 索引 来说 的）。 但是 如果 再创 建 索引（ B， A）， 则 不是 冗余 索引， 索引（ B） 也不 是， 因为 B 不是 索引（ A， B） 的 最 左 前缀 列。





### 压缩(前缀压缩)索引

* MyISAM 使用 前缀 压缩 来 减少 索引 的 大小， 从而 让 更多 的 索引 可以 放入 内存 中， 这 在 某些 情况下 能 极大 地 提高 性能。 默认 只 压缩 字符串， 但 通过 参数 设置 也可 以对 整数 做 压缩。 MyISAM 压缩 每个 索引 块 的 方法 是， 先 完全 保存 索引 块 中的 第一个 值， 然后 将 其他 值 和 第一个 值 进行 比较 得到 相同 前缀 的 字节数 和 剩余 的 不同 后缀 部分， 把这 部分 存储 起来 即可。
* 大多数 情况下 都不 需要 冗余 索引， 应该 尽量 扩展 已有 的 索引 而 不是 创建 新 索引。 但也 有时候 出于 性能 方面 的 考虑 需要 冗余 索引， 因为 扩展 已有 的 索引 会 导致 其 变得 太大， 从而 影响 其他 使用 该 索引 的 查询 的 性能。



### 未使用的索引

​	尽量删除。

​	最简单 有效 的 办法 是在 Percona Server 或者 MariaDB 中 先 打开 userstates 服务器 变量（ 默认 是 关闭 的）， 然后 让 服务器 正常 运行 一段时间， 再通过 查询 INFORMATION_ SCHEMA. INDEX_ STATISTICS 就能 查到 每个 索引 的 使用 频率。

### 索引和锁

​	索引 可以 让 查询 锁定 更 少的 行。 如果 你的 查询 从不 访问 那些 不需要 的 行， 那么 就会 锁定 更 少的 行， 从 两个 方面 来看 这对 性能 都有 好处。 首先， 虽然 InnoDB 的 行 锁 效率 很高， 内存 使用 也 很少， 但是 锁定 行的 时候 仍然 会 带来 额外 开销； 其次， 锁定 超过 需要 的 行会 增加 锁 争用 并 减少 并发 性。

### 维护索引和表

**找到并修复损坏的表**

* CHECK TABLE
  * REPAIRE TABLE
* 不支持直接修复的
  * ALTER TABLE table1 ENGINE=INNODB



​	如果 InnoDB 引擎 的 表 出现 了 损坏， 那么 一定 是 发生了 严重 的 错误， 需要 立刻 调查 一下 原因。 InnoDB 一般 不会 出现 损坏。 InnoDB 的 设计 保证 了 它 并不 容易 被 损坏。 如果 发生 损坏， 一般 要么 是 数据库 的 硬件 问题 例如 内存 或者 磁盘 问题（ 有可能）， 要么 是 由于 数据库 管理员 的 错误 例如 在 MySQL 外部 操作 了 数据 文件（ 有可能）， 抑或 是 InnoDB 本身 的 缺陷（ 不太 可能）。

​	常见 的 类似 错误 通常 是由 于 尝试 使用 rsync 备份 InnoDB 导致 的。 不存在 什么 查询 能够 让 InnoDB 表 损坏， 也不 用 担心 暗处 有“ 陷阱”。 

​	如果 某 条 查询 导致 InnoDB 数据 的 损坏， 那 一定 是 遇到 了 bug， 而 不是 查询 的 问题。 如果 遇到 数据 损坏， 最重要的 是 找出 是什么 导致 了 损坏， 而 不只 是 简单 地 修复， 否则 很有可能 还会 不断 地 损坏。 可以 通过 设置 innodb_ force_ recovery 参数 进入 InnoDB 的 强制 恢复 模式 来 修复 数据， 更多 细节 可以 参考 MySQL 手册。 另外， 还可以 使用 开源 的 InnoDB 数据 恢复 工具箱（ InnoDB Data Recovery Toolkit） 直接 从 InnoDB 数据 文件 恢复 出 数据（ 下载 地址： http:// www. percona. com/ software/ mysql- innodb- data- recovery- tools/）。

**更新所有统计信息**

* records_ in_ range()， 通过 向 存储 引擎 传入 两个 边界 值 获取 在这 个 范围 大概 有 多少 条 记录。
* 是 info()， 该 接口 返回 各种 类型 的 数据， 包括 索引 的 基数（ 每个 键值 有 多少 条 记录）。
* Memory 引擎 根本 不 存储 索引 统计 信息。
* MyISAM 将 索引 统计 信息 存储 在 磁盘 中， ANALYZE TABLE 需要 进行 一次 全 索引 扫描 来 计算 索引 基数。 在 整个 过程中 需要 锁 表。
* 直到 MySQL 5. 5 版本， InnoDB 也不 在 磁盘 存储 索引 统计 信息， 而是 通过 随机 的 索 y 引 访问 进行 评估 并将 其 存储 在 内存 中。
* 可以 使用 SHOW INDEX FROM 命令 来 查看 索引 的 基数（ Cardinality）。
* InnoDB 会在 表 首次 打开， 或者 执行 ANALYZE TABLE， 抑或 表 的 大小 发生 非常 大的 变化（ 大小 变化 超过 十六 分之一 或者 新 插入 了 20 亿 行都 会 触发） 的 时候 计算 索引 的 统计 信息。

* InnoDB 在打 开 某些 INFORMATION_ SCHEMA 表， 或者 使用 SHOW TABLE STATUS 和 SHOW INDEX， 抑或 在 MySQL 客户 端 开启 自动 补 全功能 的 时候 都会 触发 索引 统计 信息 的 更新。
* 只要 SHOW INDEX 查看 索引 统计 信息， 就 一定 会 触发 统计 信息 的 更新。 可以 关闭 innodb_ stats_ on_ metadata 参数 来 避免 上面 提到 的 问题。

**减少索引和数据碎片**

* 行碎片
  * 这种 碎片 指的 是 数据 行 被 存储 为多 个 地方 的 多个 片段 中。 即使 查询 只从 索引 中 访问 一行 记录， 行 碎片 也会 导致 性能 下降。
* 行间碎片
  * 是指逻辑上顺序的页，或者行在磁盘上不是顺序存储的。的。 行间 碎片 对 诸如 全 表 扫描 和 聚 簇 索引 扫描 之类 的 操作 有很 大的 影响， 因为 这些 操作 原本 能够 从 磁盘 上 顺序 存储 的 数据 中 获益。
* 剩余 空间 碎片（ Free space fragmentation）
  * 剩余 空间 碎片 是指 数据 页 中有 大量 的 空余 空间。 这 会 导致 服务器 读取 大量 不需 要的 数据， 从而 造成 浪费。
* 可以 通过 执行 OPTIMIZE TABLE 或者 导出 再导入 的 方式 来 重新 整理 数据。 这对 多数 存储 引擎 都是 有效 的。


​	对于 MyISAM 表， 这 三类 碎片 化 都 可能发生。 但 InnoDB 不会 出现 短 小的 行 碎片； InnoDB 会 移动 短 小的 行 并重 写到 一个 片段 中。

### 索引下推

https://zhuanlan.zhihu.com/p/121084592

* 索引下推（index condition pushdown ）简称ICP，在Mysql5.6的版本上推出，用于优化查询。

* 在不使用ICP的情况下，在使用非主键索引（又叫普通索引或者二级索引）进行查询时，存储引擎通过索引检索到数据，然后返回给MySQL服务器，服务器然后判断数据是否符合条件。
* 在使用ICP的情况下，如果存在某些被索引的列的判断条件时，MySQL服务器将这一部分判断条件传递给存储引擎，然后由存储引擎通过判断索引是否符合MySQL服务器传递的条件，只有当索引符合条件时才会将数据检索出来返回给MySQL服务器 
* 索引条件下推优化可以减少存储引擎查询基础表的次数，也可以减少MySQL服务器从存储引擎接收数据的次数。  


### 使用索引

* 使用 like 语句时，%在右边才会使用索引。
* or 条件中有未建立索引的列才,索引失效
* 条件的类型不一致
* != 号
* \> 号
* order by
  * 如果 order by 是主键或索引是整数类型，则会走索引
* 组合索引
  * 遵循最左前缀

## 回表

https://www.jianshu.com/p/8991cbca3854

## SQL优化



## 数据库事务和隔离级别&数据库锁

* 原子性：所有的SQL语句要么全部成功，要么全部失败，不会存在部分更新。如果数据库在运行两个SQL语句之后没有服从这个属性，那么账户A就会凭空损失100美元。
* 一致性：事务只能以允许的方式改变受其影响的数据。在这个例子中，如果account_number与账户B不存在，则整个事务应该被回滚。
* 隔离性：同时发生的事务（并发事务）不应该导致数据库处于不一致的状态中。系统中每个事务都应该像唯一事务一样执行。任何事务都不应影响其他事务的存在。假设A向B转账的同时，A完全转移所有600美元，两个事务应该独立进行，在进行转账前确认好余额。
*  持久性：无论数据库或系统是否发生故障，数据都会永久保存在磁盘上，并且不会丢失。

**并发事务带来的问题**

- 更新丢失：当两个事务或多个事务选择同一行，然后基于最初选定的值更新改行，由于不知道互相存在，就会发生丢失更新问题。
- 脏读：一个事务正在对一条记录做修改，在这个事务完成并提交前，另外一个事务来读取同一条记录，如果不加控制，就读取了脏数据。
- 不可重复读：一个事务正在读取某个数据后的某个时间，再次读取以前读过的数据，却发现其读出的数据已经发生了改变或某些记录已经被删除了。
- 幻读(Phantom Read)： 一个事务按相同的查询条件重新查询已经检索过的数据，却发现其他事务插入了满足其查询条件的新数据。

### 执行事务

启动事务

```mysql
START TRANSACTION;
# 或者
BEGIN;

# 执行语句
xxx
# 提交
COMMIT;
# 回滚
ROLLBACK;
```

**autocommit**

​	默认情况下，autocommit 的状态是 ON，这意味着所有单独的语句一旦被执行就会被提交，除非该语句在BEGIN...COMMIT块中。如果autocommit的状态为OFF，则需要明确发出COMMIT语句来提交事务。要禁用autocommit，请执行：

```mysql
SET autocommit=0;
```

​	DDL语句，如数据库的CREATE或DROP语句，以及表或存储例程的CREATE、DROP或ALTER语句，都是无法回滚的。

### 保存点

```mysql
BEGIN;
# 操作语句
xxx
# 保存点
SAVEPOINT transfer_to_b;
# 操作
xxx
# 回滚
ROLLBACK TO transfer_to_b
```

### 隔离级别

​	当两个或多个事务同时发生时，隔离级别定义了一个事务与其他事务在资源或者数据修改方面的隔离程度。有4种类型的隔离级别，要更改隔离级别，需要设置tx_isolation变量，该变量是动态的并具有会话级别的作用范围。

**如何修改**

​	要更改隔离级别，请执行SET@@transaction_isolation=＇READ-COMMITTED＇；语句。

**隔离级别**

* 读取未提交（read uncommitted）

  * 当前事务可以读取由另一个未提交的事务写入的数据，这也称为脏读（dirtyread）。

* 读提交（read committed）

  * 当前事务只能读取另一个事务提交的数据，这也称为不可重复读取（non-repeatable read）。

* 可重复读取（repeatable read）

  * 一个事务通过第一条语句只能看到相同的数据，即使另一个事务已提交数据。在同一事务中，读取通过第一次读取建立快照是一致的。一个例外是，一个事务可以读取在同一事务中更改的数据。

* 序列化（serializable）

  * 通过把选定的所有行锁起来，序列化可以提供最高级别的隔离。此级别与REPEATABLE READ类似，但如果禁用autocommit，则InnoDB会将所有普通SELECT语句隐式转换为SELECT...LOCK IN SHARE MODE；如果启用autocommit，则SELECT就是它自己的事务，如下表所示。

  ![img](https://pic.imgdb.cn/item/5f5a3811160a154a67d4ff4a.jpg)

### 锁



* 内部锁：MySQL在自身服务器内部执行内部锁，以管理多个会话对表内容的争用。
* 外部锁：MySQL为客户会话提供选项来显式地获取表锁，以阻止其他会话访问表。



**内部锁可以分为下面两种类型**

* 行级锁：行级锁是细粒度的。只有被访问的行会被锁定。这允许通过多个会话同时进行写访问，使其适用于多用户、高度并发和OLTP的应用程序。只有InnoDB支持行级锁。
* 表级锁：MySQL对MyISAM、MEMORY和MERGE表使用表级锁，一次只允许一个会话更新这些表。这种锁定级别使得这些存储引擎更适用于只读的或以读取操作为主的或单用户的应用程序。



* 外部锁：可以使用LOCK TABLE和UNLOCK TABLES语句来控制锁定。



READ和WRITE的表锁定解释如下。

* READ：当一个表被锁定为READ时，多个会话可以从表中读取数据而不需要获取锁。此外，多个会话可以在同一个表上获得锁，这就是为什么READ锁也被称为共享锁。当READ锁被保持时，没有会话可以将数据写入表格中（包括持有该锁的会话）。如果有任何写入尝试，该操作将处于等待状态，直到READ锁被释放。
* WRITE：当一个表被锁定为WRITE时，除持有该锁的会话之外，其他任何会话都不能读取或向表中写入数据。除非现有锁被释放，否则其他任何会话都不能获得任何锁。这就是为什么WRITE锁被称为排他锁。如果有任何读取/写入尝试，该操作将处于等待状态，直到WRITE 锁被释放。
* 当执行UNLOCK TABLES语句时或当会话终止时，所有锁都会被释放。

**如何操作**

```mysql
LOCK TABLES table_name [READ|WRITE]
UNLOCK TABLES
# 要锁定所有数据库中的所有表，请执行以下语句。在获取数据库的一致快照时需要使用该语句，它会冻结对数据库的所有写入操作：
FLUSH TABLES WITH READ LOCK;
```

**锁队列**

​	除共享锁（一个表可以有多个共享锁）之外，没有两个锁可以一起加在一个表上。如果一个表已经有一个共享锁，此时有一个排他锁要进来，那么它将被保留在队列中，直到共享锁被释放。当排他锁在队列中时，所有后续的共享锁也会被阻塞并保留在队列中。



## 编码方式

## 行数统计

​	通过在数据库中建表保存计数，利用innodb引擎支持事物的优点，保持数据在逻辑上一直的前提下，快速获取结果。

count()：mysql聚合函数，对返回的结果集一行行判断，不是null，累计值就+1，不是就不+。最后返回累计值。

不同count()的用法：

count(主键id)，innodb会遍历整张表，将主键id上的值都取出来，返回给server层，server层判断不为null的前提下，按行累加

count(1)，innodb遍历整张表，不取值。返回给server层，server层对于返回的每一行，都会放一个数字1 进去，不为空的话按行累加

count(字段)，看字段默认是否允许为null，分别进行判断

count(*)，不会将全部字段取出来，专门做了优化，不取值。肯定不是null，按行累加

性能：count(*)>count(1)>count(主键)>count（字段）

## 连接

## 数据库主备搭建

## log

## 内存数据库

## 分库分表

## 读写分离

## 分别使用数据库锁、NoSql实现分布式锁

## 性能调优

## 数据库连接池



# Memcached

# Spring

## Spring boot解决循环依赖

https://blog.csdn.net/qq_18298439/article/details/88818418

## Bean加载过程





# HotSpot源码

## JVM内存模型与操作内存的关系

JMM进程内存模型(OS内存模型)

* 代码段
* 数据区
* 栈
* 堆

JVM内存模型

* 方法区
* 本地方法栈
  * JNI
* 虚拟机栈
* 堆
  * CodeCache
  * JIT
* 程序计数器

# Spring Boot 技术内幕: 架构设计与实现原理

## 目录结构



* spring-boot-project：Spring Boot核心项目代码，包含核心、工具、安全、文档、starters等项目。
  * spring-boot：Spring Boot核心代码，也是入口类SpringApplication类所在项目，是本书重点介绍的内容。
  * spring-boot-actuator：提供应用程序的监控、统计、管理及自定义等相关功能。
  * spring-boot-actuator-autoconfigure：针对actuator提供的自动配置功能。
  * spring-boot-autoconfigure：Spring Boot自动配置核心功能，默认集成了多种常见框架的自动配置类等。
  * spring-boot-cli：命令工具，提供快速搭建项目原型、启动服务、执行Groovy脚本等功能。
  * spring-boot-dependencies：依赖和插件的版本信息。
  * spring-boot-devtools：开发者工具，提供热部署、实时加载、禁用缓存等提升开发效率的功能。
  * spring-boot-docs：参考文档相关内容。
  * spring-boot-parent：spring-boot-dependencies的子模块，是其他项目的父模块。
  * spring-boot-properties-migrator：Spring Boot 2.0版本新增的模块，支持升级版本配置属性的迁移。
  * spring-boot-starters：Spring Boot以预定义的方式集成了其他应用的starter集合。
  * spring-boot-test：测试功能相关代码。
  * spring-boot-test-autoconfigure：测试功能自动配置相关代码。
  * spring-boot-tools：Spring Boot工具支持模块，包含Ant、Maven、Gradle等构建工具。
* spring-boot-tests：Spring Boot部署及集成的测试。

## 设计目标

* 整合成熟技术框架、屏蔽系统复杂性、简化已有技术的使用，从而降低软件的使用门槛，提升软件开发和运维的效率。

## 整体架构

![](https://pic.imgdb.cn/item/60af048e08f74bc159d17bb7.jpg)

## 核心原理

![](https://pic.imgdb.cn/item/60af04cd08f74bc159d3ce7c.jpg)

​	可以用一句话来描述整个过程：Spring Boot通过@EnableAutoConfiguration注解开启自动配置，加载spring.factories中注册的各种AutoConfiguration类，当某个AutoConfiguration类满足其注解@Conditional指定的生效条件（Starters提供的依赖、配置或Spring容器中是否存在某个Bean等）时，实例化该AutoConfiguration类中定义的Bean（组件等），并注入Spring容器，就可以完成依赖框架的自动配置。

* @EnableAutoConfiguration：该注解由组合注解@SpringBootApplication引入，完成自动配置开启，扫描各个jar包下的spring.factories文件，并加载文件中注册的AutoConfiguration类等。
* spring.factories：配置文件，位于jar包的META-INF目录下，按照指定格式注册了自动配置的AutoConfiguration类。spring.factories也可以包含其他类型待注册的类。该配置文件不仅存在于Spring Boot项目中，也可以存在于自定义的自动配置（或Starter）项目中。
* AutoConfiguration类：自动配置类，代表了Spring Boot中一类以XXAutoConfiguration命名的自动配置类。其中定义了三方组件集成Spring所需初始化的Bean和条件。
* @Conditional：条件注解及其衍生注解，在AutoConfiguration类上使用，当满足该条件注解时才会实例化AutoConfiguration类。
* Starters：三方组件的依赖及配置，Spring Boot已经预置的组件。Spring Boot默认的Starters项目往往只包含了一个pom依赖的项目。如果是自定义的starter，该项目还需包含spring.factories文件、AutoConfiguration类和其他配置类。

### 注解@EnableAutoConfiguration功能分析

​	主要功能: 启动Spring应用程序上下文时进行自动配置，会尝试猜测配置项目可能需要的Bean。

@EnableAutoConfiguration注解提供了一个常量和两个成员参数的定义。

* ENABLED_OVERRIDE_PROPERTY：用来覆盖开启/关闭自动配置的功能。
* exclude：根据类（Class）排除指定的自动配置。
* excludeName：根据类名排除指定的自动配置。



​	需要注意的是，被@EnableAutoConfiguration注解的类所在package还具有特定的意义，通常会被作为扫描注解@Entity的根路径。这也是在使用@SpringBootApplication注解时需要将被注解的类放在顶级package下的原因，如果放在较低层级，它所在package的同级或上级中的类就无法被扫描到。

### AutoConfigurationImportSelector源码解析

​	关键功能是通过@Import注解导入的ImportSelector来完成的。

​	@Import(AutoConfigurationImportSelector.class)是@EnableAutoConfiguration注解的组成部分，也是自动配置功能的核心实现者。	@Import(AutoConfigurationImportSele-ctor.class)又可以分为两部分：@Import和对应的ImportSelector。本节重点讲解@Import的基本使用方法和ImportSelector的实现类AutoConfigurationImportSelector。

#### @Import注解

​	@Import注解位于spring-context项目内，主要提供导入配置类的功能。为什么要专门讲解@Import的功能及使用呢？如果查看Spring Boot的源代码，我们会发现大量的EnableXXX类都使用了该注解。了解@Import注解的基本使用方法，能够帮助我们更好地进行源代码的阅读和理解。

```java
@Target(ElementType.TYPE)
@Retention(RetentionPolicy.RUNTIME)
@Documented
public @interface Import {

	/**
	 * {@link Configuration @Configuration}, {@link ImportSelector},
	 * {@link ImportBeanDefinitionRegistrar}, or regular component classes to import.
	 */
	Class<?>[] value();

}
```

#### ImportSelector接口

ImportSelector决定可引入哪些@Configuration。ImportSelector接口源码如下。

```java
public interface ImportSelector {
    String[] selectImports(AnnotationMetadata importingClassMetadata);
}
```

​	如果实现了接口ImportSelector的类的同时又实现了以下4个Aware接口，那么Spring保证在调用ImportSelector之前会先调用Aware接口的方法。这4个接口为

EnvironmentAware、BeanFactoryAware、BeanClassLoaderAware和ResourceLoaderAware。

在AutoConfigurationImportSelector的源代码中就实现了这4个接口，部分源代码及Aware的全限定名代码如下。

```java
import org.springframework.beans.factory.BeanClassLoaderAware; 
import org.springframework.beans.factory.BeanFactoryAware; 
import org.springframework.context.EnvironmentAware;
import org.springframework.context.ResourceLoaderAware;
import org.springframework.context.annotation.DeferredImportSelector;
import org.springframework.core.Ordered;

public class AutoConfigurationImportSelector
    implements DeferredImportSelector, BeanClassLoaderAware, ResourceLoaderAware,
    BeanFactoryAware, EnvironmentAware, Ordered {
...
}
```

​	AutoConfigurationImportSelector并没有直接实现ImportSelector接口，而是实现了它的子接口DeferredImportSelector。DeferredImportSelector接口与ImportSelector的区别是，前者会在所有的@Configuration类加载完成之后再加载返回的配置类，而ImportSelector是在加载完@Configuration类之前先去加载返回的配置类。

​	DeferredImportSelector的加载顺序可以通过@Order注解或实现Ordered接口来指定。同时，DeferredImportSelector提供了新的方法getImportGroup()来跨DeferredImportSelector实现自定义Configuration的加载顺序。

#### AutoConfigurationImportSelector功能概述

![](https://pic.imgdb.cn/item/60af292108f74bc159597890.jpg)

AutoConfigurationImportSelector的selectImports方法源代码如下：

```java
@Override
	public String[] selectImports(AnnotationMetadata annotationMetadata) {
		if (!isEnabled(annotationMetadata)) {
			return NO_IMPORTS;
		}
		AutoConfigurationMetadata autoConfigurationMetadata = AutoConfigurationMetadataLoader
				.loadMetadata(this.beanClassLoader);
		AutoConfigurationEntry autoConfigurationEntry = getAutoConfigurationEntry(autoConfigurationMetadata,
				annotationMetadata);
		return StringUtils.toStringArray(autoConfigurationEntry.getConfigurations());
	}

```

#### @EnableAutoConfiguration自动配置开关

​	检查自动配置是否开启的代码位于AutoConfigurationImportSelector的selectImports方法第一段中。如果开启自动配置功能，就继续执行后续操作；如果未开启，就返回空数组。代码如下。

```java
protected boolean isEnabled(AnnotationMetadata metadata) {
    if (getClass() == AutoConfigurationImportSelector.class) {
        return getEnvironment().getProperty(
            EnableAutoConfiguration.ENABLED_OVERRIDE_PROPERTY, Boolean.class,
            true);
    }
protected AutoConfigurationEntry getAutoConfigurationEntry(
        AutoConfigurationMetadata autoConfigurationMetadata,
        AnnotationMetadata annotationMetadata) {
    if (!isEnabled(annotationMetadata)) {
		return EMPTY_ENTRY;
    }
    AnnotationAttributes attributes = getAttributes(annotationMetadata);
    // 通过SpringFactoriesLoader类提供的方法加载类路径中META-INF目录下的
    // spring.factories文件中针对EnableAutoConfiguration的注册配置类
    List<String> configurations = getCandidateConfigurations(annotationMetadata,
        attributes);
    // 对获得的注册配置类集合进行去重处理，防止多个项目引入同样的配置类
    configurations = removeDuplicates(configurations);
    // 获得注解中被exclude或excludeName所排除的类的集合
    Set<String> exclusions = getExclusions(annotationMetadata, attributes);
    // 检查被排除类是否可实例化，是否被自动注册配置所使用，不符合条件则抛出异常
    checkExcludedClasses(configurations, exclusions);
    // 从自动配置类集合中去除被排除的类
    configurations.removeAll(exclusions);
    // 检查配置类的注解是否符合spring.factories文件中AutoConfigurationImportFilter指定的注解检查条件
    configurations = filter(configurations, autoConfigurationMetadata);
    // 将筛选完成的配置类和排查的配置类构建为事件类，并传入监听器。监听器的配置在于spring.factories文件中，通过AutoConfigurationImportListener指定
    fireAutoConfigurationImportEvents(configurations, exclusions);
    return new AutoConfigurationEntry(configurations, exclusions);
}
```

​	通过isEnabled方法可以看出，如果当前类为AutoConfigurationImportSelector，程序会从环境中获取key为EnableAutoConfiguration.ENABLED_OVERRIDE_PROPERTY的配置，该常量的值为spring.boot.enableautoconfiguration。如果获取不到该属性的配置，isEnabled默认为true，也就是默认会使用自动配置。如果当前类为其他类，直接返回true。

​	如果想覆盖或重置EnableAutoConfiguration.ENABLED_OVERRIDE_PROPERTY的配置，可获取该常量的值，并在application.properties或application.yml中针对此参数进行配置。以application.properties配置关闭自动配置为例，代码如下。
spring.boot.enableautoconfiguration=false

#### @EnableAutoConfiguration加载元数据配置

```java
@Override
public String[] selectImports(AnnotationMetadata annotationMetadata) {
    ...
    AutoConfigurationMetadata autoConfigurationMetadata = AutoConfigurationMetadataLoader
        .loadMetadata(this.beanClassLoader);
    ...
}
```

​	加载元数据的配置用到了AutoConfigurationMetadataLoader类提供的loadMetaData方法，该方法会默认加载类路径下META-INF/spring-autoconfigure-metadata.properties内的配置。

```java
final class AutoConfigurationMetadataLoader {
    // 默认加载元数据的路径
    protected static final String PATH = "META-INF/spring-autoconfigure-metadata.
properties";
	// 默认调用改方法，传入默认PATH
    static AutoConfigurationMetadata loadMetadata(ClassLoader classLoader) {
        return loadMetadata(classLoader, PATH);
    }

    static AutoConfigurationMetadata loadMetadata(ClassLoader classLoader, String 
path) {
        try {
            // 获取数据存储于Enumeration中
            Enumeration<URL> urls = (classLoader != null) ? classLoader.getResources
(path)
                    : ClassLoader.getSystemResources(path);
            Properties properties = new Properties();
            while (urls.hasMoreElements()) {
                // 遍历Enumeration中的URL，加载其中的属性，存储到Properties中
                properties.putAll(PropertiesLoaderUtils.loadProperties(new UrlResource(urls.nextElement())));
            }
            return loadMetadata(properties);
        } catch (IOException ex) {
            throw new IllegalArgumentException("Unable to load @ConditionalOnClass location [" + path + "]", ex);
        }
    }

    // 创建AutoConfigurationMetadata的实现类PropertiesAutoConfigurationMetadata
    static AutoConfigurationMetadata loadMetadata(Properties properties) {
        return new PropertiesAutoConfigurationMetadata(properties);
    }
 	// AutoConfigurationMetadata的内部实现类
    private static class PropertiesAutoConfigurationMetadata implements AutoCon-
figurationMetadata {
    ...
    }
    ...
}
```

​	spring-autoconfigure-metadata.properties文件内的配置格式如下。
自动配置类的全限定名.注解名称=值

​	如果spring-autoconfigure-metadata.properties文件内有多个值，就用英文逗号分隔, 例如：
...
org.springframework.boot.autoconfigure.data.jdbc.JdbcRepositoriesAutoConfiguration.ConditionalOnClass=org.springframework.data.jdbc.repository.config.JdbcConfiguration,org.springframework.jdbc.core.namedparam.NamedParameterJdbcOperations
...

#### @EnableAutoConfiguration加载自动配置组件

​	加载自动配置组件是自动配置的核心组件之一，这些自动配置组件在类路径中META-INF目录下的spring.factories文件中进行注册。Spring Boot预置了一部分常用组件，如果我们需要创建自己的组件，可参考Spring Boot预置组件在自己的Starters中进行配置。

​	SpringFactoriesLoader类可以读取spring.factories文件中注册的类。下面我们通过源代码来看一下如何在AutoConfigurationImportSelector类中通过getCandidateConfigurations方法来读取spring.factories文件中注册的类。

```java
protected List<String> getCandidateConfigurations(AnnotationMetadata metadata,
        AnnotationAttributes attributes) {
    List<String> configurations = SpringFactoriesLoader.loadFactoryNames(
        getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader());
    Assert.notEmpty(configurations,
        "No auto configuration classes found in META-INF/spring.factories. If you "
            + "are using a custom packaging, make sure that file is correct.");
	return configurations;
}

protected Class<?> getSpringFactoriesLoaderFactoryClass() {
    return EnableAutoConfiguration.class;
}
```

​	getCandidateConfigurations方法使用SpringFactoriesLoader类提供的loadFactoryNames方法来读取META-INF/spring.factories中的配置。如果程序未读取到任何配置内容，会抛出异常信息。而loadFactoryNames方法的第一个参数为getSpringFactoriesLoaderFactoryClass方法返回的EnableAutoConfiguration.class，也就是说loadFactoryNames只会读取配置文件中针对自动配置的注册类。
SpringFactoriesLoader类的loadFactoryNames方法相关代码如下。

```java
public final class SpringFactoriesLoader {

    // 概类加载文件的路径，可能存在多个
    public static final String FACTORIES_RESOURCE_LOCATION = "META-INF/spring.factories";
    ...
    // 加载所有的META-INF/spring.factories文件，封装成Map，并从中获取指定类名的列表
	public static List<String> loadFactoryNames(Class<?> factoryClass, @Nullable ClassLoader classLoader) {
        String factoryClassName = factoryClass.getName();
        return loadSpringFactories(classLoader).getOrDefault(factoryClassName, 
Collections.emptyList());
    }
    // 加载所有的META-INF/spring.factories文件，封装成Map，Key为接口的全类名，Value为对应配置值的List集合
    private static Map<String, List<String>> loadSpringFactories(@Nullable ClassLoader classLoader) {
        MultiValueMap<String, String> result = cache.get(classLoader);
        if (result != null) {
            return result;
        }

        try {
            Enumeration<URL> urls = (classLoader != null ?
                classLoader.getResources(FACTORIES_RESOURCE_LOCATION) :
                ClassLoader.getSystemResources(FACTORIES_RESOURCE_LOCATION));
            result = new LinkedMultiValueMap<>();
            while (urls.hasMoreElements()) {
                URL url = urls.nextElement();
                UrlResource resource = new UrlResource(url);
                Properties properties = PropertiesLoaderUtils.loadProperties(resource);
                for (Map.Entry<?, ?> entry : properties.entrySet()) {
                    String factoryClassName = ((String) entry.getKey()).trim();
                    for (String factoryName : StringUtils.commaDelimitedListTo-
StringArray((String) entry.getValue())) {
                        result.add(factoryClassName, factoryName.trim());
                    }
                }
  	}
            cache.put(classLoader, result);
            return result;
        } catch (IOException ex) {
            throw new IllegalArgumentException("Unable to load factories from location [" +
                FACTORIES_RESOURCE_LOCATION + "]", ex);
        }
    }
...
}
```

​	单描述以上加载的过程就是：SpringFactoriesLoader加载器加载指定ClassLoader下面的所有META-INF/spring.factories文件，并将文件解析内容存于`Map<String,List<String>>`内。然后，通过loadFactoryNames传递过来的class的名称从Map中获得该类的配置列表。
结合下面spring.factories文件的内容格式，我们可以更加清晰地了解`Map<String,List<String>>`中都存储了什么。

```properties
.
# Auto Configure
org.springframework.boot.autoconfigure.EnableAutoConfiguration=\
org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfig-uration,\
org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\
org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\
org.springframework.boot.autoconfigure.batch.BatchAutoConfiguration,\
org.springframework.boot.autoconfigure.cache.CacheAutoConfiguration,\
org.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration,\
...
```

#### @EnableAutoConfiguration排除指定组件

```java
protected Set<String> getExclusions(AnnotationMetadata metadata,
        AnnotationAttributes attributes) {
    // 创建Set集合并把待排除的内容存于集合内，LinkedHashSet具有不可重复性
    Set<String> excluded = new LinkedHashSet<>();
    excluded.addAll(asList(attributes, "exclude"));
    excluded.addAll(Arrays.asList(attributes.getStringArray("excludeName")));
    excluded.addAll(getExcludeAutoConfigurationsProperty());
    return excluded;
}

private List<String> getExcludeAutoConfigurationsProperty() {
    if (getEnvironment() instanceof ConfigurableEnvironment) {
        Binder binder = Binder.get(getEnvironment());
        return binder.bind(PROPERTY_NAME_AUTOCONFIGURE_EXCLUDE, String[].class)
            .map(Arrays::asList).orElse(Collections.emptyList());
  	}
    String[] excludes = getEnvironment()
            .getProperty(PROPERTY_NAME_AUTOCONFIGURE_EXCLUDE, String[].class);
    return (excludes != null) ? Arrays.asList(excludes) : Collections.emptyList();
}
```

​	AutoConfigurationImportSelector中通过调用getExclusions方法来获取被排除类的集合。它会收集@EnableAutoConfiguration注解中配置的exclude属性值、excludeName属性值，并通过方法getExcludeAutoConfigurationsProperty获取在配置文件中key为spring.autoconfigure.exclude的配置值。以排除自动配置DataSourceAutoConfiguration为例，配置文件中的配置形式如下。
spring.autoconfigure.exclude=org.springframework.boot.autoconfigure.jdbc.DataSource-
AutoConfiguration

获取到被排除组件的集合之后，首先是对待排除类进行检查操作，代码如下

```java
private void checkExcludedClasses(List<String> configurations,
        Set<String> exclusions) {
    List<String> invalidExcludes = new ArrayList<>(exclusions.size());
    // 遍历并判断是否存在对应的配置类
	for (String exclusion : exclusions) {
        if (ClassUtils.isPresent(exclusion, getClass().getClassLoader())
                && !configurations.contains(exclusion)) {
            invalidExcludes.add(exclusion);
        }
    }
    // 如果不为空，就进行处理
    if (!invalidExcludes.isEmpty()) {
        handleInvalidExcludes(invalidExcludes);
    }
}
// 抛出指定异常
protected void handleInvalidExcludes(List<String> invalidExcludes) {
    StringBuilder message = new StringBuilder();
    for (String exclude : invalidExcludes) {
        message.append("\t- ").append(exclude).append(String.format("%n"));
    }
    throw new IllegalStateException(String
            .format("The following classes could not be excluded because they are"
                + " not auto-configuration classes:%n%s", message));
}
```

​	以上代码中，checkExcludedClasses方法用来确保被排除的类存在于当前的ClassLoader中，并且包含在spring.factories注册的集合中。如果不满足以上条件，调用handleInvalidExcludes方法抛出异常。
​	如果被排除类都符合条件，调用configurations.removeAll(exclusions)方法从自动配置集合中移除被排除集合的类，至此完成初步的自动配置组件排除。

#### @EnableAutoConfiguration过滤自动配置组件

​	当完成初步的自动配置组件排除工作之后，AutoConfigurationImportSelector会结合在此之前获取的AutoConfigurationMetadata对象，对组件进行再次过滤。

```java
private List<String> filter(List<String> configurations,
        AutoConfigurationMetadata autoConfigurationMetadata) {
    ...
}
protected List<AutoConfigurationImportFilter> getAutoConfigurationImportFilters() {
...
}
```

​	在明确了以上信息之后，该filter方法的过滤功能就很简单了。用一句话来概述就是：对自动配置组件列表进行再次过滤，过滤条件为该列表中自动配置类的注解得包含在OnBeanCondition、OnClassCondition和OnWebApplicationCondition中指定的注解，依次包含@ConditionalOnBean、@ConditionalOnClass和@ConditionalOnWebApplication。

​	那么在这个实现过程中，AutoConfigurationMetadata对应的元数据和AutoConfiguration-ImportFilter接口及其实现类是如何进行具体筛选的呢？我们先来看一下AutoConfiguration-ImportFilter接口相关类的结构及功能，如图2-4所示。

![](https://pic.imgdb.cn/item/60af43d908f74bc159c1fd23.jpg)

下面进行相关的源代码及步骤的分解。我们已经知道AutoConfigurationImportFilter接口可以在spring.factories中注册过滤器，用来过滤自动配置类，在实例化之前快速排除不需要的自动配置，代码如下。

```java
@FunctionalInterface
public interface AutoConfigurationImportFilter {
    boolean[] match(String[] autoConfigurationClasses,
        AutoConfigurationMetadata autoConfigurationMetadata);
}
```

​	match返回的结果为匹配过滤后的结果布尔数组，数组的大小与String[ ] autoConfigurationClasses一致，如果需排除，设置对应值为false。

​	AutoConfigurationImportFilter接口的match方法主要在其抽象子类中实现，而抽象子类FilteringSpringBootCondition在实现match方法的同时又定义了新的抽象方法getOutcomes，继承该抽象类的其他3个子类均实现了getOutcomes方法

```java
abstract class FilteringSpringBootCondition extends SpringBootCondition
        implements AutoConfigurationImportFilter, BeanFactoryAware, BeanClassLoader
Aware {
    ...
    @Override
    public boolean[] match(String[] autoConfigurationClasses,
            AutoConfigurationMetadata autoConfigurationMetadata) {
        ...
        ConditionOutcome[] outcomes = getOutcomes(autoConfigurationClasses,
            autoConfigurationMetadata);
        boolean[] match = new boolean[outcomes.length];
        for (int i = 0; i < outcomes.length; i++) {
            match[i] = (outcomes[i] == null || outcomes[i].isMatch());
     ...
        }
        return match;
    }

    // 过滤核心功能，该方法由子类实现
    protected abstract ConditionOutcome[] getOutcomes(String[] autoConfigur
ationClasses,
         AutoConfigurationMetadata autoConfigurationMetadata);
    ...
}
```

​	match方法在抽象类FilteringSpringBootCondition中主要的功能就是调用getOutcomes方法，并将其返回的结果转换成布尔数组。而相关的过滤核心功能由子类实现的getOutcomes方法来实现。

下面以实现类OnClassCondition来具体说明执行过程。首先看一下入口方法getOutcomes的源代码。

```java
@Order(Ordered.HIGHEST_PRECEDENCE)
class OnClassCondition extends FilteringSpringBootCondition {
    @Override
    protected final ConditionOutcome[] getOutcomes(String[] autoConfigurationClasses,
            AutoConfigurationMetadata autoConfigurationMetadata) {
        // 如果有多个处理器，采用后台线程处理
        if (Runtime.getRuntime().availableProcessors() > 1) {
            return resolveOutcomesThreaded(autoConfigurationClasses, autoConfigu-
rationMetadata);
        } else {
        OutcomesResolver outcomesResolver = new 
        StandardOutcomesResolver(autoConfigurationClasses, 0,
            autoConfigurationClasses.length, autoConfigurationMetadata, 
            getBeanClassLoader());
        return outcomesResolver.resolveOutcomes();
        }
    }
    ...
}
```

​	Spring Boot当前版本对getOutcomes方法进行了性能优化，根据处理器的情况不同采用了不同的方式进行操作。如果使用多个处理器，采用后台线程处理（之前版本的实现方法）。否则，getOutcomes直接创建StandardOutcomesResolver来处理。

​	在resolveOutcomesThreaded方法中主要采用了分半处理的方法来提升处理效率，而核心功能都是在内部类StandardOutcomesResolver的resolveOutcomes方法中实现。
resolveOutcomesThreaded的分半处理实现代码如下。

```java
@Order(Ordered.HIGHEST_PRECEDENCE)
class OnClassCondition extends FilteringSpringBootCondition {
    private ConditionOutcome[] resolveOutcomesThreaded(String[] autoConfigurationClasses,
            AutoConfigurationMetadata autoConfigurationMetadata) {
        int split = autoConfigurationClasses.length / 2;
        OutcomesResolver firstHalfResolver = createOutcomesResolver(autoConfigu-
rationClasses, 0, split, autoConfigurationMetadata);
        OutcomesResolver secondHalfResolver = new StandardOutcomesResolver(au-
toConfigurationClasses, split,
            autoConfigurationClasses.length, autoConfigurationMetadata, getBean-
ClassLoader());
        ConditionOutcome[] secondHalf = secondHalfResolver.resolveOutcomes();
        ConditionOutcome[] firstHalf = firstHalfResolver.resolveOutcomes();
        ConditionOutcome[] outcomes = new ConditionOutcome[autoConfigurationClas-
ses.length];
        System.arraycopy(firstHalf, 0, outcomes, 0, firstHalf.length);
        System.arraycopy(secondHalf, 0, outcomes, split, secondHalf.length);
        return outcomes;
    }
    ...
        // 判断该类是否符合条件
        private ConditionOutcome getOutcome(String className, ClassLoader class-
Loader) {
            if (ClassNameFilter.MISSING.matches(className, classLoader)) {
                return ConditionOutcome.noMatch(ConditionMessage
                        .forCondition(ConditionalOnClass.class)
                        .didNotFind("required class").items(Style.QUOTE, className));
            }
            return null;
        }
    }

}
```

![](https://pic.imgdb.cn/item/60af562c08f74bc159ac50fa.jpg)

#### @EnableAutoConfiguration事件注册

在完成了以上步骤的过滤、筛选之后，我们最终获得了要进行自动配置的类的集合，在将该集合返回之前，在AutoConfigurationImportSelector类中完成的最后一步操作就是相关事件的封装和广播，相关代码如下。

```java
private void fireAutoConfigurationImportEvents(List<String> configurations,
        Set<String> exclusions) {
    List<AutoConfigurationImportListener> listeners = getAutoConfigurationImportListeners();
    if (!listeners.isEmpty()) {
        AutoConfigurationImportEvent event = new AutoConfigurationImportEvent(this,
                configurations, exclusions);
        for (AutoConfigurationImportListener listener : listeners) {
            invokeAwareMethods(listener);
            listener.onAutoConfigurationImportEvent(event);
        }
    }
}

protected List<AutoConfigurationImportListener> getAutoConfigurationImportListeners() {
    return SpringFactoriesLoader.loadFactories(AutoConfigurationImportListener.class, this.beanClassLoader);
}
```

​	以上代码首先通过SpringFactoriesLoader类提供的loadFactories方法将spring.factories中配置的接口AutoConfigurationImportListener的实现类加载出来。然后，将筛选出的自动配置类集合和被排除的自动配置类集合封装成AutoConfigurationImportEvent事件对象，并传入该事件对象通过监听器提供的onAutoConfigurationImportEvent方法，最后进行事件广播。关于事件及事件监听相关的内容不在此过多展开。

### @Conditional条件注解

#### 认识条件注解

@Conditional注解是由Spring 4.0版本引入的新特性，可根据是否满足指定的条件来决定是否进行Bean的实例化及装配，比如，设定当类路径下包含某个jar包的时候才会对注解的类进行实例化操作。总之，就是根据一些特定条件来控制Bean实例化的行为，@Conditional注解代码如下。

```java
@Target({ElementType.TYPE, ElementType.METHOD})
@Retention(RetentionPolicy.RUNTIME)
@Documented
public @interface Conditional {
    Class<? extends Condition>[] value();
}
```

@Conditional注解唯一的元素属性是接口Condition的数组，只有在数组中指定的所有Condition的matches方法都返回true的情况下，被注解的类才会被加载。我们前面讲到的OnClassCondition类就是Condition的子类之一，相关代码如下。

```java
@FunctionalInterface
public interface Condition {
    // 决定条件是否匹配
    boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata);
}
```

#### 条件注解的衍生注解

* @ConditionalOnBean：在容器中有指定Bean的条件下。
* @ConditionalOnClass：在classpath类路径下有指定类的条件下。
* @ConditionalOnCloudPlatform：当指定的云平台处于active状态时。
* @ConditionalOnExpression：基于SpEL表达式的条件判断。
* @ConditionalOnJava：基于JVM版本作为判断条件。
* @ConditionalOnJndi：在JNDI存在的条件下查找指定的位置
* @ConditionalOnMissingBean：当容器里没有指定Bean的条件时。
* @ConditionalOnMissingClass：当类路径下没有指定类的条件时。
* @ConditionalOnNotWebApplication：在项目不是一个Web项目的条件下。
* @ConditionalOnProperty：在指定的属性有指定值的条件下。
* ·@ConditionalOnResource：类路径是否有指定的值。
* @ConditionalOnSingleCandidate：当指定的Bean在容器中只有一个或者有多个但是指定了首选的Bean时。
* @ConditionalOnWebApplication：在项目是一个Web项目的条件下。

![](https://pic.imgdb.cn/item/60af80d108f74bc15947fbcc.jpg)

```java
public abstract class SpringBootCondition implements Condition {

    @Override
    public final boolean matches(ConditionContext context,
 	AnnotatedTypeMetadata metadata) {
        ...
        ConditionOutcome outcome = getMatchOutcome(context, metadata);
        ...
        return outcome.isMatch();
        ...
    }

    ...
    public abstract ConditionOutcome getMatchOutcome(ConditionContext context,
        AnnotatedTypeMetadata metadata);
    ...
}
                                
```

​	在抽象类SpringBootCondition中实现了matches方法，而该方法中最核心的部分是通过调用新定义的抽象方法getMatchOutcome并交由子类来实现，在matches方法中根据子类返回的结果判断是否匹配。下面我们来看OnWebApplicationCondition的源代码是如何实现相关功能的。

```java
@Order(Ordered.HIGHEST_PRECEDENCE + 20)
class OnWebApplicationCondition extends FilteringSpringBootCondition {
    ...
    @Override
public ConditionOutcome getMatchOutcome(ConditionContext context,
            AnnotatedTypeMetadata metadata) {
        boolean required = metadata.isAnnotated(ConditionalOnWebApplication.class.
getName());
        ConditionOutcome outcome = isWebApplication(context, metadata, required);
        if (required && !outcome.isMatch()) {
            return ConditionOutcome.noMatch(outcome.getConditionMessage());
        }
        if (!required && outcome.isMatch()) {
            return ConditionOutcome.noMatch(outcome.getConditionMessage());
        }
        return ConditionOutcome.match(outcome.getConditionMessage());
    }
    ...
}
```

可以看出，是否匹配是由两个条件决定的：被注解的类或方法是否包含ConditionalOn-WebApplication注解，是否为Web应用。

* 如果包含ConditionalOnWebApplication注解，并且不是Web应用，那么返回不匹配。
* 如果不包含ConditionalOnWebApplication注解，并且是Web应用，那么返回不匹配
* 其他情况，返回匹配。

下面我们以SERVLET Web应用为例，看相关源代码是如何判断是否为Web应用的。REACTIVE Web应用和其他类型的Web应用可参照学习。

```java
@Order(Ordered.HIGHEST_PRECEDENCE + 20)
class OnWebApplicationCondition extends FilteringSpringBootCondition {

    private static final String SERVLET_WEB_APPLICATION_CLASS = "org.springframework.
web.context.support.GenericWebApplicationContext";
    ...
    // 推断Web应用是否匹配
    private ConditionOutcome isWebApplication(ConditionContext context,
            AnnotatedTypeMetadata metadata, boolean required) {
        switch (deduceType(metadata)) {
        case SERVLET:
            // 是否为SERVLET
            return isServletWebApplication(context);
        case REACTIVE:
            // 是否为REACTIVE
            return isReactiveWebApplication(context);
        default:
            // 其他
            return isAnyWebApplication(context, required);
        }
 }

    private ConditionOutcome isServletWebApplication(ConditionContext context) {
        ConditionMessage.Builder message = ConditionMessage.forCondition("");
        // 判断常量定义类是否存在
        if (!ClassNameFilter.isPresent(SERVLET_WEB_APPLICATION_CLASS,
                context.getClassLoader())) {
            return ConditionOutcome.noMatch(
                    message.didNotFind("servlet web application classes").atAll());
        }
        // 判断BeanFactory是否存在
        if (context.getBeanFactory() != null) {
            String[] scopes = context.getBeanFactory().getRegisteredScopeNames();
            if (ObjectUtils.containsElement(scopes, "session")) {
                return ConditionOutcome.match(message.foundExactly("'session' scope"));
            }
        }

        // 判断Environment的类型是否为ConfigurableWebEnvironment类型
        if (context.getEnvironment() instanceof ConfigurableWebEnvironment) {
            return ConditionOutcome
                .match(message.foundExactly("ConfigurableWebEnvironment"));
        }
        // 判断ResourceLoader的类型是否为WebApplicationContext类型
        if (context.getResourceLoader() instanceof WebApplicationContext) {
            return ConditionOutcome.match(message.foundExactly("WebApplicationContext"));
        }
        return ConditionOutcome.noMatch(message.because("not a servlet web-application"));
    }
    ...
    // 从AnnotatedTypeMetadata中获取type值
    private Type deduceType(AnnotatedTypeMetadata metadata) {
        Map<String, Object> attributes = metadata
                .getAnnotationAttributes(ConditionalOnWebApplication.class.getName());
        if (attributes != null) {
            return (Type) attributes.get("type");
        }
        return Type.ANY;
    }
}
```

# 深入浅出SpringSecurity

架构概览

## 核心功能

* 认证
* 授权

### 认证

* 表单认证
* OAuth2.0认证
* SAML2.0认证
* CAS认证
* Remember me自动认证。
* JAAS认证。
* OpenID去中心化认证。
* Pre-Authentication Scenarios认证。
* X509认证
* Http Basic认证。
* Http Digest认证。

### 授权

* 基于URL的请求授权
* 方法访问授权
* SpEL访问控制
* 域对象安全(ACL)
* 动态权限配置
* RBAC权限模型。

### 其他

* 安全管理。

## 整体架构

### 认证和授权

#### 认证

认证信息保存在Authentication的实现类

```java
public interface Authentication extends Principal, Serializable {
       Collection<? extends GrantedAuthority> getAuthorities();
       Object getCredentials();
       Object getDetails();
       Object getPrincipal();
       boolean isAuthenticated();
       void setAuthenticated(boolean isAuthenticated);
    }
```

* getAuthorities方法：用来获取用户的权限。
* getCredentials方法：用来获取用户凭证，一般来说就是密码。
* getDetails方法：用来获取用户携带的详细信息，可能是当前请求之类。
* getPrincipal方法：用来获取当前用户，例如是一个用户名或者一个用户对象。
* isAuthenticated：当前用户是否认证成功。



​	当用户使用用户名／密码登录或使用Remember-me登录时，都会对应一个不同的Authentication实例。

​	Spring Security中的认证工作主要由AuthenticationManager接口来负责，下面来看一下该接口的定义：

```java
 public interface AuthenticationManager {
       Authentication authenticate(Authentication authentication)
           throws AuthenticationException;
    }
```

* 返回Authentication，表示认证成功。
* 抛出AuthenticationException异常，表示用户输入了无效的凭证。
* 返回null，表示不能断定。



​	AuthenticationManager最主要的实现类是ProviderManager，ProviderManager管理了众多的AuthenticationProvider实例，AuthenticationProvider有点类似于AuthenticationManager，但是它多了一个supports方法用来判断是否支持给定的Authentication类型。

```java
   public interface AuthenticationProvider {
       Authentication authenticate(Authentication authentication)
	throws AuthenticationException;
       boolean supports(Class<?> authentication);
    }
```

#### 授权

两个关键接口

* AccessDecisionManager
* AccessDecisionVoter



​	AccessDecisionVoter是一个投票器，投票器会检查用户是否具备应有的角色，进而投出赞成、反对或者弃权票；AccessDecisionManager则是一个决策器，来决定此次访问是否被允许。AccessDecisionVoter和AccessDecisionManager都有众多的实现类，在AccessDecisionManager中会挨个遍历AccessDecisionVoter，进而决定是否允许用户访问，因而AccessDecisionVoter和AccessDecisionManager两者的关系类似于AuthenticationProvider和ProviderManager的关系。

### Web安全

​	在Spring Security中，认证、授权等功能都是基于过滤器来完成的。

![](https://pic.imgdb.cn/item/60b0a0e808f74bc159797553.jpg)

​	开发者所见到的Spring Security提供的功能，都是通过这些过滤器来实现的，这些过滤器按照既定的优先级排列，最终形成一个过滤器链。开发者也可以自定义过滤器，并通过@Order注解去调整自定义过滤器在过滤器链中的位置。

​	默认过滤器并不是直接放在Web项目的原生过滤器链中，而是通过一个FilterChainProxy来统一管理。Spring Security中的过滤器链通过FilterChainProxy嵌入到Web项目的原生过滤器链中，如图1-1所示。

![](https://pic.imgdb.cn/item/60b0a15008f74bc1597e4d82.jpg)

### 登录数据保护

​	如果不使用Spring Security这一类的安全管理框架，大部分的开发者可能会将登录用户数据保存在Session中，事实上，Spring Security也是这么做的。但是，为了使用方便，Spring Security在此基础上还做了一些改进，其中最主要的一个变化就是线程绑定。

​	当用户登录成功后，Spring Security会将登录成功的用户信息保存到SecurityContextHolder中。SecurityContextHolder中的数据保存默认是通过ThreadLocal来实现的，使用ThreadLocal创建的变量只能被当前线程访问，不能被其他线程访问和修改，也就是用户数据和请求线程绑定在一起。当登录请求处理完毕后，Spring Security会将SecurityContextHolder中的数据拿出来保存到Session中，同时将SecurityContextHolder中的数据清空。以后每当有请求到来时，Spring Security就会先从Session中取出用户登录数据，保存到SecurityContextHolder中，方便在该请求的后续处理过程中使用，同时在请求结束时将SecurityContextHolder中的数据拿出来保存到Session中，然后将SecurityContextHolder中的数据清空。

​	这一策略非常方便用户在Controller或者Service层获取当前登录用户数据，但是带来的另外一个问题就是，在子线程中想要获取用户登录数据就比较麻烦。Spring Security对此也提供了相应的解决方案，如果开发者使用@Async注解来开启异步任务的话，那么只需要添加如下配置，使用Spring Security提供的异步任务代理，就可以在异步任务中从Security ContextHolder里边获取当前登录用户的信息：

```java
@Configuration
    public class ApplicationConfiguration extends AsyncConfigurerSupport {
       @Override
       public Executor getAsyncExecutor() {
           return new DelegatingSecurityContextExecutorService(
                                                 Executors.newFixedThreadPool(5));
       }
    }
```

## Spring security认证

* 基本认证
* 登录表单配置
* 登录用户数据获取
* 用户的四种定义方式。

### 基本认证

```xml
<dependency>
       <groupId>org.springframework.boot</groupId>
       <artifactId>spring-boot-starter-security</artifactId>
    </dependency>
    <dependency>
       <groupId>org.springframework.boot</groupId>
       <artifactId>spring-boot-starter-web</artifactId>
    </dependency>
```

```java
	@RestController
    public class HelloController {
       @GetMapping("/hello")
       public String hello() {
           return "hello spring security";
       }
    }
```

​	接下来启动项目，/hello接口就已经被自动保护起来了。当用户访问/hello接口时，会自动跳转到登录页面，如图2-1所示，用户登录成功后，才能访问到/hello接口。

![](https://pic.imgdb.cn/item/60b0a3c008f74bc1599b1866.jpg)

#### 流程分析

![](https://pic.imgdb.cn/item/60b0c03408f74bc159c0064c.jpg)

（1）客户端（浏览器）发起请求去访问/hello接口，这个接口默认是需要认证之后才能访问的。
（2）这个请求会走一遍Spring Security中的过滤器链，在最后的FilterSecurityInterceptor过滤器中被拦截下来，因为系统发现用户未认证。请求拦截下来之后，接下来会抛出AccessDeniedException异常。
（3）抛出的AccessDeniedException异常在ExceptionTranslationFilter过滤器中被捕获，ExceptionTranslationFilter过滤器通过调用LoginUrlAuthenticationEntryPoint#commence方法给客户端返回302，要求客户端重定向到/login页面。

（4）客户端发送/login请求。
（5）/login请求被DefaultLoginPageGeneratingFilter过滤器拦截下来，并在该过滤器中返回登录页面。所以当用户访问/hello接口时会首先看到登录页面。

#### 原理分析

虽然开发者只是引入了一个依赖，代码不多，但是Spring Boot背后却默默做了很多事情：

* 开启Spring Security自动化配置，开启后，会自动创建一个名为springSecurityFilterChain的过滤器，并注入到Spring容器中，这个过滤器将负责所有的安全管理，包括用户的认证、授权、重定向到登录页面等（springSecurityFilterChain实际上代理了Spring Security中的过滤器链）。
* 创建一个UserDetailsService实例，UserDetailsService负责提供用户数据，默认的用户数据是基于内存的用户，用户名为user，密码则是随机生成的UUID字符串。
* 给用户生成一个默认的登录页面。
* 开启CSRF攻击防御。
* 开启会话固定攻击防御。
* 集成X-XSS-Protection。
* 集成X-Frame-Options以防止单击劫持。

##### 默认用户生成

​	Spring Security中定义了UserDetails接口来规范开发者自定义的用户对象，这样方便一些旧系统、用户表已经固定的系统集成到Spring Security认证体系中。

```java
public interface UserDetails extends Serializable {
       Collection<? extends GrantedAuthority> getAuthorities();
       String getPassword();
       String getUsername();
  	   boolean isAccountNonExpired();
       boolean isAccountNonLocked();
       boolean isCredentialsNonExpired();
       boolean isEnabled();
}

```

（1）getAuthorities方法：返回当前账户所具备的权限。
（2）getPassword方法：返回当前账户的密码。
（3）getUsername方法：返回当前账户的用户名。
（4）isAccountNonExpired方法：返回当前账户是否未过期。
（5）isAccountNonLocked方法：返回当前账户是否未锁定。
（6）isCredentialsNonExpired方法：返回当前账户凭证（如密码）是否未过期。
（7）isEnabled方法：返回当前账户是否可用。

这是用户对象的定义，而负责提供用户数据源的接口是UserDetailsService，UserDetailsService中只有一个查询用户的方法，代码如下：

```java
public interface UserDetailsService {
       UserDetails loadUserByUsername(String username)
                          throws UsernameNotFoundException;
    }
```

![](https://pic.imgdb.cn/item/60b1181b08f74bc1596f57c2.jpg)



UserDetailsManager在UserDetailsService的基础上，继续定义了添加用户、更新用户、删除用户、修改密码以及判断用户是否存在共5种方法。

* JdbcDaoImpl在UserDetailsService的基础上，通过spring-jdbc实现了从数据库中查询用户的方法。
* InMemoryUserDetailsManager实现了UserDetailsManager中关于用户的增删改查方法，不过都是基于内存的操作，数据并没有持久化
* JdbcUserDetailsManager继承自JdbcDaoImpl同时又实现了UserDetailsManager接口，因此可以通过JdbcUserDetailsManager实现对用户的增删改查操作，这些操作都会持久化到数据库中。不过JdbcUserDetailsManager有一个局限性，就是操作数据库中用户的SQL都是提前写好的，不够灵活，因此在实际开发中JdbcUserDetailsManager使用并不多。
* CachingUserDetailsService的特点是会将UserDetailsService缓存起来
* UserDetailsServiceDelegator则是提供了UserDetailsService的懒加载功能。
* ReactiveUserDetailsServiceAdapter是webflux-web-security模块定义的UserDetailsService实现。



​	当我们使用Spring Security时，如果仅仅只是引入一个Spring Security依赖，则默认使用的用户就是由InMemoryUserDetailsManager

​	Spring Boot之所以能够做到零配置使用Spring Security，就是因为它提供了众多的自动化配置类。其中，针对UserDetailsService的自动化配置类是UserDetailsServiceAutoConfiguration，这个类的源码并不长，我们一起来看一下：

```java
@Configuration(proxyBeanMethods = false)
@ConditionalOnClass(AuthenticationManager.class)
@ConditionalOnBean(ObjectPostProcessor.class)
@ConditionalOnMissingBean(
		value = { AuthenticationManager.class, AuthenticationProvider.class, UserDetailsService.class },
		type = { "org.springframework.security.oauth2.jwt.JwtDecoder",
				"org.springframework.security.oauth2.server.resource.introspection.OpaqueTokenIntrospector" })
public class UserDetailsServiceAutoConfiguration {

	private static final String NOOP_PASSWORD_PREFIX = "{noop}";

	private static final Pattern PASSWORD_ALGORITHM_PATTERN = Pattern.compile("^\\{.+}.*$");

	private static final Log logger = LogFactory.getLog(UserDetailsServiceAutoConfiguration.class);

	@Bean
	@ConditionalOnMissingBean(
			type = "org.springframework.security.oauth2.client.registration.ClientRegistrationRepository")
	@Lazy
	public InMemoryUserDetailsManager inMemoryUserDetailsManager(SecurityProperties properties,
			ObjectProvider<PasswordEncoder> passwordEncoder) {
		SecurityProperties.User user = properties.getUser();
		List<String> roles = user.getRoles();
		return new InMemoryUserDetailsManager(
				User.withUsername(user.getName()).password(getOrDeducePassword(user, passwordEncoder.getIfAvailable()))
						.roles(StringUtils.toStringArray(roles)).build());
	}

	private String getOrDeducePassword(SecurityProperties.User user, PasswordEncoder encoder) {
		String password = user.getPassword();
		if (user.isPasswordGenerated()) {
			logger.info(String.format("%n%nUsing generated security password: %s%n", user.getPassword()));
		}
		if (encoder != null || PASSWORD_ALGORITHM_PATTERN.matcher(password).matches()) {
			return password;
		}
		return NOOP_PASSWORD_PREFIX + password;
	}

}
```

（1）当前classpath下存在AuthenticationManager类。
（2）当前项目中，系统没有提供AuthenticationManager、AuthenticationProvider、UserDetailsService以及ClientRegistrationRepository实例。
默认情况下，上面的条件都会满足，此时Spring Security会提供一个InMemoryUser DetailsManager实例。从inMemoryUserDetailsManager方法中可以看到，用户数据源自SecurityProperties#getUser方法：

```java
 @ConfigurationProperties(prefix = "spring.security")
    public class SecurityProperties {
          private User user = new User();
          public User getUser() {
             return this.user;
          }
          public static class User {
             private String name = "user";
             private String password = UUID.randomUUID().toString();
             private List<String> roles = new ArrayList<>();
             //省略getter/setter
          }
    }
```

​	只要我们在项目的application.properties配置文件中添加如下配置，就能定制SecurityProperties.User类中各属性的值：

```properties
spring.security.user.name=javaboy
spring.security.user.password=123
spring.security.user.roles=admin,user
```

##### 默认页面生成

​	http://localhost:8080/logout就可以看到注销登录页面

​	在这些常见的过滤器中就包含两个和页面相关的过滤器：DefaultLoginPageGeneratingFilter和DefaultLogoutPage GeneratingFilter。



​	先来看DefaultLoginPageGeneratingFilter。作为Spring Security过滤器链中的一员，在第一次请求/hello接口的时候，就会经过DefaultLoginPageGeneratingFilter过滤器，但是由于/hello接口和登录无关，因此DefaultLoginPageGeneratingFilter过滤器并未干涉/hello接口。等到第二次重定向到/login页面的时候，这个时候就和DefaultLoginPageGeneratingFilter有关系了，此时请求就会在DefaultLoginPageGeneratingFilter中进行处理，生成登录页面返回给客户端。

```java
public class DefaultLoginPageGeneratingFilter extends GenericFilterBean {

   public static final String DEFAULT_LOGIN_PAGE_URL = "/login";

   public static final String ERROR_PARAMETER_NAME = "error";

   private String loginPageUrl;

   private String logoutSuccessUrl;

   private String failureUrl;

   private boolean formLoginEnabled;

   private boolean openIdEnabled;

   private boolean oauth2LoginEnabled;

   private boolean saml2LoginEnabled;

   private String authenticationUrl;

   private String usernameParameter;

   private String passwordParameter;

   private String rememberMeParameter;

   private String openIDauthenticationUrl;

   private String openIDusernameParameter;

   private String openIDrememberMeParameter;

   private Map<String, String> oauth2AuthenticationUrlToClientName;

   private Map<String, String> saml2AuthenticationUrlToProviderName;

   private Function<HttpServletRequest, Map<String, String>> resolveHiddenInputs = (request) -> Collections.emptyMap();

   public DefaultLoginPageGeneratingFilter() {
   }

   public DefaultLoginPageGeneratingFilter(AbstractAuthenticationProcessingFilter filter) {
      if (filter instanceof UsernamePasswordAuthenticationFilter) {
         init((UsernamePasswordAuthenticationFilter) filter, null);
      }
      else {
         init(null, filter);
      }
   }

   public DefaultLoginPageGeneratingFilter(UsernamePasswordAuthenticationFilter authFilter,
         AbstractAuthenticationProcessingFilter openIDFilter) {
      init(authFilter, openIDFilter);
   }

   private void init(UsernamePasswordAuthenticationFilter authFilter,
         AbstractAuthenticationProcessingFilter openIDFilter) {
      this.loginPageUrl = DEFAULT_LOGIN_PAGE_URL;
      this.logoutSuccessUrl = DEFAULT_LOGIN_PAGE_URL + "?logout";
      this.failureUrl = DEFAULT_LOGIN_PAGE_URL + "?" + ERROR_PARAMETER_NAME;
      if (authFilter != null) {
         initAuthFilter(authFilter);
      }
      if (openIDFilter != null) {
         initOpenIdFilter(openIDFilter);
      }
   }

   private void initAuthFilter(UsernamePasswordAuthenticationFilter authFilter) {
      this.formLoginEnabled = true;
      this.usernameParameter = authFilter.getUsernameParameter();
      this.passwordParameter = authFilter.getPasswordParameter();
      if (authFilter.getRememberMeServices() instanceof AbstractRememberMeServices) {
         this.rememberMeParameter = ((AbstractRememberMeServices) authFilter.getRememberMeServices()).getParameter();
      }
   }

   private void initOpenIdFilter(AbstractAuthenticationProcessingFilter openIDFilter) {
      this.openIdEnabled = true;
      this.openIDusernameParameter = "openid_identifier";
      if (openIDFilter.getRememberMeServices() instanceof AbstractRememberMeServices) {
         this.openIDrememberMeParameter = ((AbstractRememberMeServices) openIDFilter.getRememberMeServices())
               .getParameter();
      }
   }

   /**
    * Sets a Function used to resolve a Map of the hidden inputs where the key is the
    * name of the input and the value is the value of the input. Typically this is used
    * to resolve the CSRF token.
    * @param resolveHiddenInputs the function to resolve the inputs
    */
   public void setResolveHiddenInputs(Function<HttpServletRequest, Map<String, String>> resolveHiddenInputs) {
      Assert.notNull(resolveHiddenInputs, "resolveHiddenInputs cannot be null");
      this.resolveHiddenInputs = resolveHiddenInputs;
   }

   public boolean isEnabled() {
      return this.formLoginEnabled || this.openIdEnabled || this.oauth2LoginEnabled || this.saml2LoginEnabled;
   }

  // getter setter

   @Override
   public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain)
         throws IOException, ServletException {
      doFilter((HttpServletRequest) request, (HttpServletResponse) response, chain);
   }

   private void doFilter(HttpServletRequest request, HttpServletResponse response, FilterChain chain)
         throws IOException, ServletException {
      boolean loginError = isErrorPage(request);
      boolean logoutSuccess = isLogoutSuccess(request);
      if (isLoginUrlRequest(request) || loginError || logoutSuccess) {
         String loginPageHtml = generateLoginPageHtml(request, loginError, logoutSuccess);
         response.setContentType("text/html;charset=UTF-8");
         response.setContentLength(loginPageHtml.getBytes(StandardCharsets.UTF_8).length);
         response.getWriter().write(loginPageHtml);
         return;
      }
      chain.doFilter(request, response);
   }

   private String generateLoginPageHtml(HttpServletRequest request, boolean loginError, boolean logoutSuccess) {
      String errorMsg = "Invalid credentials";
      if (loginError) {
         HttpSession session = request.getSession(false);
         if (session != null) {
            AuthenticationException ex = (AuthenticationException) session
                  .getAttribute(WebAttributes.AUTHENTICATION_EXCEPTION);
            errorMsg = (ex != null) ? ex.getMessage() : "Invalid credentials";
         }
      }
      String contextPath = request.getContextPath();
      StringBuilder sb = new StringBuilder();
      sb.append("<!DOCTYPE html>\n");
      sb.append("<html lang=\"en\">\n");
      sb.append("  <head>\n");
      sb.append("    <meta charset=\"utf-8\">\n");
      sb.append("    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\">\n");
      sb.append("    <meta name=\"description\" content=\"\">\n");
      sb.append("    <meta name=\"author\" content=\"\">\n");
      sb.append("    <title>Please sign in</title>\n");
      sb.append("    <link href=\"https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css\" "
            + "rel=\"stylesheet\" integrity=\"sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M\" crossorigin=\"anonymous\">\n");
      sb.append("    <link href=\"https://getbootstrap.com/docs/4.0/examples/signin/signin.css\" "
            + "rel=\"stylesheet\" crossorigin=\"anonymous\"/>\n");
      sb.append("  </head>\n");
      sb.append("  <body>\n");
      sb.append("     <div class=\"container\">\n");
      if (this.formLoginEnabled) {
         sb.append("      <form class=\"form-signin\" method=\"post\" action=\"" + contextPath
               + this.authenticationUrl + "\">\n");
         sb.append("        <h2 class=\"form-signin-heading\">Please sign in</h2>\n");
         sb.append(createError(loginError, errorMsg) + createLogoutSuccess(logoutSuccess) + "        <p>\n");
         sb.append("          <label for=\"username\" class=\"sr-only\">Username</label>\n");
         sb.append("          <input type=\"text\" id=\"username\" name=\"" + this.usernameParameter
               + "\" class=\"form-control\" placeholder=\"Username\" required autofocus>\n");
         sb.append("        </p>\n");
         sb.append("        <p>\n");
         sb.append("          <label for=\"password\" class=\"sr-only\">Password</label>\n");
         sb.append("          <input type=\"password\" id=\"password\" name=\"" + this.passwordParameter
               + "\" class=\"form-control\" placeholder=\"Password\" required>\n");
         sb.append("        </p>\n");
         sb.append(createRememberMe(this.rememberMeParameter) + renderHiddenInputs(request));
         sb.append("        <button class=\"btn btn-lg btn-primary btn-block\" type=\"submit\">Sign in</button>\n");
         sb.append("      </form>\n");
      }
      if (this.openIdEnabled) {
         sb.append("      <form name=\"oidf\" class=\"form-signin\" method=\"post\" action=\"" + contextPath
               + this.openIDauthenticationUrl + "\">\n");
         sb.append("        <h2 class=\"form-signin-heading\">Login with OpenID Identity</h2>\n");
         sb.append(createError(loginError, errorMsg) + createLogoutSuccess(logoutSuccess) + "        <p>\n");
         sb.append("          <label for=\"username\" class=\"sr-only\">Identity</label>\n");
         sb.append("          <input type=\"text\" id=\"username\" name=\"" + this.openIDusernameParameter
               + "\" class=\"form-control\" placeholder=\"Username\" required autofocus>\n");
         sb.append("        </p>\n");
         sb.append(createRememberMe(this.openIDrememberMeParameter) + renderHiddenInputs(request));
         sb.append("        <button class=\"btn btn-lg btn-primary btn-block\" type=\"submit\">Sign in</button>\n");
         sb.append("      </form>\n");
      }
      if (this.oauth2LoginEnabled) {
         sb.append("<h2 class=\"form-signin-heading\">Login with OAuth 2.0</h2>");
         sb.append(createError(loginError, errorMsg));
         sb.append(createLogoutSuccess(logoutSuccess));
         sb.append("<table class=\"table table-striped\">\n");
         for (Map.Entry<String, String> clientAuthenticationUrlToClientName : this.oauth2AuthenticationUrlToClientName
               .entrySet()) {
            sb.append(" <tr><td>");
            String url = clientAuthenticationUrlToClientName.getKey();
            sb.append("<a href=\"").append(contextPath).append(url).append("\">");
            String clientName = HtmlUtils.htmlEscape(clientAuthenticationUrlToClientName.getValue());
            sb.append(clientName);
            sb.append("</a>");
            sb.append("</td></tr>\n");
         }
         sb.append("</table>\n");
      }
      if (this.saml2LoginEnabled) {
         sb.append("<h2 class=\"form-signin-heading\">Login with SAML 2.0</h2>");
         sb.append(createError(loginError, errorMsg));
         sb.append(createLogoutSuccess(logoutSuccess));
         sb.append("<table class=\"table table-striped\">\n");
         for (Map.Entry<String, String> relyingPartyUrlToName : this.saml2AuthenticationUrlToProviderName
               .entrySet()) {
            sb.append(" <tr><td>");
            String url = relyingPartyUrlToName.getKey();
            sb.append("<a href=\"").append(contextPath).append(url).append("\">");
            String partyName = HtmlUtils.htmlEscape(relyingPartyUrlToName.getValue());
            sb.append(partyName);
            sb.append("</a>");
            sb.append("</td></tr>\n");
         }
         sb.append("</table>\n");
      }
      sb.append("</div>\n");
      sb.append("</body></html>");
      return sb.toString();
   }

   private String renderHiddenInputs(HttpServletRequest request) {
      StringBuilder sb = new StringBuilder();
      for (Map.Entry<String, String> input : this.resolveHiddenInputs.apply(request).entrySet()) {
         sb.append("<input name=\"");
         sb.append(input.getKey());
         sb.append("\" type=\"hidden\" value=\"");
         sb.append(input.getValue());
         sb.append("\" />\n");
      }
      return sb.toString();
   }

   private String createRememberMe(String paramName) {
      if (paramName == null) {
         return "";
      }
      return "<p><input type='checkbox' name='" + paramName + "'/> Remember me on this computer.</p>\n";
   }

   private boolean isLogoutSuccess(HttpServletRequest request) {
      return this.logoutSuccessUrl != null && matches(request, this.logoutSuccessUrl);
   }

   private boolean isLoginUrlRequest(HttpServletRequest request) {
      return matches(request, this.loginPageUrl);
   }

   private boolean isErrorPage(HttpServletRequest request) {
      return matches(request, this.failureUrl);
   }

   private static String createError(boolean isError, String message) {
      if (!isError) {
         return "";
      }
      return "<div class=\"alert alert-danger\" role=\"alert\">" + HtmlUtils.htmlEscape(message) + "</div>";
   }

   private static String createLogoutSuccess(boolean isLogoutSuccess) {
      if (!isLogoutSuccess) {
         return "";
      }
      return "<div class=\"alert alert-success\" role=\"alert\">You have been signed out</div>";
   }

   private boolean matches(HttpServletRequest request, String url) {
      if (!"GET".equals(request.getMethod()) || url == null) {
         return false;
      }
      String uri = request.getRequestURI();
      int pathParamIndex = uri.indexOf(';');
      if (pathParamIndex > 0) {
         // strip everything after the first semi-colon
         uri = uri.substring(0, pathParamIndex);
      }
      if (request.getQueryString() != null) {
         uri += "?" + request.getQueryString();
      }
      if ("".equals(request.getContextPath())) {
         return uri.equals(url);
      }
      return uri.equals(request.getContextPath() + url);
   }

}
```

（1）在doFilter方法中，首先判断出当前请求是否为登录出错请求、注销成功请求或者登录请求。如果是这三种请求中的任意一个，就会在DefaultLoginPageGeneratingFilter过滤器中生成登录页面并返回，否则请求继续往下走，执行下一个过滤器（这就是一开始的/hello请求为什么没有被DefaultLoginPageGeneratingFilter拦截下来的原因）。
（2）如果当前请求是登录出错请求、注销成功请求或者登录请求中的任意一个，就会调用generateLoginPageHtml方法去生成登录页面。在该方法中，如果有异常信息就把异常信息取出来一同返回给前端，然后根据不同的登录场景，生成不同的登录页面。生成过程其实就是字符串拼接，拼接出不同的登录表单（由于源码太长，上面没有贴出来具体的字符串拼接源码，读者可以自行查看DefaultLoginPageGeneratingFilter类的源码）。
（3）登录页面生成后，接下来通过HttpServletResponse将登录页面写回到前端，然后调用return方法跳出过滤器链。

DefaultLogoutPageGeneratingFilter部分核心源码如下：

```java
public class DefaultLogoutPageGeneratingFilter extends OncePerRequestFilter {

	private RequestMatcher matcher = new AntPathRequestMatcher("/logout", "GET");

	private Function<HttpServletRequest, Map<String, String>> resolveHiddenInputs = (request) -> Collections.emptyMap();

	@Override
	protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain)
			throws ServletException, IOException {
		if (this.matcher.matches(request)) {
			renderLogout(request, response);
		}
		else {
			if (logger.isTraceEnabled()) {
				logger.trace(LogMessage.format("Did not render default logout page since request did not match [%s]",
						this.matcher));
			}
			filterChain.doFilter(request, response);
		}
	}

	private void renderLogout(HttpServletRequest request, HttpServletResponse response) throws IOException {
		StringBuilder sb = new StringBuilder();
		sb.append("<!DOCTYPE html>\n");
		sb.append("<html lang=\"en\">\n");
		sb.append("  <head>\n");
		sb.append("    <meta charset=\"utf-8\">\n");
		sb.append("    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\">\n");
		sb.append("    <meta name=\"description\" content=\"\">\n");
		sb.append("    <meta name=\"author\" content=\"\">\n");
		sb.append("    <title>Confirm Log Out?</title>\n");
		sb.append("    <link href=\"https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css\" "
				+ "rel=\"stylesheet\" integrity=\"sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M\" "
				+ "crossorigin=\"anonymous\">\n");
		sb.append("    <link href=\"https://getbootstrap.com/docs/4.0/examples/signin/signin.css\" "
				+ "rel=\"stylesheet\" crossorigin=\"anonymous\"/>\n");
		sb.append("  </head>\n");
		sb.append("  <body>\n");
		sb.append("     <div class=\"container\">\n");
		sb.append("      <form class=\"form-signin\" method=\"post\" action=\"" + request.getContextPath()
				+ "/logout\">\n");
		sb.append("        <h2 class=\"form-signin-heading\">Are you sure you want to log out?</h2>\n");
		sb.append(renderHiddenInputs(request)
				+ "        <button class=\"btn btn-lg btn-primary btn-block\" type=\"submit\">Log Out</button>\n");
		sb.append("      </form>\n");
		sb.append("    </div>\n");
		sb.append("  </body>\n");
		sb.append("</html>");
		response.setContentType("text/html;charset=UTF-8");
		response.getWriter().write(sb.toString());
	}

}

```

​	从上述源码中可以看出，请求到来之后，会先判断是否是注销请求/logout，如果是/logout请求，则渲染一个注销请求的页面返回给客户端，渲染过程和前面登录页面的渲染过程类似，也是字符串拼接（这里省略了字符串拼接，读者可以参考DefaultLogoutPageGeneratingFilter的源码）；否则请求继续往下走，执行下一个过滤器。

### 登录表单配置

项目创建好之后，为了方便测试，需要在application.properties中添加如下配置，将登录用户名和密码固定下来：

```properties
spring.security.user.name=javaboy
spring.security.user.password=123
```

接下来，我们在resources/static目录下创建一个login.html页面，这个是我们自定义的登录页面：

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>登录</title>
    <link href="//maxcdn.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css" rel="stylesheet" id="bootstrap-css">
    <script src="//maxcdn.bootstrapcdn.com/bootstrap/4.1.1/js/bootstrap.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
</head>
<style>
    #login .container #login-row #login-column #login-box {
        border: 1px solid #9C9C9C;
        background-color: #EAEAEA;
    }
</style>
<body>
<div id="login">
    <div class="container">
        <div id="login-row" class="row justify-content-center align-items-center">
            <div id="login-column" class="col-md-6">
                <div id="login-box" class="col-md-12">
                    <form id="login-form" class="form" action="/doLogin" method="post">
                        <h3 class="text-center text-info">登录</h3>
                        <div class="form-group">
                            <label for="username" class="text-info">用户名:</label><br>
                            <input type="text" name="uname" id="username" class="form-control">
                        </div>
                        <div class="form-group">
                            <label for="password" class="text-info">密码:</label><br>
                            <input type="text" name="passwd" id="password" class="form-control">
                        </div>
                        <div class="form-group">
                            <input type="submit" name="submit" class="btn btn-info btn-md" value="登录">
                        </div>
                    </form>
                </div>
            </div>
        </div>
    </div>
</div>
</body>
```

（1）form的action，这里给出的是/doLogin，表示表单要提交到/doLogin接口上。
（2）用户名输入框的name属性值为uname，当然这个值是可以自定义的，这里采用了uname。
（3）密码输入框的name属性值为passwd，passwd也是可以自定义的。

login.html定义好之后，接下来定义两个测试接口，作为受保护的资源。当用户登录成功后，就可以访问到受保护的资源。接口定义如下：

```java
@RestController
    public class LoginController {
       @RequestMapping("/index")
       public String index() {
           return "login success";
       }
       @RequestMapping("/hello")
       public String hello() {
           return "hello spring security";
       }
    }
```

配置类

```java
 @Configuration
    public class SecurityConfig extends WebSecurityConfigurerAdapter {
       @Override
       protected void configure(HttpSecurity http) throws Exception {
           http.authorizeRequests()
                   .anyRequest().authenticated()
                   .and()
                   .formLogin()
                   .loginPage("/login.html")
                   .loginProcessingUrl("/doLogin")
                   .defaultSuccessUrl("/index")
                   .failureUrl("/login.html")
                   .usernameParameter("uname")
                   .passwordParameter("passwd")
                   .permitAll()
                   .and()
                   .csrf().disable();
       }
    }
```

在Spring Security中，如果我们需要自定义配置，基本上都是继承自WebSecurityConfigurer Adapter来实现的，当然WebSecurityConfigurerAdapter本身的配置还是比较复杂，同时也是比较丰富的，这里先不做过多的展开，仅就结合上面的代码来解释，在下一个小节中我们将会对这里的配置再做更加详细的介绍。

（1）首先configure方法中是一个链式配置，当然也可以不用链式配置，每一个属性配置完毕后再从http.重新开始写起。
（2）authorizeRequests()方法表示开启权限配置（该方法的含义其实比较复杂，我们在13.4.4小节还会再次介绍该方法），.anyRequest().authenticated()表示所有的请求都要认证之后才能访问。
（3）有的读者会对and()方法表示疑惑，and()方法会返回HttpSecurityBuilder对象的一个子类（实际上就是HttpSecurity），所以and()方法相当于又回到HttpSecurity实例，重新开启新一轮的配置。如果觉得and()方法很难理解，也可以不用and()方法，在.anyRequest().authenticated()配置完成后直接用分号（;）结束，然后通过http.formLogin()继续配置表单登录。
（4）formLogin()表示开启表单登录配置，loginPage用来配置登录页面地址；loginProcessingUrl用来配置登录接口地址；defaultSuccessUrl表示登录成功后的跳转地址；failureUrl表示登录失败后的跳转地址；usernameParameter表示登录用户名的参数名称；passwordParameter表示登录密码的参数名称；permitAll表示跟登录相关的页面和接口不做拦截，直接通过。需要注意的是，loginProcessingUrl、usernameParameter、passwordParameter需要和login.html中登录表单的配置一致。
（5）最后的csrf().disable()表示禁用CSRF防御功能，Spring Security自带了CSRF防御机制，但是我们这里为了测试方便，先将CSRF防御机制关闭，本书第9章将会详细介绍CSRF攻击与防御问题。

​	配置完成后，启动Spring Boot项目，浏览器地址栏中输入http://localhost:8080/index，会自动跳转到http://localhost:8080/login.html页面，如图2-5所示。输入用户名和密码进行登录（用户名为javaboy，密码为123），登录成功之后，就可以访问到index页面了。

#### 配置细节

​	defaultSuccessUrl表示用户登录成功后的跳转地址，用failureUrl表示用户登录失败后的跳转地址。关于登录成功和登录失败，除了这两个方法可以配置之外，还有另外两个方法也可以配置。

##### 登录成功

​	successForwardUrl也可以实现登录成功后的跳转，代码如下：

```java
@Configuration
    public class SecurityConfig extends WebSecurityConfigurerAdapter {
       @Override
       protected void configure(HttpSecurity http) throws Exception {
           http.authorizeRequests()
                   .anyRequest().authenticated()
                   .and()
                   .formLogin().
					loginPage("/login.html")
                   .loginProcessingUrl("/doLogin")
                   .successForwardUrl("/index")
                   .failureUrl("/login.html")
                   .usernameParameter("uname")
                   .passwordParameter("passwd")
                   .permitAll()
                   .and()
                   .csrf().disable();
       }
    }
```

（1）defaultSuccessUrl表示当用户登录成功之后，会自动重定向到登录之前的地址上，如果用户本身就是直接访问的登录页面，则登录成功后就会重定向到defaultSuccessUrl指定的页面中。例如，用户在未认证的情况下，访问了/hello页面，此时会自动重定向到登录页面，当用户登录成功后，就会自动重定向到/hello页面；而用户如果一开始就访问登录页面，则登录成功后就会自动重定向到defaultSuccessUrl所指定的页面中。

（2）successForwardUrl则不会考虑用户之前的访问地址，只要用户登录成功，就会通过服务器端跳转到successForwardUrl所指定的页面。

（3）defaultSuccessUrl有一个重载方法，如果重载方法的第二个参数传入true，则defaultSuccessUrl的效果与successForwardUrl类似，即不考虑用户之前的访问地址，只要登录成功，就重定向到defaultSuccessUrl所指定的页面。不同之处在于，defaultSuccessUrl是通过重定向实现的跳转（客户端跳转），而successForwardUrl则是通过服务器端跳转实现的。
	无论是defaultSuccessUrl还是successForwardUrl，最终所配置的都是AuthenticationSuccess Handler接口的实例。
	Spring Security中专门提供了AuthenticationSuccessHandler接口用来处理登录成功事项：

```java
public interface AuthenticationSuccessHandler {
       default void onAuthenticationSuccess(HttpServletRequest request, HttpServletResponse response, FilterChain chain, Authentication authentication) throws IOException, ServletException{
           onAuthenticationSuccess(request, response, authentication);
           chain.doFilter(request, response);
       }
       void onAuthenticationSuccess(HttpServletRequest request,
                                          HttpServletResponse response, Authentication authentication) throws IOException, ServletException;
    }

```

![](https://pic.imgdb.cn/item/60b1c48308f74bc159cc189e.jpg)

​	当通过defaultSuccessUrl来设置登录成功后重定向的地址时，实际上对应的实现类就是SavedRequestAwareAuthenticationSuccessHandler，由于该类的源码比较长，这里列出来一部分核心代码：

```java
public class SavedRequestAwareAuthenticationSuccessHandler extends SimpleUrlAuthenticationSuccessHandler {

	protected final Log logger = LogFactory.getLog(this.getClass());

	private RequestCache requestCache = new HttpSessionRequestCache();

	@Override
	public void onAuthenticationSuccess(HttpServletRequest request, HttpServletResponse response,
			Authentication authentication) throws ServletException, IOException {
		SavedRequest savedRequest = this.requestCache.getRequest(request, response);
		if (savedRequest == null) {
			super.onAuthenticationSuccess(request, response, authentication);
			return;
		}
		String targetUrlParameter = getTargetUrlParameter();
		if (isAlwaysUseDefaultTargetUrl()
				|| (targetUrlParameter != null && StringUtils.hasText(request.getParameter(targetUrlParameter)))) {
			this.requestCache.removeRequest(request, response);
			super.onAuthenticationSuccess(request, response, authentication);
			return;
		}
		clearAuthenticationAttributes(request);
		// Use the DefaultSavedRequest URL
		String targetUrl = savedRequest.getRedirectUrl();
		getRedirectStrategy().sendRedirect(request, response, targetUrl);
	}

	public void setRequestCache(RequestCache requestCache) {
		this.requestCache = requestCache;
	}

}

```

（1）首先从requestCache中获取缓存下来的请求，如果没有获取到缓存请求，就说明用户在访问登录页面之前并没有访问其他页面，此时直接调用父类的onAuthenticationSuccess方法来处理，最终会重定向到defaultSuccessUrl指定的地址。
（2）接下来会获取一个targetUrlParameter，这个是用户显式指定的、希望登录成功后重定向的地址，例如用户发送的登录请求是http://localhost:8080/doLogin?target=/hello，这就表示当用户登录成功之后，希望自动重定向到/hello这个接口。getTargetUrlParameter就是要获取重定向地址参数的key，也就是上面的target，拿到target之后，就可以获取到重定向地址了。
（3）如果targetUrlParameter存在，或者用户设置了alwaysUseDefaultTargetUrl为true，这个时候缓存下来的请求就没有意义了。此时会直接调用父类的onAuthenticationSuccess方法完成重定向。targetUrlParameter存在，则直接重定向到targetUrlParameter指定的地址；alwaysUseDefaultTargetUrl为true，则直接重定向到defaultSuccessUrl指定的地址；如果targetUrlParameter存在并且alwaysUseDefaultTargetUrl为true，则重定向到defaultSuccessUrl指定的地址。
（4）如果前面的条件都不满足，那么最终会从缓存请求savedRequest中获取重定向地址，然后进行重定向操作。



这就是SavedRequestAwareAuthenticationSuccessHandler的实现逻辑，开发者也可以配置自己的SavedRequestAwareAuthenticationSuccessHandler，代码如下：

```java
@Configuration
    public class SecurityConfig extends WebSecurityConfigurerAdapter {
       @Override
       protected void configure(HttpSecurity http) throws Exception {
           http.authorizeRequests()
                   .anyRequest().authenticated()
                   .and()
                   .formLogin()
                   .loginPage("/login.html")
                   .loginProcessingUrl("/doLogin")
                   .successHandler(successHandler())
                   .failureUrl("/login.html")
                   .usernameParameter("uname")
                   .passwordParameter("passwd")
                   .permitAll()
                   .and()
                   .csrf().disable();
       }
SavedRequestAwareAuthenticationSuccessHandler successHandler() {
           SavedRequestAwareAuthenticationSuccessHandler handler =
                           new SavedRequestAwareAuthenticationSuccessHandler();
           handler.setDefaultTargetUrl("/index");
           handler.setTargetUrlParameter("target");
           return handler;
       }
    }
```

​	注意在配置时指定了targetUrlParameter为target，这样用户就可以在登录请求中，通过target来指定跳转地址了，然后我们修改一下前面login.html中的form表单：

```html
 <form id="login-form" class="form"
                                 action="/doLogin?target=/hello" method="post">
       <h3 class="text-center text-info">登录</h3>
       <div class="form-group">
           <label for="username" class="text-info">用户名:</label><br 
           <input type="text" name="uname" id="username" class="form-control">
       </div>
       <div class="form-group">
           <label for="password" class="text-info">密码:</label><br>
           <input type="text" name="passwd" id="password" class="form-control">
       </div>
<div class="form-group">
           <input type="submit" name="submit" class="btn btn-info btn-md"
                value="登录">
       </div>
    </form>
```

​	在form表单中，action修改为/doLogin?target=/hello，这样当用户登录成功之后，就始终跳转到/hello接口了。
​	当我们通过successForwardUrl来设置登录成功后重定向的地址时，实际上对应的实现类就是ForwardAuthenticationSuccessHandler，ForwardAuthenticationSuccessHandler的源码特别简单，就是一个服务端转发，代码如下：

```java
public class ForwardAuthenticationSuccessHandler implements AuthenticationSuccessHandler {

	private final String forwardUrl;

	/**
	 * @param forwardUrl
	 */
	public ForwardAuthenticationSuccessHandler(String forwardUrl) {
		Assert.isTrue(UrlUtils.isValidRedirectUrl(forwardUrl), () -> "'" + forwardUrl + "' is not a valid forward URL");
		this.forwardUrl = forwardUrl;
	}

	@Override
	public void onAuthenticationSuccess(HttpServletRequest request, HttpServletResponse response,
			Authentication authentication) throws IOException, ServletException {
		request.getRequestDispatcher(this.forwardUrl).forward(request, response);
	}

}
```

​	AuthenticationSuccessHandler默认的三个实现类，无论是哪一个，都是用来处理页面跳转的。有时候页面跳转并不能满足我们的需求，特别是现在流行的前后端分离开发中，用户登录成功后，就不再需要页面跳转了，只需要给前端返回一个JSON数据即可，告诉前端登录成功还是登录失败，前端收到消息之后自行处理。像这样的需求，我们可以通过自定义AuthenticationSuccessHandler的实现类来完成：

```java
public class MyAuthenticationSuccessHandler implements AuthenticationSuccessHandler{
    @Override
    public void onAuthenticationSuccess(HttpServletRequest request, HttpServletResponse response, Authentication authentication) throws IOException, ServletException {
        response.setContentType("application/json;charset=utf-8");
        Map<String, Object> resp = new HashMap<>();
        resp.put("status", 200);
        resp.put("msg", "登录成功!");
        ObjectMapper om = new ObjectMapper();
        String s = om.writeValueAsString(resp);
        response.getWriter().write(s);
    }
}
```

​	最后，在SecurityConfig中配置自定义的MyAuthenticationSuccessHandler，代码如下：

```java
@Configuration
    public class SecurityConfig extends WebSecurityConfigurerAdapter {
       @Override
       protected void configure(HttpSecurity http) throws Exception {
           http.authorizeRequests()
                   .anyRequest().authenticated()
                   .and()
                   .formLogin()
                   .loginPage("/login.html")
                   .loginProcessingUrl("/doLogin")
                   .successHandler(new MyAuthenticationSuccessHandler())
					.failureUrl("/login.html")
                   .usernameParameter("uname")
                   .passwordParameter("passwd")
                   .permitAll()
                   .and()
                   .csrf().disable();
       }
    }

```

##### 登录失败

​	引入依赖

```xml
  <dependency>
       <groupId>org.springframework.boot</groupId>
       <artifactId>spring-boot-starter-thymeleaf</artifactId>
    </dependency>
```

然后在resources/templates目录下新建mylogin.html，代码如下：

```html
<!DOCTYPE html>
<html lang="en" xmlns:th="http://www.thymeleaf.org">
<head>
    <meta charset="UTF-8">
    <title>登录</title>
    <link href="//maxcdn.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css" rel="stylesheet" id="bootstrap-css">
    <script src="//maxcdn.bootstrapcdn.com/bootstrap/4.1.1/js/bootstrap.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
</head>
<style>
    #login .container #login-row #login-column #login-box {
        border: 1px solid #9C9C9C;
        background-color: #EAEAEA;
    }
</style>
<body>
<div id="login">
    <div class="container">
        <div id="login-row" class="row justify-content-center align-items-center">
            <div id="login-column" class="col-md-6">
                <div id="login-box" class="col-md-12">
                    <form id="login-form" class="form" action="/doLogin" method="post">
                        <h3 class="text-center text-info">登录</h3>
                        <div th:text="${SPRING_SECURITY_LAST_EXCEPTION}"></div>
                        <div class="form-group">
                            <label for="username" class="text-info">用户名:</label><br>
                            <input type="text" name="uname" id="username" class="form-control">
                        </div>
                        <div class="form-group">
                            <label for="password" class="text-info">密码:</label><br>
                            <input type="text" name="passwd" id="password" class="form-control">
                        </div>
                        <div class="form-group">
                            <input type="submit" name="submit" class="btn btn-info btn-md" value="登录">
                        </div>
                    </form>
                </div>
            </div>
        </div>
    </div>
</div>
</body>
```

​	mylogin.html和前面的login.html基本类似，前面的login.html是静态页面，这里的mylogin.html是thymeleaf模板页面，mylogin.html页面在form中多了一个div，用来展示登录失败时候的异常信息，登录失败的异常信息会放在request中返回到前端，开发者可以将其直接提取出来展示。
既然mylogin.html是动态页面，就不能像静态页面那样直接访问了，需要我们给mylogin.html页面提供一个访问控制器：

```java
@Controller
    public class MyLoginController {
       @RequestMapping("/mylogin.html")
       public String mylogin() {
           return "mylogin";
       }
    }
```

​	最后再在SecurityConfig中配置登录页面，代码如下：

```java
@Configuration
    public class SecurityConfig extends WebSecurityConfigurerAdapter {
       @Override
       protected void configure(HttpSecurity http) throws Exception {
			http.authorizeRequests()
                  .anyRequest().authenticated()
                  .and()
                  .formLogin()
                  .loginPage("/mylogin.html")
                  .loginProcessingUrl("/doLogin")
                  .defaultSuccessUrl("/index.html")
                  .failureUrl("/mylogin.html")
                  .usernameParameter("uname")
                  .passwordParameter("passwd")
                  .permitAll()
                  .and()
                  .csrf().disable();
       }
    }
```

​	failureUrl表示登录失败后重定向到mylogin.html页面。重定向是一种客户端跳转，重定向不方便携带请求失败的异常信息（只能放在URL中）。
如果希望能够在前端展示请求失败的异常信息，可以使用下面这种方式：

```java
  @Configuration
    public class SecurityConfig extends WebSecurityConfigurerAdapter {
 @Override
       protected void configure(HttpSecurity http) throws Exception {
           http.authorizeRequests()
                   .anyRequest().authenticated()
                   .and()
                   .formLogin()
                   .loginPage("/mylogin.html")
                   .loginProcessingUrl("/doLogin")
                   .defaultSuccessUrl("/index.html")
                   .failureForwardUrl("/mylogin.html")
                   .usernameParameter("uname")
                   .passwordParameter("passwd")
                   .permitAll()
                   .and()
                   .csrf().disable();
       }
    }
```

failureForwardUrl方法从名字上就可以看出，这种跳转是一种服务器端跳转，服务器端跳转的好处是可以携带登录异常信息。如果登录失败，自动跳转回登录页面后，就可以将错误信息展示出来，如图2-8所示。

![](https://pic.imgdb.cn/item/60b1e78408f74bc159afcf20.jpg)

​	无论是failureUrl还是failureForwardUrl，最终所配置的都是AuthenticationFailureHandler接口的实现。Spring Security中提供了AuthenticationFailureHandler接口，用来规范登录失败的实现：

```java
 public interface AuthenticationFailureHandler {
       void onAuthenticationFailure(HttpServletRequest request,
               HttpServletResponse response, AuthenticationException exception)
               throws IOException, ServletException;
    }
```

![](https://pic.imgdb.cn/item/60b1e7cf08f74bc159b428cb.jpg)

（1）SimpleUrlAuthenticationFailureHandler默认的处理逻辑就是通过重定向跳转到登录页面，当然也可以通过配置forwardToDestination属性将重定向改为服务器端跳转，failureUrl方法的底层实现逻辑就是SimpleUrlAuthenticationFailureHandler。
（2）ExceptionMappingAuthenticationFailureHandler可以实现根据不同的异常类型，映射到不同的路径。
（3）ForwardAuthenticationFailureHandler表示通过服务器端跳转来重新回到登录页面，failureForwardUrl方法的底层实现逻辑就是ForwardAuthenticationFailureHandler。
（4）AuthenticationEntryPointFailureHandler是Spring Security 5.2新引进的处理类，可以通过AuthenticationEntryPoint来处理登录异常。
（5）DelegatingAuthenticationFailureHandler可以实现为不同的异常类型配置不同的登录失败处理回调。

可以自定义SimpleUrlAuthenticationFailureHandler配置，并将forwardToDestination属性设置为true，代码如下：

```java
 @Configuration
    public class SecurityConfig extends WebSecurityConfigurerAdapter {
       @Override
       protected void configure(HttpSecurity http) throws Exception {
           http.authorizeRequests()
                   .anyRequest().authenticated()
                   .and()
                   .formLogin()
                   .loginPage("/mylogin.html")
                   .loginProcessingUrl("/doLogin")
                   .defaultSuccessUrl("/index.html")
                   .failureHandler(failureHandler())
                   .usernameParameter("uname")
                   .passwordParameter("passwd")
                   .permitAll()
                   .and()
                   .csrf().disable();
       }
       SimpleUrlAuthenticationFailureHandler failureHandler() {
           SimpleUrlAuthenticationFailureHandler handler =
	new SimpleUrlAuthenticationFailureHandler("/mylogin.html");
           handler.setUseForward(true);
           return handler;
       }
    }

```

SimpleUrlAuthenticationFailureHandler的源码也很简单，我们一起来看一下实现逻辑（源码比较长，这里列出来核心部分）：

```java
public class SimpleUrlAuthenticationFailureHandler implements AuthenticationFailureHandler {

	protected final Log logger = LogFactory.getLog(getClass());

	private String defaultFailureUrl;

	private boolean forwardToDestination = false;

	private boolean allowSessionCreation = true;

	private RedirectStrategy redirectStrategy = new DefaultRedirectStrategy();

	public SimpleUrlAuthenticationFailureHandler() {
	}

	public SimpleUrlAuthenticationFailureHandler(String defaultFailureUrl) {
		setDefaultFailureUrl(defaultFailureUrl);
	}

	/**
	 * Performs the redirect or forward to the {@code defaultFailureUrl} if set, otherwise
	 * returns a 401 error code.
	 * <p>
	 * If redirecting or forwarding, {@code saveException} will be called to cache the
	 * exception for use in the target view.
	 */
	@Override
	public void onAuthenticationFailure(HttpServletRequest request, HttpServletResponse response,
			AuthenticationException exception) throws IOException, ServletException {
		if (this.defaultFailureUrl == null) {
			if (this.logger.isTraceEnabled()) {
				this.logger.trace("Sending 401 Unauthorized error since no failure URL is set");
			}
			else {
				this.logger.debug("Sending 401 Unauthorized error");
			}
			response.sendError(HttpStatus.UNAUTHORIZED.value(), HttpStatus.UNAUTHORIZED.getReasonPhrase());
			return;
		}
		saveException(request, exception);
		if (this.forwardToDestination) {
			this.logger.debug("Forwarding to " + this.defaultFailureUrl);
			request.getRequestDispatcher(this.defaultFailureUrl).forward(request, response);
		}
		else {
			this.redirectStrategy.sendRedirect(request, response, this.defaultFailureUrl);
		}
	}

	/**
	 * Caches the {@code AuthenticationException} for use in view rendering.
	 * <p>
	 * If {@code forwardToDestination} is set to true, request scope will be used,
	 * otherwise it will attempt to store the exception in the session. If there is no
	 * session and {@code allowSessionCreation} is {@code true} a session will be created.
	 * Otherwise the exception will not be stored.
	 */
	protected final void saveException(HttpServletRequest request, AuthenticationException exception) {
		if (this.forwardToDestination) {
			request.setAttribute(WebAttributes.AUTHENTICATION_EXCEPTION, exception);
			return;
		}
		HttpSession session = request.getSession(false);
		if (session != null || this.allowSessionCreation) {
			request.getSession().setAttribute(WebAttributes.AUTHENTICATION_EXCEPTION, exception);
		}
	}
}
```

​	如果是前后端分离开发，登录失败时就不需要页面跳转了，只需要返回JSON字符串给前端即可，此时可以通过自定义AuthenticationFailureHandler的实现类来完成，代码如下：

```java
public class MyAuthenticationFailureHandler implements
        AuthenticationFailureHandler {
       @Override
       public void onAuthenticationFailure(HttpServletRequest request,
                                                  HttpServletResponse response,
                                                  AuthenticationException exception)
                                                  throws IOException, ServletException {
           response.setContentType("application/json;charset=utf-8");
           Map<String, Object> resp = new HashMap<>();
           resp.put("status", 500);
	resp.put("msg", "登录失败!" + exception.getMessage());
           ObjectMapper om = new ObjectMapper();
           String s = om.writeValueAsString(resp);
           response.getWriter().write(s);
       }
    }
```

然后在SecurityConfig中进行配置即可：

```java
@Configuration
    public class SecurityConfig extends WebSecurityConfigurerAdapter {
       @Override
       protected void configure(HttpSecurity http) throws Exception {
           http.authorizeRequests()
                   .anyRequest().authenticated()
                   .and()
                   .formLogin()
                   .loginPage("/mylogin.html")
                   .loginProcessingUrl("/doLogin")
                   .defaultSuccessUrl("/index.html")
                   .failureHandler(new MyAuthenticationFailureHandler())
                   .usernameParameter("uname")
                   .passwordParameter("passwd")
				   .permitAll()
                   .and()
                   .csrf().disable();
       }
    }
```

##### 注销登录

​	开发者也可以根据自己的需求对注销登录进行定制。

```java
 @Configuration
    public class SecurityConfig extends WebSecurityConfigurerAdapter {
       @Override
       protected void configure(HttpSecurity http) throws Exception {
           http.authorizeRequests()
				.anyRequest().authenticated()
                   .and()
                   .formLogin()
                      //省略其他配置
                   .and()
                   .logout()
                   .logoutUrl("/logout")
                   .invalidateHttpSession(true)
                   .clearAuthentication(true)
                   .logoutSuccessUrl("/mylogin.html")
                   .and()
                   .csrf().disable();
       }
    }
```

（1）通过.logout()方法开启注销登录配置。
（2）logoutUrl指定了注销登录请求地址，默认是GET请求，路径为/logout。
（3）invalidateHttpSession表示是否使session失效，默认为true。
（4）clearAuthentication表示是否清除认证信息，默认为true。
（5）logoutSuccessUrl表示注销登录后的跳转地址。

​	配置完成后，再次启动项目，登录成功后，在浏览器中输入http://localhost:8080/logout就可以发起注销登录请求了。注销成功后，会自动跳转到mylogin.html页面。

​	开发者也可以配置多个注销登录的请求，同时还可以指定请求的方法：

![](https://pic.imgdb.cn/item/60b1fa4908f74bc159b19dcc.jpg)

​	如果项目是前后端分离的架构，注销成功后就不需要页面跳转了，只需将注销成功的信息返回给前端即可，此时我们可以自定义返回内容：

![](https://pic.imgdb.cn/item/60b1fa8408f74bc159b482d0.jpg)

如果开发者希望为不同的注销地址返回不同的结果，也是可以的，配置如下：

![](https://pic.imgdb.cn/item/60b1fab608f74bc159b6e20f.jpg)

### 登录用户数据获取

​	开发者若再想获取用户登录数据就会有两种不同的思路：

（1）从SecurityContextHolder中获取。
（2）从当前请求对象中获取。



​	开发者也可以使用一些非主流的方式获取登录成功后的用户信息，例如直接从HttpSession中获取用户登录数据。

（1）作为AuthenticationManager的输入参数，提供用户身份认证的凭证，当它作为一个输入参数时，它的isAuthenticated方法返回false，表示用户还未认证。
（2）代表已经经过身份认证的用户，此时的Authentication可以从SecurityContext中获取。

​	一个Authentication对象主要包含三个方面的信息：
（1）principal：定义认证的用户。如果用户使用用户名／密码的方式登录，principal通常就是一个UserDetails对象。
（2）credentials：登录凭证，一般就是指密码。当用户登录成功之后，登录凭证会被自动擦除，以防止泄漏。
（3）authorities：用户被授予的权限信息。
Java中本身提供了Principal接口用来描述认证主体，Principal可以代表一个公司、个人或者登录ID。Spring Security中定义了Authentication接口用来规范登录用户信息，Authentication继承自Principal：

```java
public interface Authentication extends Principal, Serializable {
       Collection<? extends GrantedAuthority> getAuthorities();
       Object getCredentials();
       Object getDetails();
       Object getPrincipal();
boolean isAuthenticated();
       void setAuthenticated(boolean isAuthenticated);
    }
```

不同的认证方式对应不同的Authentication实例

![](https://pic.imgdb.cn/item/60b201a608f74bc1590c1a13.jpg)



（1）AbstractAuthenticationToken：该类实现了Authentication和CredentialsContainer两个接口，在AbstractAuthenticationToken中对Authentication接口定义的各个数据获取方法进行了实现，CredentialsContainer则提供了登录凭证擦除方法。一般在登录成功后，为了防止用户信息泄漏，可以将登录凭证（例如密码）擦除。
（2）RememberMeAuthenticationToken：如果用户使用RememberMe的方式登录，登录信息将封装在RememberMeAuthenticationToken中。
（3）TestingAuthenticationToken：单元测试时封装的用户对象。
（4）AnonymousAuthenticationToken：匿名登录时封装的用户对象。
（5）RunAsUserToken：替换验证身份时封装的用户对象。
（6）UsernamePasswordAuthenticationToken：表单登录时封装的用户对象。
（7）JaasAuthenticationToken：JAAS认证时封装的用户对象。
（8）PreAuthenticatedAuthenticationToken：Pre-Authentication场景下封装的用户对象。

#### 从SecurityContextHolder中获取

```java
Authentication authentication = SecurityContextHolder.getContext().getAuthtication();
```

##### SecurityContextHolder

​	SecurityContextHolder中存储的是SecurityContext，SecurityContext中存储的则是Authentication。

![](https://pic.imgdb.cn/item/60b2500808f74bc159844e55.jpg)



​	首先在SecurityContextHolder中存放的是SecurityContext，SecurityContextHolder中定义了三种不同的数据存储策略，这实际上是一种典型的策略模式：

（1）MODE_THREADLOCAL：这种存放策略是将SecurityContext存放在ThreadLocal中，大家知道ThreadLocal的特点是在哪个线程中存储就要在哪个线程中读取，这其实非常适合Web应用，因为在默认情况下，一个请求无论经过多少Filter到达Servlet，都是由一个线程来处理的。这也是SecurityContextHolder的默认存储策略，这种存储策略意味着如果在具体的业务处理代码中，开启了子线程，在子线程中去获取登录用户数据，就会获取不到。
（2）MODE_INHERITABLETHREADLOCAL：这种存储模式适用于多线程环境，如果希望在子线程中也能够获取到登录用户数据，那么可以使用这种存储模式。
（3）MODE_GLOBAL：这种存储模式实际上是将数据保存在一个静态变量中，在Java Web开发中，这种模式很少使用到。

```java
 public interface SecurityContextHolderStrategy {
        void clearContext();
        SecurityContext getContext();
        void setContext(SecurityContext context);
        SecurityContext createEmptyContext();
    }
```

![](https://pic.imgdb.cn/item/60b2515608f74bc1599a7ac7.jpg)

 



```java
final class ThreadLocalSecurityContextHolderStrategy implements SecurityContextHolderStrategy {

	private static final ThreadLocal<SecurityContext> contextHolder = new ThreadLocal<>();

	@Override
	public void clearContext() {
		contextHolder.remove();
	}

	@Override
	public SecurityContext getContext() {
		SecurityContext ctx = contextHolder.get();
		if (ctx == null) {
			ctx = createEmptyContext();
			contextHolder.set(ctx);
		}
		return ctx;
	}

	@Override
	public void setContext(SecurityContext context) {
		Assert.notNull(context, "Only non-null SecurityContext instances are permitted");
		contextHolder.set(context);
	}

	@Override
	public SecurityContext createEmptyContext() {
		return new SecurityContextImpl();
	}

}
```

​	InheritableThreadLocalSecurityContextHolderStrategy和ThreadLocalSecurityContextHolder Strategy的实现策略基本一致，不同的是存储数据的载体变了，在InheritableThreadLocalSecurity ContextHolderStrategy中存储数据的载体变成了InheritableThreadLocal。InheritableThreadLocal继承自ThreadLocal，但是多了一个特性，就是在子线程创建的一瞬间，会自动将父线程中的数据复制到子线程中。该存储策略正是利用了这一特性，实现了在子线程中获取登录用户信息的功能。

```java
final class InheritableThreadLocalSecurityContextHolderStrategy implements SecurityContextHolderStrategy {

	private static final ThreadLocal<SecurityContext> contextHolder = new InheritableThreadLocal<>();

	@Override
	public void clearContext() {
		contextHolder.remove();
	}

	@Override
	public SecurityContext getContext() {
		SecurityContext ctx = contextHolder.get();
		if (ctx == null) {
			ctx = createEmptyContext();
			contextHolder.set(ctx);
		}
		return ctx;
	}

	@Override
	public void setContext(SecurityContext context) {
		Assert.notNull(context, "Only non-null SecurityContext instances are permitted");
		contextHolder.set(context);
	}

	@Override
	public SecurityContext createEmptyContext() {
		return new SecurityContextImpl();
	}

}
```

GlobalSecurityContextHolderStrategy的实现就更简单了，用一个静态变量来保存SecurityContext，所以它也可以在多线程环境下使用。但是一般在Web开发中，这种存储策略使用得较少。

```java
final class GlobalSecurityContextHolderStrategy implements SecurityContextHolderStrategy {

	private static SecurityContext contextHolder;

	@Override
	public void clearContext() {
		contextHolder = null;
	}

	@Override
	public SecurityContext getContext() {
		if (contextHolder == null) {
			contextHolder = new SecurityContextImpl();
		}
		return contextHolder;
	}

	@Override
	public void setContext(SecurityContext context) {
		Assert.notNull(context, "Only non-null SecurityContext instances are permitted");
		contextHolder = context;
	}

	@Override
	public SecurityContext createEmptyContext() {
		return new SecurityContextImpl();
	}

}
```



最后我们再来看一下SecurityContextHolder的源码：

```java
public class SecurityContextHolder {

	public static final String MODE_THREADLOCAL = "MODE_THREADLOCAL";

	public static final String MODE_INHERITABLETHREADLOCAL = "MODE_INHERITABLETHREADLOCAL";

	public static final String MODE_GLOBAL = "MODE_GLOBAL";

	public static final String SYSTEM_PROPERTY = "spring.security.strategy";

	private static String strategyName = System.getProperty(SYSTEM_PROPERTY);

	private static SecurityContextHolderStrategy strategy;

	private static int initializeCount = 0;

	static {
		initialize();
	}

	private static void initialize() {
		if (!StringUtils.hasText(strategyName)) {
			// Set default
			strategyName = MODE_THREADLOCAL;
		}
		if (strategyName.equals(MODE_THREADLOCAL)) {
			strategy = new ThreadLocalSecurityContextHolderStrategy();
		}
		else if (strategyName.equals(MODE_INHERITABLETHREADLOCAL)) {
			strategy = new InheritableThreadLocalSecurityContextHolderStrategy();
		}
		else if (strategyName.equals(MODE_GLOBAL)) {
			strategy = new GlobalSecurityContextHolderStrategy();
		}
		else {
			// Try to load a custom strategy
			try {
				Class<?> clazz = Class.forName(strategyName);
				Constructor<?> customStrategy = clazz.getConstructor();
				strategy = (SecurityContextHolderStrategy) customStrategy.newInstance();
			}
			catch (Exception ex) {
				ReflectionUtils.handleReflectionException(ex);
			}
		}
		initializeCount++;
	}

	/**
	 * Explicitly clears the context value from the current thread.
	 */
	public static void clearContext() {
		strategy.clearContext();
	}

	/**
	 * Obtain the current <code>SecurityContext</code>.
	 * @return the security context (never <code>null</code>)
	 */
	public static SecurityContext getContext() {
		return strategy.getContext();
	}

	/**
	 * Primarily for troubleshooting purposes, this method shows how many times the class
	 * has re-initialized its <code>SecurityContextHolderStrategy</code>.
	 * @return the count (should be one unless you've called
	 * {@link #setStrategyName(String)} to switch to an alternate strategy.
	 */
	public static int getInitializeCount() {
		return initializeCount;
	}

	/**
	 * Associates a new <code>SecurityContext</code> with the current thread of execution.
	 * @param context the new <code>SecurityContext</code> (may not be <code>null</code>)
	 */
	public static void setContext(SecurityContext context) {
		strategy.setContext(context);
	}

	/**
	 * Changes the preferred strategy. Do <em>NOT</em> call this method more than once for
	 * a given JVM, as it will re-initialize the strategy and adversely affect any
	 * existing threads using the old strategy.
	 * @param strategyName the fully qualified class name of the strategy that should be
	 * used.
	 */
	public static void setStrategyName(String strategyName) {
		SecurityContextHolder.strategyName = strategyName;
		initialize();
	}

	/**
	 * Allows retrieval of the context strategy. See SEC-1188.
	 * @return the configured strategy for storing the security context.
	 */
	public static SecurityContextHolderStrategy getContextHolderStrategy() {
		return strategy;
	}

	/**
	 * Delegates the creation of a new, empty context to the configured strategy.
	 */
	public static SecurityContext createEmptyContext() {
		return strategy.createEmptyContext();
	}

	@Override
	public String toString() {
		return "SecurityContextHolder[strategy='" + strategyName + "'; initializeCount=" + initializeCount + "]";
	}

}
```

默认情况下，如果开发者试图从子线程中获取当前登录用户数据，就会获取失败，代码如下：

![](https://pic.imgdb.cn/item/60b2540508f74bc159ca30e6.jpg)

​	子线程之所以获取不到登录用户信息，就是因为数据存储在ThreadLocal中，存储和读取不是同一个线程，所以获取不到。如果希望子线程中也能够获取到登录用户信息，可以将SecurityContextHolder中的存储策略改为MODE_INHERITABLETHREADLOCAL，这样就支持多线程环境下获取登录用户信息了。
​	默认的存储策略是通过System.getProperty加载的，因此我们可以通过配置系统变量来修改默认的存储策略。

```java
-Dspring.security.strategy=MODE_INHERITABLETHREADLOCAL
```

##### SecurityContextPersistenceFilter

​	默认情况下，在Spring Security过滤器链中，SecurityContextPersistenceFilter是第二道防线，位于WebAsyncManagerIntegrationFilter之后。从SecurityContextPersistenceFilter这个过滤器的名字上就可以推断出来，它的作用是为了存储SecurityContext而设计的。

（1）当一个请求到来时，从HttpSession中获取SecurityContext并存入SecurityContext Holder中，这样在同一个请求的后续处理过程中，开发者始终可以通过SecurityContextHolder获取到当前登录用户信息。
（2）当一个请求处理完毕时，从SecurityContextHolder中获取SecurityContext并存入HttpSession中（主要针对异步Servlet），方便下一个请求到来时，再从HttpSession中拿出来使用，同时擦除SecurityContextHolder中的登录用户信息。

> 在SecurityContextPersistenceFilter过滤器中，当一个请求处理完毕时，从SecurityContextHolder中获取SecurityContext存入HttpSession中，这一步的操作主要是针对异步Servlet。如果不是异步Servlet，在响应提交时，就会将SecurityContext保存到HttpSession中了，而不会等到在SecurityContextPersistenceFilter过滤器中再去存储。

​	将SecurityContext存入HttpSession，或者从HttpSession中加载数据并转为Security Context对象，这些事情都是由SecurityContextRepository接口的实现类完成的，因此这里我们就先从SecurityContextRepository接口开始看起。

```java
public interface SecurityContextRepository {

	SecurityContext loadContext(HttpRequestResponseHolder requestResponseHolder);

	void saveContext(SecurityContext context, HttpServletRequest request, HttpServletResponse response);

	boolean containsContext(HttpServletRequest request);
}

```

（1）loadContext：这个方法用来加载SecurityContext对象出来，对于没有登录的用户，这里会返回一个空的SecurityContext对象，注意空的SecurityContext对象是指SecurityContext中不存在Authentication对象，而不是该方法返回null。
（2）saveContext：该方法用来保存一个SecurityContext对象。
（3）containsContext：该方法可以判断SecurityContext对象是否存在。

![](https://pic.imgdb.cn/item/60b301e308f74bc159cbb90d.jpg)

​	在这三个实现类中，TestSecurityContextRepository为单元测试提供支持；NullSecurity ContextRepository实现类中，loadContext方法总是返回一个空的SecurityContext对象，saveContext方法未做任何实现，containsContext方法总是返回false，所以NullSecurityContextRepository实现类实际上未做SecurityContext的存储工作。
​	在Spring Security中默认使用的实现类是HttpSessionSecurityContextRepository，通过HttpSessionSecurityContextRepository实现了将SecurityContext存储到HttpSession以及从HttpSession中加载SecurityContext出来。

​	两个内部类:

**SaveToSessionResponseWrapper**

![](https://pic.imgdb.cn/item/60b305f308f74bc159f9a17b.jpg)

SaveToSessionResponseWrapper实际上就是我们所熟知的HttpServletResponse功能的扩展。这里有三个关键的实现类：
（1）HttpServletResponseWrapper：HttpServletResponseWrapper实现了HttpServletResponse接口，它是HttpServletResponse的装饰类，利用HttpServletResponseWrapper可以方便地操作参数和输出流等。
（2）OnCommittedResponseWrapper：OnCommittedResponseWrapper继承自HttpServlet ResponseWrapper，对其功能进行了增强，最重要的增强在于可以获取HttpServletResponse的提交行为。当HttpServletResponse的sendError、sendRedirect、flushBuffer、flush以及close等方法被调用时，onResponseCommitted方法会被触发，开发者可以在onResponseCommitted方法中做一些数据保存操作，例如保存SecurityContext。不过OnCommittedResponseWrapper中的onResponseCommitted方法只是一个抽象方法，并没有具体的实现，具体的实现则在它的实现类SaveContextOnUpdateOrErrorResponseWrapper中。
（3）SaveContextOnUpdateOrErrorResponseWrapper：该类继承自OnCommittedResponse Wrapper并对onResponseCommitted方法做了实现。在SaveContextOnUpdateOrErrorResponse Wrapper类中声明了一个contextSaved变量，表示SecuirtyContext是否已经存储成功。当HttpServletResponse提交时，会调用onResponseCommitted方法，在onResponseCommitted方法中调用saveContext方法，将SecurityContext保存到HttpSession中，同时将contextSaved变量标记为true。saveContext方法在这里也是一个抽象方法，具体的实现则在SaveToSession ResponseWrapper类中。

```java
final class SaveToSessionResponseWrapper extends SaveContextOnUpdateOrErrorResponseWrapper {

		private final Log logger = HttpSessionSecurityContextRepository.this.logger;

		private final HttpServletRequest request;

		private final boolean httpSessionExistedAtStartOfRequest;

		private final SecurityContext contextBeforeExecution;

		private final Authentication authBeforeExecution;

		private boolean isSaveContextInvoked;

		/**
		 * Takes the parameters required to call <code>saveContext()</code> successfully
		 * in addition to the request and the response object we are wrapping.
		 * @param request the request object (used to obtain the session, if one exists).
		 * @param httpSessionExistedAtStartOfRequest indicates whether there was a session
		 * in place before the filter chain executed. If this is true, and the session is
		 * found to be null, this indicates that it was invalidated during the request and
		 * a new session will now be created.
		 * @param context the context before the filter chain executed. The context will
		 * only be stored if it or its contents changed during the request.
		 */
		SaveToSessionResponseWrapper(HttpServletResponse response, HttpServletRequest request,
				boolean httpSessionExistedAtStartOfRequest, SecurityContext context) {
			super(response, HttpSessionSecurityContextRepository.this.disableUrlRewriting);
			this.request = request;
			this.httpSessionExistedAtStartOfRequest = httpSessionExistedAtStartOfRequest;
			this.contextBeforeExecution = context;
			this.authBeforeExecution = context.getAuthentication();
		}

		/**
		 * Stores the supplied security context in the session (if available) and if it
		 * has changed since it was set at the start of the request. If the
		 * AuthenticationTrustResolver identifies the current user as anonymous, then the
		 * context will not be stored.
		 * @param context the context object obtained from the SecurityContextHolder after
		 * the request has been processed by the filter chain.
		 * SecurityContextHolder.getContext() cannot be used to obtain the context as it
		 * has already been cleared by the time this method is called.
		 *
		 */
		@Override
		protected void saveContext(SecurityContext context) {
			final Authentication authentication = context.getAuthentication();
			HttpSession httpSession = this.request.getSession(false);
			String springSecurityContextKey = HttpSessionSecurityContextRepository.this.springSecurityContextKey;
			// See SEC-776
			if (authentication == null
					|| HttpSessionSecurityContextRepository.this.trustResolver.isAnonymous(authentication)) {
				if (httpSession != null && this.authBeforeExecution != null) {
					// SEC-1587 A non-anonymous context may still be in the session
					// SEC-1735 remove if the contextBeforeExecution was not anonymous
					httpSession.removeAttribute(springSecurityContextKey);
					this.isSaveContextInvoked = true;
				}
				if (this.logger.isDebugEnabled()) {
					if (authentication == null) {
						this.logger.debug("Did not store empty SecurityContext");
					}
					else {
						this.logger.debug("Did not store anonymous SecurityContext");
					}
				}
				return;
			}
			httpSession = (httpSession != null) ? httpSession : createNewSessionIfAllowed(context);
			// If HttpSession exists, store current SecurityContext but only if it has
			// actually changed in this thread (see SEC-37, SEC-1307, SEC-1528)
			if (httpSession != null) {
				// We may have a new session, so check also whether the context attribute
				// is set SEC-1561
				if (contextChanged(context) || httpSession.getAttribute(springSecurityContextKey) == null) {
					httpSession.setAttribute(springSecurityContextKey, context);
					this.isSaveContextInvoked = true;
					if (this.logger.isDebugEnabled()) {
						this.logger.debug(LogMessage.format("Stored %s to HttpSession [%s]", context, httpSession));
					}
				}
			}
		}

		private boolean contextChanged(SecurityContext context) {
			return this.isSaveContextInvoked || context != this.contextBeforeExecution
					|| context.getAuthentication() != this.authBeforeExecution;
		}

		private HttpSession createNewSessionIfAllowed(SecurityContext context) {
			if (isTransientAuthentication(context.getAuthentication())) {
				return null;
			}
			if (this.httpSessionExistedAtStartOfRequest) {
				this.logger.debug("HttpSession is now null, but was not null at start of request; "
						+ "session was invalidated, so do not create a new session");
				return null;
			}
			if (!HttpSessionSecurityContextRepository.this.allowSessionCreation) {
				this.logger.debug("The HttpSession is currently null, and the "
						+ HttpSessionSecurityContextRepository.class.getSimpleName()
						+ " is prohibited from creating an HttpSession "
						+ "(because the allowSessionCreation property is false) - SecurityContext thus not "
						+ "stored for next request");
				return null;
			}
			// Generate a HttpSession only if we need to
			if (HttpSessionSecurityContextRepository.this.contextObject.equals(context)) {
				this.logger.debug(LogMessage.format(
						"HttpSession is null, but SecurityContext has not changed from "
								+ "default empty context %s so not creating HttpSession or storing SecurityContext",
						context));
				return null;
			}
			try {
				HttpSession session = this.request.getSession(true);
				this.logger.debug("Created HttpSession as SecurityContext is non-default");
				return session;
			}
			catch (IllegalStateException ex) {
				// Response must already be committed, therefore can't create a new
				// session
				this.logger.warn("Failed to create a session, as response has been committed. "
						+ "Unable to store SecurityContext.");
			}
			return null;
		}

	}
```

（1）saveContext：该方法主要是用来保存SecurityContext，如果authentication对象为null或者它是一个匿名对象，则不需要保存SecurityContext（参见SEC-776：https://github.com/spring-projects/spring-security/issues/1036）；同时，如果httpSession不为null并且authBefore Execution也不为null，就从httpSession中将保存的登录用户数据移除，这个主要是为了防止开发者在注销成功的回调中继续调用chain.doFilter方法，进而导致原始的登录信息无法清除的问题（参见SEC-1587：https://github.com/spring-projects/spring-security/issues/1826）；如果httpSession为null，则去创建一个HttpSession对象；最后，如果SecurityContext发生了变化，或者httpSession中没有保存SecurityContext，则调用httpSession中的setAttribute方法将SecurityContext保存起来。
（2）contextChanged：该方法主要用来判断SecurityContext是否发生变化，因为在程序运行过程中，开发者可能修改了SecurityContext中的Authentication对象。
（3）createNewSessionIfAllowed：该方法用来创建一个HttpSession对象。

**SaveToSessionRequestWrapper**

```java
private static class SaveToSessionRequestWrapper extends HttpServletRequestWrapper {

   private final SaveContextOnUpdateOrErrorResponseWrapper response;

   SaveToSessionRequestWrapper(HttpServletRequest request, SaveContextOnUpdateOrErrorResponseWrapper response) {
      super(request);
      this.response = response;
   }

   @Override
   public AsyncContext startAsync() {
      this.response.disableSaveOnResponseCommitted();
      return super.startAsync();
   }

   @Override
   public AsyncContext startAsync(ServletRequest servletRequest, ServletResponse servletResponse)
         throws IllegalStateException {
      this.response.disableSaveOnResponseCommitted();
      return super.startAsync(servletRequest, servletResponse);
   }

}
```

​	SaveToSessionRequestWrapper类实际上是在Spring Security 3.2之后出现的封装类，在Spring Security 3.2之前并不存在SaveToSessionRequestWrapper类。封装的SaveToSession RequestWrapper类主要作用是禁止在异步Servlet提交时，自动保存SecurityContext。

​	在异步Servlet中，当任务执行完毕之后，HttpServletResponse也会自动提交，在提交的过程中会自动保存SecurityContext到HttpSession中，但是由于是在子线程中，因此无法获取到SecurityContext对象（SecurityContextHolder默认将数据存储在ThreadLocal中），所以会保存失败。如果开发者使用了异步Servlet，则默认情况下会禁用HttpServletResponse提交时自动保存SecurityContext这一功能，改为在SecurityContextPersistenceFilter过滤器中完成SecurityContext保存操作。



HttpSessionSecurityContextRepository类的功能：

```java
public class HttpSessionSecurityContextRepository implements SecurityContextRepository {

	/**
	 * The default key under which the security context will be stored in the session.
	 */
	public static final String SPRING_SECURITY_CONTEXT_KEY = "SPRING_SECURITY_CONTEXT";

	protected final Log logger = LogFactory.getLog(this.getClass());

	/**
	 * SecurityContext instance used to check for equality with default (unauthenticated)
	 * content
	 */
	private final Object contextObject = SecurityContextHolder.createEmptyContext();

	private boolean allowSessionCreation = true;

	private boolean disableUrlRewriting = false;

	private String springSecurityContextKey = SPRING_SECURITY_CONTEXT_KEY;

	private AuthenticationTrustResolver trustResolver = new AuthenticationTrustResolverImpl();

	/**
	 * Gets the security context for the current request (if available) and returns it.
	 * <p>
	 * If the session is null, the context object is null or the context object stored in
	 * the session is not an instance of {@code SecurityContext}, a new context object
	 * will be generated and returned.
	 */
	@Override
	public SecurityContext loadContext(HttpRequestResponseHolder requestResponseHolder) {
		HttpServletRequest request = requestResponseHolder.getRequest();
		HttpServletResponse response = requestResponseHolder.getResponse();
		HttpSession httpSession = request.getSession(false);
		SecurityContext context = readSecurityContextFromSession(httpSession);
		if (context == null) {
			context = generateNewContext();
			if (this.logger.isTraceEnabled()) {
				this.logger.trace(LogMessage.format("Created %s", context));
			}
		}
		SaveToSessionResponseWrapper wrappedResponse = new SaveToSessionResponseWrapper(response, request,
				httpSession != null, context);
		requestResponseHolder.setResponse(wrappedResponse);
		requestResponseHolder.setRequest(new SaveToSessionRequestWrapper(request, wrappedResponse));
		return context;
	}

	@Override
	public void saveContext(SecurityContext context, HttpServletRequest request, HttpServletResponse response) {
		SaveContextOnUpdateOrErrorResponseWrapper responseWrapper = WebUtils.getNativeResponse(response,
				SaveContextOnUpdateOrErrorResponseWrapper.class);
		Assert.state(responseWrapper != null, () -> "Cannot invoke saveContext on response " + response
				+ ". You must use the HttpRequestResponseHolder.response after invoking loadContext");
		responseWrapper.saveContext(context);
	}

	@Override
	public boolean containsContext(HttpServletRequest request) {
		HttpSession session = request.getSession(false);
		if (session == null) {
			return false;
		}
		return session.getAttribute(this.springSecurityContextKey) != null;
	}

	/**
	 * @param httpSession the session obtained from the request.
	 */
	private SecurityContext readSecurityContextFromSession(HttpSession httpSession) {
		if (httpSession == null) {
			this.logger.trace("No HttpSession currently exists");
			return null;
		}
		// Session exists, so try to obtain a context from it.
		Object contextFromSession = httpSession.getAttribute(this.springSecurityContextKey);
		if (contextFromSession == null) {
			if (this.logger.isTraceEnabled()) {
				this.logger.trace(LogMessage.format("Did not find SecurityContext in HttpSession %s "
						+ "using the SPRING_SECURITY_CONTEXT session attribute", httpSession.getId()));
			}
			return null;
		}

		// We now have the security context object from the session.
		if (!(contextFromSession instanceof SecurityContext)) {
			this.logger.warn(LogMessage.format(
					"%s did not contain a SecurityContext but contained: '%s'; are you improperly "
							+ "modifying the HttpSession directly (you should always use SecurityContextHolder) "
							+ "or using the HttpSession attribute reserved for this class?",
					this.springSecurityContextKey, contextFromSession));
			return null;
		}

		if (this.logger.isTraceEnabled()) {
			this.logger.trace(
					LogMessage.format("Retrieved %s from %s", contextFromSession, this.springSecurityContextKey));
		}
		else if (this.logger.isDebugEnabled()) {
			this.logger.debug(LogMessage.format("Retrieved %s", contextFromSession));
		}
		// Everything OK. The only non-null return from this method.
		return (SecurityContext) contextFromSession;
	}

	/**
	 * By default, calls {@link SecurityContextHolder#createEmptyContext()} to obtain a
	 * new context (there should be no context present in the holder when this method is
	 * called). Using this approach the context creation strategy is decided by the
	 * {@link SecurityContextHolderStrategy} in use. The default implementations will
	 * return a new <tt>SecurityContextImpl</tt>.
	 * @return a new SecurityContext instance. Never null.
	 */
	protected SecurityContext generateNewContext() {
		return SecurityContextHolder.createEmptyContext();
	}

	/**
	 * If set to true (the default), a session will be created (if required) to store the
	 * security context if it is determined that its contents are different from the
	 * default empty context value.
	 * <p>
	 * Note that setting this flag to false does not prevent this class from storing the
	 * security context. If your application (or another filter) creates a session, then
	 * the security context will still be stored for an authenticated user.
	 * @param allowSessionCreation
	 */
	public void setAllowSessionCreation(boolean allowSessionCreation) {
		this.allowSessionCreation = allowSessionCreation;
	}

	/**
	 * Allows the use of session identifiers in URLs to be disabled. Off by default.
	 * @param disableUrlRewriting set to <tt>true</tt> to disable URL encoding methods in
	 * the response wrapper and prevent the use of <tt>jsessionid</tt> parameters.
	 */
	public void setDisableUrlRewriting(boolean disableUrlRewriting) {
		this.disableUrlRewriting = disableUrlRewriting;
	}

	/**
	 * Allows the session attribute name to be customized for this repository instance.
	 * @param springSecurityContextKey the key under which the security context will be
	 * stored. Defaults to {@link #SPRING_SECURITY_CONTEXT_KEY}.
	 */
	public void setSpringSecurityContextKey(String springSecurityContextKey) {
		Assert.hasText(springSecurityContextKey, "springSecurityContextKey cannot be empty");
		this.springSecurityContextKey = springSecurityContextKey;
	}

	private boolean isTransientAuthentication(Authentication authentication) {
		return AnnotationUtils.getAnnotation(authentication.getClass(), Transient.class) != null;
	}

	/**
	 * Sets the {@link AuthenticationTrustResolver} to be used. The default is
	 * {@link AuthenticationTrustResolverImpl}.
	 * @param trustResolver the {@link AuthenticationTrustResolver} to use. Cannot be
	 * null.
	 */
	public void setTrustResolver(AuthenticationTrustResolver trustResolver) {
		Assert.notNull(trustResolver, "trustResolver cannot be null");
		this.trustResolver = trustResolver;
	}

	private static class SaveToSessionRequestWrapper extends HttpServletRequestWrapper {
		...
	}

	/**
	 * Wrapper that is applied to every request/response to update the
	 * <code>HttpSession</code> with the <code>SecurityContext</code> when a
	 * <code>sendError()</code> or <code>sendRedirect</code> happens. See SEC-398.
	 * <p>
	 * Stores the necessary state from the start of the request in order to make a
	 * decision about whether the security context has changed before saving it.
	 */
	final class SaveToSessionResponseWrapper extends SaveContextOnUpdateOrErrorResponseWrapper {
		...
	}

}

```

（1）首先通过SPRING_SECURITY_CONTEXT_KEY变量定义了SecurityContext在HttpSession中存储的key，如果开发者需要手动操作HttpSession中存储的SecurityContext，可以通过该key来操作。
（2）trustResolver是一个用户身份评估器，用来判断当前用户是匿名用户还是通过RememberMe登录的用户。
（3）在loadContext方法中，通过调用readSecurityContextFromSession方法来获取SecurityContext对象。如果获取到的对象为null，则调用generateNewContext方法去生成一个空的SecurityContext对象，最后构造请求和响应的装饰类并存入requestResponseHolder对象中。
（4）saveContext方法用来保存SecurityContext，在保存之前，会先调用isContextSaved方法判断是否已经保存了，如果已经保存了，则不再保存。正常情况下，在HttpServletResponse提交时SecurityContext就已经保存到HttpSession中了；如果是异步Servlet，则提交时不会自动将SecurityContext保存到HttpSession，此时会在这里进行保存操作。
（5）containsContext方法用来判断请求中是否存在SecurityContext对象。
（6）readSecurityContextFromSession方法执行具体的SecurityContext读取逻辑，从HttpSession中获取SecurityContext并返回。
（7）generateNewContext方法用来生成一个不包含Authentication的空的SecurityContext对象。
（8）setAllowSessionCreation方法用来设置是否允许创建HttpSession，默认是true。
（9）setDisableUrlRewriting方法表示是否禁用URL重写，默认是false。
（10）setSpringSecurityContextKey方法可以用来配置HttpSession中存储SecurityContext的key。
（11）isTransientAuthentication方法用来判断Authentication是否免于存储。
（12）setTrustResolver方法用来配置身份评估器。

​	HttpSessionSecurityContextRepository所提供的所有功能，这些功能都将在SecurityContextPersistenceFilter过滤器中进行调用，那么接下来我们就来看一下SecurityContext PersistenceFilter中的调用逻辑:

```java
public class SecurityContextPersistenceFilter extends GenericFilterBean {

	static final String FILTER_APPLIED = "__spring_security_scpf_applied";

	private SecurityContextRepository repo;

	private boolean forceEagerSessionCreation = false;

	public SecurityContextPersistenceFilter() {
		this(new HttpSessionSecurityContextRepository());
	}

	public SecurityContextPersistenceFilter(SecurityContextRepository repo) {
		this.repo = repo;
	}

	@Override
	public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain)
			throws IOException, ServletException {
		doFilter((HttpServletRequest) request, (HttpServletResponse) response, chain);
	}

	private void doFilter(HttpServletRequest request, HttpServletResponse response, FilterChain chain)
			throws IOException, ServletException {
		// ensure that filter is only applied once per request
		if (request.getAttribute(FILTER_APPLIED) != null) {
			chain.doFilter(request, response);
			return;
		}
		request.setAttribute(FILTER_APPLIED, Boolean.TRUE);
		if (this.forceEagerSessionCreation) {
			HttpSession session = request.getSession();
			if (this.logger.isDebugEnabled() && session.isNew()) {
				this.logger.debug(LogMessage.format("Created session %s eagerly", session.getId()));
			}
		}
		HttpRequestResponseHolder holder = new HttpRequestResponseHolder(request, response);
		SecurityContext contextBeforeChainExecution = this.repo.loadContext(holder);
		try {
			SecurityContextHolder.setContext(contextBeforeChainExecution);
			if (contextBeforeChainExecution.getAuthentication() == null) {
				logger.debug("Set SecurityContextHolder to empty SecurityContext");
			}
			else {
				if (this.logger.isDebugEnabled()) {
					this.logger
							.debug(LogMessage.format("Set SecurityContextHolder to %s", contextBeforeChainExecution));
				}
			}
			chain.doFilter(holder.getRequest(), holder.getResponse());
		}
		finally {
			SecurityContext contextAfterChainExecution = SecurityContextHolder.getContext();
			// Crucial removal of SecurityContextHolder contents before anything else.
			SecurityContextHolder.clearContext();
			this.repo.saveContext(contextAfterChainExecution, holder.getRequest(), holder.getResponse());
			request.removeAttribute(FILTER_APPLIED);
			this.logger.debug("Cleared SecurityContextHolder to complete request");
		}
	}

	public void setForceEagerSessionCreation(boolean forceEagerSessionCreation) {
		this.forceEagerSessionCreation = forceEagerSessionCreation;
	}

}
```

（1）首先从request中获取FILTER_APPLIED属性，如果该属性值不为null，则直接执行chain.doFilter方法，当前过滤器到此为止，这个判断主要是确保该请求只执行一次该过滤器。如果确实是该request第一次经过该过滤器，则给其设置上FILTER_APPLIED属性。
（2）forceEagerSessionCreation变量表示是否要在过滤器链执行之前确保会话有效，由于这是一个比较耗费资源的操作，因此默认为false。
（3）构造HttpRequestResponseHolder对象，将HttpServletRequest和HttpServletResponse都存储进去。
（4）调用repo.loadContext方法去加载SecurityContext，repo实际上就是我们前面所说HttpSessionSecurityContextRepository的实例，所以loadContext方法这里就不再赘述了。
（5）将读取到的SecurityContext存入SecurityContextHolder之中，这样，在接下来的处理逻辑中，开发者就可以直接通过SecurityContextHolder获取当前登录用户对象了。
（6）调用chain.doFilter方法使请求继续向下走，但是要注意，此时传递的request和response对象是在HttpSessionSecurityContextRepository中封装后的对象，即SaveToSession ResponseWrapper和SaveToSessionRequestWrapper的实例。
（7）当请求处理完毕后，在finally模块中，获取最新的SecurityContext对象（开发者可能在后续处理中修改了SecurityContext中的Authentication对象），然后清空SecurityContext Holder中的数据；再调用repo.saveContext方法保存SecurityContext，具体的保存逻辑前面已经说过，这里就不再赘述了。
（8）最后，从request中移除FILTER_APPLIED属性。

​	这就是整个SecurityContextPersistenceFilter过滤器的工作逻辑。一言以蔽之，请求在到达SecurityContextPersistenceFilter过滤器之后，先从HttpSession中读取SecurityContext出来，并存入SecurityContextHolder之中以备后续使用；当请求离开SecurityContextPersistenceFilter过滤器的时候，获取最新的SecurityContext并存入HttpSession中，同时清空SecurityContextHolder中的登录用户信息。

#### 从当前请求对象中获取

```java
	@RequestMapping("/authentication")
    public void authentication(Authentication authentication) {
       System.out.println("authentication = " + authentication);
    }
    @RequestMapping("/principal")
    public void principal(Principal principal) {
       System.out.println("principal = " + principal);
    }
```

在Servlet规范中，最早有三个和安全管理相关的方法：

```java
  public String getRemoteUser();
  public boolean isUserInRole(String role);
  public java.security.Principal getUserPrincipal();
```

（1）getRemoteUser方法用来获取登录用户名。
（2）isUserInRole方法用来判断当前登录用户是否具备某一个指定的角色。
（3）getUserPrincipal方法用来获取当前认证主体。
从Servlet 3.0开始，在这三个方法的基础之上，又增加了三个和安全管理相关的方法：

```java
public boolean authenticate(HttpServletResponse response)
               throws IOException, ServletException;
    public void login(String username, String password) throws ServletException;
    public void logout() throws ServletException;
```

（1）authenticate方法可以判断当前请求是否认证成功。
（2）login方法可以执行登录操作。
（3）logout方法可以执行注销操作。
不过HttpServletRequest只是一个接口，这些安全认证相关的方法，在不同环境下会有不同的实现。

​	如果是一个普通的Web项目，不使用任何框架，HttpServletRequest的默认实现类是Tomcat中的RequestFacade，从这个类的名字上就可以看出来，这是一个使用了Facade模式（外观模式）的类，真正提供底层服务的是Tomcat中的Request对象，只不过这个Request对象在实现Servlet规范的同时，还定义了很多Tomcat内部的方法，为了避免开发者直接调用到这些内部方法，这里使用了外观模式。
在Tomcat的Request类中，对上面这些方法都做了实现，基本上都是基于Tomcat提供的Realm来实现的，这种认证方式非常冷门，项目中很少使用，因此这里不做过多介绍，感兴趣的读者可以查看https://github.com/lenve/javaboy-code-samples仓库中的basiclogin案例来了解其用法。
​	如果使用了Spring Security框架，那么我们在Controller参数中拿到的HttpServletRequest实例将是Servlet3SecurityContextHolderAwareRequestWrapper，很明显，这是被Spring Security封装过的请求。
我们来看一下Servlet3SecurityContextHolderAwareRequestWrapper的继承关系，如图2-21所示。

![](https://pic.imgdb.cn/item/60b3273108f74bc1599df0a1.jpg)



# Spring源码

## Spring如何加载配置文件到应用

读取规范

* BeanDefinitionReader
* BeanDefinition
* 根据BeanDefinition实例化(反射)。
* 初始化(属性完成复制)
  * 填充属性、赋值。
  * 调用具体的初始化方法。

![](https://pic.imgdb.cn/item/60acd38008f74bc159214a43.jpg)

* BeanFactory
* 反射
  * Class.forName("完全限定名");
  * 对象.getClass();
  * 类.class
  * Constructor ctor = clazz.getDeclaredConstructor();
  * Object obj  = ctor.newInstance();
* PostProcessor
  * 增强器/后置处理器
  * 扩展
  * BeanFactoryPostProcessor
    * 处理占位符:PlaceholderConfigurerSupport

![](https://pic.imgdb.cn/item/60acd5ba08f74bc15932cf2f.jpg)

```java
@FunctionalInterface
public interface BeanFactoryPostProcessor {

	/**
	 * Modify the application context's internal bean factory after its standard
	 * initialization. All bean definitions will have been loaded, but no beans
	 * will have been instantiated yet. This allows for overriding or adding
	 * properties even to eager-initializing beans.
	 * @param beanFactory the bean factory used by the application context
	 * @throws org.springframework.beans.BeansException in case of errors
	 */
	void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException;

}
```

* BeanPostProcessor
  * AOP
    * AbstractAutoProxyCreator
    * DefaultAopProxyFactory



![](https://pic.imgdb.cn/item/60acd85508f74bc15947a2bf.jpg)

还需要了解

* Environment
  * env
  * properties
  * StandardEnviroment
  * 为了方便使用，在容器创建的时候会提前将系统的相关属性加载到StandardEnvironment对象中。
* 如果需要了解创建过程中每一步的进度，如何做？在不同的阶段要做不同的处理工作，应该怎么办？
  * 观察者模式:监听器，监听事件



```java
/* <p>Bean factory implementations should support the standard bean lifecycle interfaces
 * as far as possible. The full set of initialization methods and their standard order is:
 * <ol>
 * <li>BeanNameAware's {@code setBeanName}
 * <li>BeanClassLoaderAware's {@code setBeanClassLoader}
 * <li>BeanFactoryAware's {@code setBeanFactory}
 * <li>EnvironmentAware's {@code setEnvironment}
 * <li>EmbeddedValueResolverAware's {@code setEmbeddedValueResolver}
 * <li>ResourceLoaderAware's {@code setResourceLoader}
 * (only applicable when running in an application context)
 * <li>ApplicationEventPublisherAware's {@code setApplicationEventPublisher}
 * (only applicable when running in an application context)
 * <li>MessageSourceAware's {@code setMessageSource}
 * (only applicable when running in an application context)
 * <li>ApplicationContextAware's {@code setApplicationContext}
 * (only applicable when running in an application context)
 * <li>ServletContextAware's {@code setServletContext}
 * (only applicable when running in a web application context)
 * <li>{@code postProcessBeforeInitialization} methods of BeanPostProcessors
 * <li>InitializingBean's {@code afterPropertiesSet}
 * <li>a custom init-method definition
 * <li>{@code postProcessAfterInitialization} methods of BeanPostProcessors
 * </ol>
 *
 * <p>On shutdown of a bean factory, the following lifecycle methods apply:
 * <ol>
 * <li>{@code postProcessBeforeDestruction} methods of DestructionAwareBeanPostProcessors
 * <li>DisposableBean's {@code destroy}
 * <li>a custom destroy-method definition
 * </ol>
 *
 * @author Rod Johnson
 * @author Juergen Hoeller
 * @author Chris Beams
 * @since 13 April 2001
 * @see BeanNameAware#setBeanName
 * @see BeanClassLoaderAware#setBeanClassLoader
 * @see BeanFactoryAware#setBeanFactory
 * @see org.springframework.context.ResourceLoaderAware#setResourceLoader
 * @see org.springframework.context.ApplicationEventPublisherAware#setApplicationEventPublisher
 * @see org.springframework.context.MessageSourceAware#setMessageSource
 * @see org.springframework.context.ApplicationContextAware#setApplicationContext
 * @see org.springframework.web.context.ServletContextAware#setServletContext
 * @see org.springframework.beans.factory.config.BeanPostProcessor#postProcessBeforeInitialization
 * @see InitializingBean#afterPropertiesSet
 * @see org.springframework.beans.factory.support.RootBeanDefinition#getInitMethodName
 * @see org.springframework.beans.factory.config.BeanPostProcessor#postProcessAfterInitialization
 * @see DisposableBean#destroy
 * @see org.springframework.beans.factory.support.RootBeanDefinition#getDestroyMethodName
 */
public interface BeanFactory {
 }
```

## Spring Bean初始化过程

AbstractApplicationContext::refresh

* 准备工作prepareRefresh()
* 创建BeanFactory对象:DefaultListableBeanFactory
  * 加载BeanDefinition
    * beanDefinitionMap
    * beanDefinitionNames
* prepareBeanFactory
  * 填充BeanFactory添加一些属性值。
* postProcessBeanFactory
  * 空实现
* invokeBeanFactoryPostProcessors
  * 注册实例化BeanFactoryPostProcessor并且调用方法。
* registerBeanPostProcessors
  * 注册Bean的PostProcessor
* initMessageSource
  * 国际化
* initApplicationEventMulticaster
  * 初始化事件多播器
* OnRefresh
  * Initialize other special beans in specific context subclasses.
  * 空实现
* registerListeners
  * 注册监听器
* finishBeanFactoryInitialization
  * 实例化非懒加载的实例初始化
  * 获取BeanDefinition
  * 如果不是abstract、是singleton不是懒加载
  * FactoryBean和普通Bean是两个加载方式，主要是普通Bean(getBean)
    * doGetBean
      * createBean
      * doCreateBean
      * createBeanInstance
      * 获取构造器, 实例化BeannewInstance()
      * 调用Aware方法
      * 执行PostProcessor的before



Spring提供了什么扩展性

* 在对象创建之前添加某些功能。
* 在容器初始化之前添加某些功能。
* 在不同的阶段发出不同的时间，完成一些功能。
* 抽象出一堆的接口来帮助扩展。
* 面向接口编程。

## 循环依赖问题

## AOP

* 动态代理
  * cglib
  * jdk

![](https://pic.imgdb.cn/item/60ae446708f74bc159af823f.jpg)

* aop具体执行步骤
  * IOC容器，存放对象，先准备存放对象的容器。
  * 进行对象的实例化和初始化操作，将生成的完整对象放到容器中，存放具体的对象。
    * 容器运行过程中需要的对象
      * BeanFactoryPostProcessor
      * BeanPostProcessor
      * AbstractAutoProxyCreator
    * 用户需要的对象。
    * advice
      * MethodInterceptor
  * 从已经创建好的容器中获取需要的对象。
  * 调用具体的方法。

![](https://pic.imgdb.cn/item/60ae486208f74bc159e6e155.jpg)

* TransactionInterceptor
* AutowiredAnnotationBeanPostProcessor

# 英文自我介绍

​	Interviewer Hello, I am pleased to have the opportunity to participate in the interview, my name is Songzili, you can call me Sorie.

​	I graduated from Chongqing University in 2018, but I have been working since the summer of 2016. In the beginning, I took on the role of product manager.From the summer of 2017, I started to work as project manager, product manager and Java developer.

​	The first project I participated in was the educational administration system, which included more than ten systems.

​	For example, CET registration system, TA management system, course management system, course enrollment system.

​	My second job was as a C language developer,  developing SMB protocol.But I think Java is better for me as a working language.

​	That's all, Thank you.